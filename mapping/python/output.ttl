@prefix schema1: <http://schema.org/> .
@prefix software_vocab: <http://ontologies.siemens-energy.com/software/> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

<https://data.siemens-energy.com/cybersec/package/Flask> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/Jinja2>,
        <https://data.siemens-energy.com/cybersec/package/Werkzeug>,
        <https://data.siemens-energy.com/cybersec/package/asgiref>,
        <https://data.siemens-energy.com/cybersec/package/blinker>,
        <https://data.siemens-energy.com/cybersec/package/click>,
        <https://data.siemens-energy.com/cybersec/package/importlib-metadata>,
        <https://data.siemens-energy.com/cybersec/package/itsdangerous> ;
    schema1:description """# Flask

Flask is a lightweight [WSGI][] web application framework. It is designed
to make getting started quick and easy, with the ability to scale up to
complex applications. It began as a simple wrapper around [Werkzeug][]
and [Jinja][], and has become one of the most popular Python web
application frameworks.

Flask offers suggestions, but doesn't enforce any dependencies or
project layout. It is up to the developer to choose the tools and
libraries they want to use. There are many extensions provided by the
community that make adding new functionality easy.

[WSGI]: https://wsgi.readthedocs.io/
[Werkzeug]: https://werkzeug.palletsprojects.com/
[Jinja]: https://jinja.palletsprojects.com/


## A Simple Example

```python
# save this as app.py
from flask import Flask

app = Flask(__name__)

@app.route("/")
def hello():
    return "Hello, World!"
```

```
$ flask run
  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
```


## Donate

The Pallets organization develops and supports Flask and the libraries
it uses. In order to grow the community of contributors and users, and
allow the maintainers to devote more time to the projects, [please
donate today][].

[please donate today]: https://palletsprojects.com/donate

"""^^xsd:string ;
    schema1:name "Flask"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.1.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/PyJWT> a software_vocab:Software_package ;
    schema1:description """PyJWT
=====

.. image:: https://travis-ci.com/jpadilla/pyjwt.svg?branch=master
   :target: http://travis-ci.com/jpadilla/pyjwt?branch=master

.. image:: https://ci.appveyor.com/api/projects/status/h8nt70aqtwhht39t?svg=true
   :target: https://ci.appveyor.com/project/jpadilla/pyjwt

.. image:: https://img.shields.io/pypi/v/pyjwt.svg
   :target: https://pypi.python.org/pypi/pyjwt

.. image:: https://coveralls.io/repos/jpadilla/pyjwt/badge.svg?branch=master
   :target: https://coveralls.io/r/jpadilla/pyjwt?branch=master

.. image:: https://readthedocs.org/projects/pyjwt/badge/?version=latest
   :target: https://pyjwt.readthedocs.io

A Python implementation of `RFC 7519 <https://tools.ietf.org/html/rfc7519>`_. Original implementation was written by `@progrium <https://github.com/progrium>`_.

Sponsor
-------

+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| |auth0-logo| | If you want to quickly add secure token-based authentication to Python projects, feel free to check Auth0's Python SDK and free plan at `auth0.com/overview <https://auth0.com/overview?utm_source=GHsponsor&utm_medium=GHsponsor&utm_campaign=pyjwt&utm_content=auth>`_. |
+--------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

.. |auth0-logo| image:: https://user-images.githubusercontent.com/83319/31722733-de95bbde-b3ea-11e7-96bf-4f4e8f915588.png

Installing
----------

Install with **pip**:

.. code-block:: sh

    $ pip install PyJWT


Usage
-----

.. code:: python

    >>> import jwt
    >>> encoded = jwt.encode({'some': 'payload'}, 'secret', algorithm='HS256')
    'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb21lIjoicGF5bG9hZCJ9.4twFt5NiznN84AWoo1d7KO1T_yoc0Z6XOpOVswacPZg'

    >>> jwt.decode(encoded, 'secret', algorithms=['HS256'])
    {'some': 'payload'}


Command line
------------

Usage::

    pyjwt [options] INPUT

Decoding examples::

    pyjwt --key=secret decode TOKEN
    pyjwt decode --no-verify TOKEN

See more options executing ``pyjwt --help``.


Documentation
-------------

View the full docs online at https://pyjwt.readthedocs.io/en/latest/


Tests
-----

You can run tests from the project root after cloning with:

.. code-block:: sh

    $ python setup.py test


"""^^xsd:string ;
    schema1:name "PyJWT"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.7.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/PyYAML> a software_vocab:Software_package ;
    schema1:description """YAML is a data serialization format designed for human readability
and interaction with scripting languages.  PyYAML is a YAML parser
and emitter for Python.

PyYAML features a complete YAML 1.1 parser, Unicode support, pickle
support, capable extension API, and sensible error messages.  PyYAML
supports standard YAML tags and provides Python-specific tags that
allow to represent an arbitrary Python object.

PyYAML is applicable for a broad range of tasks from complex
configuration files to object serialization and persistence.
"""^^xsd:string ;
    schema1:name "PyYAML"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "6.0.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/Pygments> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/colorama> ;
    schema1:description """Pygments
~~~~~~~~

Pygments is a syntax highlighting package written in Python.

It is a generic syntax highlighter suitable for use in code hosting, forums,
wikis or other applications that need to prettify source code.  Highlights
are:

* a wide range of over 500 languages and other text formats is supported
* special attention is paid to details, increasing quality by a fair amount
* support for new languages and formats are added easily
* a number of output formats, presently HTML, LaTeX, RTF, SVG, all image
  formats that PIL supports and ANSI sequences
* it is usable as a command-line tool and as a library

Copyright 2006-2024 by the Pygments team, see ``AUTHORS``.
Licensed under the BSD, see ``LICENSE`` for details.
"""^^xsd:string ;
    schema1:name "Pygments"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.18.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/arrow> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/dateparser>,
        <https://data.siemens-energy.com/cybersec/package/python-dateutil>,
        <https://data.siemens-energy.com/cybersec/package/pytz>,
        <https://data.siemens-energy.com/cybersec/package/simplejson>,
        <https://data.siemens-energy.com/cybersec/package/sphinx>,
        <https://data.siemens-energy.com/cybersec/package/sphinx_rtd_theme>,
        <https://data.siemens-energy.com/cybersec/package/types-python-dateutil> ;
    schema1:description """Arrow: Better dates & times for Python
======================================

.. start-inclusion-marker-do-not-remove

.. image:: https://github.com/arrow-py/arrow/workflows/tests/badge.svg?branch=master
   :alt: Build Status
   :target: https://github.com/arrow-py/arrow/actions?query=workflow%3Atests+branch%3Amaster

.. image:: https://codecov.io/gh/arrow-py/arrow/branch/master/graph/badge.svg
   :alt: Coverage
   :target: https://codecov.io/gh/arrow-py/arrow

.. image:: https://img.shields.io/pypi/v/arrow.svg
   :alt: PyPI Version
   :target: https://pypi.python.org/pypi/arrow

.. image:: https://img.shields.io/pypi/pyversions/arrow.svg
   :alt: Supported Python Versions
   :target: https://pypi.python.org/pypi/arrow

.. image:: https://img.shields.io/pypi/l/arrow.svg
   :alt: License
   :target: https://pypi.python.org/pypi/arrow

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :alt: Code Style: Black
   :target: https://github.com/psf/black


**Arrow** is a Python library that offers a sensible and human-friendly approach to creating, manipulating, formatting and converting dates, times and timestamps. It implements and updates the datetime type, plugging gaps in functionality and providing an intelligent module API that supports many common creation scenarios. Simply put, it helps you work with dates and times with fewer imports and a lot less code.

Arrow is named after the `arrow of time <https://en.wikipedia.org/wiki/Arrow_of_time>`_ and is heavily inspired by `moment.js <https://github.com/moment/moment>`_ and `requests <https://github.com/psf/requests>`_.

Why use Arrow over built-in modules?
------------------------------------

Python's standard library and some other low-level modules have near-complete date, time and timezone functionality, but don't work very well from a usability perspective:

- Too many modules: datetime, time, calendar, dateutil, pytz and more
- Too many types: date, time, datetime, tzinfo, timedelta, relativedelta, etc.
- Timezones and timestamp conversions are verbose and unpleasant
- Timezone naivety is the norm
- Gaps in functionality: ISO 8601 parsing, timespans, humanization

Features
--------

- Fully-implemented, drop-in replacement for datetime
- Support for Python 3.6+
- Timezone-aware and UTC by default
- Super-simple creation options for many common input scenarios
- ``shift`` method with support for relative offsets, including weeks
- Format and parse strings automatically
- Wide support for the `ISO 8601 <https://en.wikipedia.org/wiki/ISO_8601>`_ standard
- Timezone conversion
- Support for ``dateutil``, ``pytz``, and ``ZoneInfo`` tzinfo objects
- Generates time spans, ranges, floors and ceilings for time frames ranging from microsecond to year
- Humanize dates and times with a growing list of contributed locales
- Extensible for your own Arrow-derived types
- Full support for PEP 484-style type hints

Quick Start
-----------

Installation
~~~~~~~~~~~~

To install Arrow, use `pip <https://pip.pypa.io/en/stable/quickstart/>`_ or `pipenv <https://docs.pipenv.org>`_:

.. code-block:: console

    $ pip install -U arrow

Example Usage
~~~~~~~~~~~~~

.. code-block:: python

    >>> import arrow
    >>> arrow.get('2013-05-11T21:23:58.970460+07:00')
    <Arrow [2013-05-11T21:23:58.970460+07:00]>

    >>> utc = arrow.utcnow()
    >>> utc
    <Arrow [2013-05-11T21:23:58.970460+00:00]>

    >>> utc = utc.shift(hours=-1)
    >>> utc
    <Arrow [2013-05-11T20:23:58.970460+00:00]>

    >>> local = utc.to('US/Pacific')
    >>> local
    <Arrow [2013-05-11T13:23:58.970460-07:00]>

    >>> local.timestamp()
    1368303838.970460

    >>> local.format()
    '2013-05-11 13:23:58 -07:00'

    >>> local.format('YYYY-MM-DD HH:mm:ss ZZ')
    '2013-05-11 13:23:58 -07:00'

    >>> local.humanize()
    'an hour ago'

    >>> local.humanize(locale='ko-kr')
    'í•œì‹œê°„ ì „'

.. end-inclusion-marker-do-not-remove

Documentation
-------------

For full documentation, please visit `arrow.readthedocs.io <https://arrow.readthedocs.io>`_.

Contributing
------------

Contributions are welcome for both code and localizations (adding and updating locales). Begin by gaining familiarity with the Arrow library and its features. Then, jump into contributing:

#. Find an issue or feature to tackle on the `issue tracker <https://github.com/arrow-py/arrow/issues>`_. Issues marked with the `"good first issue" label <https://github.com/arrow-py/arrow/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22>`_ may be a great place to start!
#. Fork `this repository <https://github.com/arrow-py/arrow>`_ on GitHub and begin making changes in a branch.
#. Add a few tests to ensure that the bug was fixed or the feature works as expected.
#. Run the entire test suite and linting checks by running one of the following commands: ``tox && tox -e lint,docs`` (if you have `tox <https://tox.readthedocs.io>`_ installed) **OR** ``make build39 && make test && make lint`` (if you do not have Python 3.9 installed, replace ``build39`` with the latest Python version on your system).
#. Submit a pull request and await feedback ðŸ˜ƒ.

If you have any questions along the way, feel free to ask them `here <https://github.com/arrow-py/arrow/discussions>`_.

Support Arrow
-------------

`Open Collective <https://opencollective.com/>`_ is an online funding platform that provides tools to raise money and share your finances with full transparency. It is the platform of choice for individuals and companies to make one-time or recurring donations directly to the project. If you are interested in making a financial contribution, please visit the `Arrow collective <https://opencollective.com/arrow>`_.

"""^^xsd:string ;
    schema1:name "arrow"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.3.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/backcall> a software_vocab:Software_package ;
    schema1:description """========
backcall
========

.. image:: https://travis-ci.org/takluyver/backcall.png?branch=master
        :target: https://travis-ci.org/takluyver/backcall

Specifications for callback functions passed in to an API

If your code lets other people supply callback functions, it's important to
specify the function signature you expect, and check that functions support that.
Adding extra parameters later would break other peoples code unless you're careful.

backcall provides a way of specifying the callback signature using a prototype
function::

    from backcall import callback_prototype
    
    @callback_prototype
    def handle_ping(sender, delay=None):
        # Specify positional parameters without a default, and keyword
        # parameters with a default.
        pass
    
    def register_ping_handler(callback):
        # This checks and adapts the function passed in:
        callback = handle_ping.adapt(callback)
        ping_callbacks.append(callback)

If the callback takes fewer parameters than your prototype, *backcall* will wrap
it in a function that discards the extra arguments. If the callback expects
more arguments, a TypeError is thrown when it is registered.

For more details, see the `docs <http://backcall.readthedocs.org/en/latest/>`_ or
the `Demo notebook <http://nbviewer.ipython.org/github/takluyver/backcall/blob/master/Demo.ipynb>`_.

The tests are run with `pytest <http://pytest.org/latest/>`_. In the root directory,
execute::

    py.test

"""^^xsd:string ;
    schema1:name "backcall"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.2.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/boolean.py> a software_vocab:Software_package ;
    schema1:description """

This library helps you deal with boolean expressions and algebra with variables
and the boolean functions AND, OR, NOT.

You can parse expressions from strings and simplify and compare expressions.
You can also easily create your custom algreba and mini DSL and create custom
tokenizers to handle custom expressions.

For extensive documentation look either into the docs directory or view it online, at
https://booleanpy.readthedocs.org/en/latest/

https://github.com/bastikr/boolean.py

Copyright (c) 2009-2020 Sebastian Kraemer, basti.kr@gmail.com and others
SPDX-License-Identifier: BSD-2-Clause


"""^^xsd:string ;
    schema1:name "boolean.py"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "4.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/boto3> a software_vocab:Software_package ;
    schema1:description """===============================
Boto3 - The AWS SDK for Python
===============================

|Version| |Python| |License|

Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for
Python, which allows Python developers to write software that makes use
of services like Amazon S3 and Amazon EC2. You can find the latest, most
up to date, documentation at our `doc site`_, including a list of
services that are supported.

Boto3 is maintained and published by `Amazon Web Services`_.

Boto (pronounced boh-toh) was named after the fresh water dolphin native to the Amazon river. The name was chosen by the author of the original Boto library, Mitch Garnaat, as a reference to the company.

Notices
-------

On 2023-12-13, support for Python 3.7 ended for Boto3. This follows the
Python Software Foundation `end of support <https://peps.python.org/pep-0537/#lifespan>`__
for the runtime which occurred on 2023-06-27.
For more information, see this `blog post <https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/>`__.

.. _boto: https://docs.pythonboto.org/
.. _`doc site`: https://boto3.amazonaws.com/v1/documentation/api/latest/index.html
.. _`Amazon Web Services`: https://aws.amazon.com/what-is-aws/
.. |Python| image:: https://img.shields.io/pypi/pyversions/boto3.svg?style=flat
    :target: https://pypi.python.org/pypi/boto3/
    :alt: Python Versions
.. |Version| image:: http://img.shields.io/pypi/v/boto3.svg?style=flat
    :target: https://pypi.python.org/pypi/boto3/
    :alt: Package Version
.. |License| image:: http://img.shields.io/pypi/l/boto3.svg?style=flat
    :target: https://github.com/boto/boto3/blob/develop/LICENSE
    :alt: License

Getting Started
---------------
Assuming that you have a supported version of Python installed, you can first
set up your environment with:

.. code-block:: sh

    $ python -m venv .venv
    ...
    $ . .venv/bin/activate

Then, you can install boto3 from PyPI with:

.. code-block:: sh

    $ python -m pip install boto3

or install from source with:

.. code-block:: sh

    $ git clone https://github.com/boto/boto3.git
    $ cd boto3
    $ python -m pip install -r requirements.txt
    $ python -m pip install -e .


Using Boto3
~~~~~~~~~~~~~~
After installing boto3

Next, set up credentials (in e.g. ``~/.aws/credentials``):

.. code-block:: ini

    [default]
    aws_access_key_id = YOUR_KEY
    aws_secret_access_key = YOUR_SECRET

Then, set up a default region (in e.g. ``~/.aws/config``):

.. code-block:: ini

   [default]
   region=us-east-1

Other credential configuration methods can be found `here <https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html>`__

Then, from a Python interpreter:

.. code-block:: python

    >>> import boto3
    >>> s3 = boto3.resource('s3')
    >>> for bucket in s3.buckets.all():
            print(bucket.name)

Running Tests
~~~~~~~~~~~~~
You can run tests in all supported Python versions using ``tox``. By default,
it will run all of the unit and functional tests, but you can also specify your own
``pytest`` options. Note that this requires that you have all supported
versions of Python installed, otherwise you must pass ``-e`` or run the
``pytest`` command directly:

.. code-block:: sh

    $ tox
    $ tox -- unit/test_session.py
    $ tox -e py26,py33 -- integration/

You can also run individual tests with your default Python version:

.. code-block:: sh

    $ pytest tests/unit


Getting Help
------------

We use GitHub issues for tracking bugs and feature requests and have limited
bandwidth to address them. Please use these community resources for getting
help:

* Ask a question on `Stack Overflow <https://stackoverflow.com/>`__ and tag it with `boto3 <https://stackoverflow.com/questions/tagged/boto3>`__
* Open a support ticket with `AWS Support <https://console.aws.amazon.com/support/home#/>`__
* If it turns out that you may have found a bug, please `open an issue <https://github.com/boto/boto3/issues/new>`__


Contributing
------------

We value feedback and contributions from our community. Whether it's a bug report, new feature, correction, or additional documentation, we welcome your issues and pull requests. Please read through this `CONTRIBUTING <https://github.com/boto/boto3/blob/develop/CONTRIBUTING.rst>`__ document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your contribution.


Maintenance and Support for SDK Major Versions
----------------------------------------------

Boto3 was made generally available on 06/22/2015 and is currently in the full support phase of the availability life cycle.

For information about maintenance and support for SDK major versions and their underlying dependencies, see the following in the AWS SDKs and Tools Shared Configuration and Credentials Reference Guide:

* `AWS SDKs and Tools Maintenance Policy <https://docs.aws.amazon.com/sdkref/latest/guide/maint-policy.html>`__
* `AWS SDKs and Tools Version Support Matrix <https://docs.aws.amazon.com/sdkref/latest/guide/version-support-matrix.html>`__


More Resources
--------------

* `NOTICE <https://github.com/boto/boto3/blob/develop/NOTICE>`__
* `Changelog <https://github.com/boto/boto3/blob/develop/CHANGELOG.rst>`__
* `License <https://github.com/boto/boto3/blob/develop/LICENSE>`__


"""^^xsd:string ;
    schema1:name "boto3"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.35.21"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/botocore> a software_vocab:Software_package ;
    schema1:description """botocore
========

|Version| |Python| |License|

A low-level interface to a growing number of Amazon Web Services. The
botocore package is the foundation for the
`AWS CLI <https://github.com/aws/aws-cli>`__ as well as
`boto3 <https://github.com/boto/boto3>`__.

Botocore is maintained and published by `Amazon Web Services`_.

Notices
-------

On 2023-12-13, support was dropped for Python 3.7. This follows the
Python Software Foundation `end of support <https://www.python.org/dev/peps/pep-0537/#lifespan>`__
for the runtime which occurred on 2023-06-27.
For more information, see this `blog post <https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/>`__.

.. _`Amazon Web Services`: https://aws.amazon.com/what-is-aws/
.. |Python| image:: https://img.shields.io/pypi/pyversions/botocore.svg?style=flat
    :target: https://pypi.python.org/pypi/botocore/
    :alt: Python Versions
.. |Version| image:: http://img.shields.io/pypi/v/botocore.svg?style=flat
    :target: https://pypi.python.org/pypi/botocore/
    :alt: Package Version
.. |License| image:: http://img.shields.io/pypi/l/botocore.svg?style=flat
    :target: https://github.com/boto/botocore/blob/develop/LICENSE.txt
    :alt: License

Getting Started
---------------
Assuming that you have Python and ``virtualenv`` installed, set up your environment and install the required dependencies like this or you can install the library using ``pip``:

.. code-block:: sh

    $ git clone https://github.com/boto/botocore.git
    $ cd botocore
    $ virtualenv venv
    ...
    $ . venv/bin/activate
    $ pip install -r requirements.txt
    $ pip install -e .

.. code-block:: sh

    $ pip install botocore

Using Botocore
~~~~~~~~~~~~~~
After installing botocore

Next, set up credentials (in e.g. ``~/.aws/credentials``):

.. code-block:: ini

    [default]
    aws_access_key_id = YOUR_KEY
    aws_secret_access_key = YOUR_SECRET

Then, set up a default region (in e.g. ``~/.aws/config``):

.. code-block:: ini

   [default]
   region=us-east-1

Other credentials configuration method can be found `here <https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html>`__

Then, from a Python interpreter:

.. code-block:: python

    >>> import botocore.session
    >>> session = botocore.session.get_session()
    >>> client = session.create_client('ec2')
    >>> print(client.describe_instances())


Getting Help
------------

We use GitHub issues for tracking bugs and feature requests and have limited
bandwidth to address them. Please use these community resources for getting
help. Please note many of the same resources available for ``boto3`` are
applicable for ``botocore``:

* Ask a question on `Stack Overflow <https://stackoverflow.com/>`__ and tag it with `boto3 <https://stackoverflow.com/questions/tagged/boto3>`__
* Open a support ticket with `AWS Support <https://console.aws.amazon.com/support/home#/>`__
* If it turns out that you may have found a bug, please `open an issue <https://github.com/boto/botocore/issues/new/choose>`__


Contributing
------------

We value feedback and contributions from our community. Whether it's a bug report, new feature, correction, or additional documentation, we welcome your issues and pull requests. Please read through this `CONTRIBUTING <https://github.com/boto/botocore/blob/develop/CONTRIBUTING.rst>`__ document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your contribution.


Maintenance and Support for SDK Major Versions
----------------------------------------------

Botocore was made generally available on 06/22/2015 and is currently in the full support phase of the availability life cycle.

For information about maintenance and support for SDK major versions and their underlying dependencies, see the following in the AWS SDKs and Tools Reference Guide:

* `AWS SDKs and Tools Maintenance Policy <https://docs.aws.amazon.com/sdkref/latest/guide/maint-policy.html>`__
* `AWS SDKs and Tools Version Support Matrix <https://docs.aws.amazon.com/sdkref/latest/guide/version-support-matrix.html>`__


More Resources
--------------

* `NOTICE <https://github.com/boto/botocore/blob/develop/NOTICE>`__
* `Changelog <https://github.com/boto/botocore/blob/develop/CHANGELOG.rst>`__
* `License <https://github.com/boto/botocore/blob/develop/LICENSE.txt>`__


"""^^xsd:string ;
    schema1:name "botocore"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.35.21"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/capycli> a software_vocab:Software_package ;
    schema1:description """<!--
# SPDX-FileCopyrightText: (c) 2018-2024 Siemens
# SPDX-License-Identifier: MIT
-->

![Header_Image](images/Github-social-capycli.png)

# CaPyCli - Clearing Automation Python Command Line Tool for SW360

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/sw360/capycli/blob/main/License.md)
[![PyPI](https://shields.io/pypi/v/capycli)](https://pypi.org/project/capycli/)
[![Python Version](https://img.shields.io/badge/python-3.8%2C3.9%2C3.10%2C3.11-yellow?logo=python)](https://www.python.org/doc/versions/)
[![Static Checks](https://github.com/sw360/capycli/actions/workflows/static-checks.yml/badge.svg)](https://github.com/sw360/capycli/actions/workflows/static-checks.yml)
[![Unit Tests](https://github.com/sw360/capycli/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/sw360/capycli/actions/workflows/unit-tests.yml)
[![Coverage](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/tngraf/c8f15831ecdcf6e86ab2b69cbb2d4f89/raw/df1a91c074c5ee34dc1f0dcf82bc0e76e39b5b4e/capycli-cobertura-coverage.json&color=green)](https://github.com/sw360/capycli/actions/workflows/unit-tests.yml)
[![SBOM](https://img.shields.io/badge/SBOM-CycloneDX-brightgreen)](https://github.com/tngraf/Tethys.Dgml/blob/master/SBOM/sbom.cyclonedx.xml)
[![REUSE status](https://api.reuse.software/badge/git.fsfe.org/reuse/api)](https://api.reuse.software/info/git.fsfe.org/reuse/api)

Python 3 scripts to allow license clearing automation using the
[SW360](https://github.com/eclipse/sw360) software catalogue.

## What is SW360?

[SW360](https://github.com/eclipse/sw360) is a software component catalogue application designed to
provide a central place for sharing information about software components used by an organization.
It is designed to neatly integrate into existing infrastructures related to the management of
software artifacts and projects by providing separate backend services for distinct tasks and a set
of portlets to access these services. A complete deployment unit exists (vagrant box or docker
container) that contains a complete configuration of all services and portlets.

Companies like Cariad, Siemens or Toshiba use SW360 to track their use of third party software components.

## Why CaPyCli?

SW360 is for software developers and software developers love to automate tasks. The SW360 user
interface is nice if you want to check a project or search for a single component. But if you have
a project with a JavaScript frontend and hundreds of components, you do not want to add all of them
manually. You want to be able to determine your software bill of materials (SBOM) and you want to
map this SBOM to the information that is already available of SW360.

CaPyCli allows you to

* determine your list of dependencies, your software bill of materials (SBOM)
* determine meta-data for the SBOM items and download source files
* map an SBOM to the data available on SW360
* create all missing components and releases
* create a project that contains all releases of your SBOM
* track the progress on license compliance checks
* show information about the project and its releases
* show information about export control information and security vulnerabilities (if tracked via SW360)

## Basic Syntax

```code
CaPyCli command [sub-command...] [options]

Commands and Sub-Commands
    getdependencies     dependency detection specific commands
        Nuget             determine dependencies for a .Net/Nuget project
        Python            determine dependencies for a Python project
        Javascript        determine dependencies for a JavaScript project
        MavenPom          determine dependencies for a Java/Maven project using the pom.xml file
        MavenList         determine dependencies for a Java/Maven project using a Maven command

    bom                 bill of material (SBOM) specific commands
        Show              display contents of a SBOM
        Convert           convert SBOM formats
        Filter            apply filter file to a SBOM
        Check             check that all releases in the SBOM exist on target SW360 instance
        CheckItemStatus   show additional information about SBOM items on SW360
        Map               map a given SBOM to data on SW360
        CreateReleases    create new releases for existing components on SW360
        CreateComponents  create new components and releases on SW360 (use with care!)
        DownloadSources   download source files from the URL specified in the SBOM
        Granularity       check a bill of material for potential component granularity issues
        Diff              compare two bills of material.
        Merge             merge two bills of material.
        Findsources       determine the source code for SBOM items.

    mapping
        ToHtml            create a HTML page showing the mapping result
        ToXlsx            create an Excel sheet showing the mapping result

    moverview
        ToHtml            create a HTML page showing the mapping result overview
        ToXlsx            create an Excel sheet showing the mapping result overview

    project
        Find              find a project by name
        Prerequisites     checks whether all prerequisites for a successful
                          software clearing are fulfilled
        Show              show project details
        Licenses          show licenses of all cleared compponents
        Create            create or update a project on SW360
        Update            update an exiting project, preserving linked releases
        GetLicenseInfo    get license info of all project components
        CreateReadme      create a Readme_OSS
        Vulnerabilities   show security vulnerabilities of a project
        ECC               Show export control status of a project

Options:
  command                                           command and subcommand to process
  -h, --help                                        show a help message and exit
  -i INPUTFILE, --inputfile INPUTFILE               input file to read from
  -ri RAW_INPUT, --raw-input RAW_INPUT              raw data input file to parse repository urls
  -o OUTPUTFILE, --outputfile OUTPUTFILE            output file to write to
  -filterfile FILTERFILE                            filter file to use
  -v VERBOSE                                        be verbose
  -t SW360_TOKEN, --token SW360_TOKEN               use this token for access to SW360
  -oa, --oauth2                                     this is an oauth2 token
  -url SW360_URL                                    use this URL for access to SW360
  --nocache NOCACHE                                 do not use component cache
  -cf CACHEFILE, --cachefile CACHEFILE              cache file name to use
  -rc REFRESH_CACHE, --refresh_cache REFRESH_CACHE  refresh component cache
  -sc, --similar                                    look for components with similar name
  -ov CREATE_OVERVIEW, --overview CREATE_OVERVIEW   create an mapping overview JSON file
  -mr WRITE_MAPRESULT, --mapresult WRITE_MAPRESULT  create a JSON file with the mapping details
  -name                                             name of the project
  -version                                          version of the project
  -id ID                                            SW360 id of the project, supersedes name and 
                                                    version parameters
  -ncli NCLI, --no-overwrite-cli NCLI               do not overwrite existing CLI files
  -nconf NCONF, --no-overwrite-config NCONF         do not overwrite an existing configuration file
  -dest DESTINATION, --destination DESTINATION      the destination folder
  -source SOURCE                                    source folder or additional source file
  --dbx DBX                                         relaxed handling of debian version numbers
  --download                                        enable automatic download of missing sources
  --search-meta-data SEARCH_META_DATA               search for component meta-data
  -old-version OLD_VERSION                          previous version
  -ex                                               show exit code
  -rr RESULT_REQUIRED                               there must be a clearing result available
  -xml XML                                          use XML format
  -package-source PACKAGE_SOURCE                    URL of the package manager to use
  -all                                              show/use all items
  -format FORMAT                                    format to use (text, json, xml)
  -fe FORCE_EXIT, --forceexit FORCE_EXIT            force a specific exit code
  -m MODE, --mode MODE                              specific mode for some commands
  -if INPUTFORMAT                                   Specify input file format
  -of OUTPUTFORMAT                                  Specify output file format
  -X DEBUG                                          Enable debug output
  --forceerror FORCE_ERROR                          force an error exit code in case of visual errors
```

## Use Cases

Over the time we implemented more and more commands with more and more parameters.  
We understand that it is hard for beginners to find the right command for the task
they want to do. Have a look at our [Use Case Overview](UseCaseOverview.md).

## Software Clearing Approaches

From time to time there are questions **why** a command has been implemented in this
specific way or why a command exists at all. Not all organization have the same
approach when doing license compliance. Have a look at our
[Software Clearing Approach Overview](SoftwareClearingApproachOverview.md) to see our
approaches.

## Note about Python Dependency Detection

At the moment there is only support for dependencies defined in a `requirements.txt` file.  
Poetry users can create the `requirements.txt` file via

```sh
poetry export --format requirements.txt -o requirements.txt --without-hashes
```

If you are using pipenv, you can create the `requirements.txt` file via

```sh
pipenv lock -r > requirements.txt
```

If your dependencies are defined in `setup.py` you may take a look at
https://dephell.readthedocs.io/cmd-deps-convert.html or
https://github.com/jazzband/pip-tools#example-usage-for-pip-compile to generate
a `requirements.txt` file.

Probably the best solution is if you enhance CaPyCli to support poetry, pipenv or setup.py
directly and open a merge request.

## Examples

### Find project by name

Command:

```sh
capycli project find -name "tr-card"
- or -
python -m capycli project find -name tr-card
```

Result

```sh
CaPyCli - Find a project by name

  Searching for projects by name
    TR-Card, 1.0 => ID = ff697cd18fe178b26fc601b60e00fcdf
```

More examples and usage notes can be found in [examples.md](examples.md).

## Prerequisites

* Python 3
* A SW360 read (and write) token, see next section.

## API Access

Access to the SW360 REST API requires an access token.
The token can be requested on SW360/Preferences/REST API Token.

The scripts in this repository expect, that a valid token
is stored in the environment variable ``SW360ProductionToken``.
Alternatively you can specify a token using the `-t` option.

For proper access to an SW360 instance the correct url must be own.
The SW360 url can be specified on the commandline with the `-url`
parameter, via the environment variable ``SW360ServerUrl`` or in the
config file (`.capycli.cfg`).

## SBOM Format

The software bill of materials (SBOM) is a crucial information for most operations.
There is no common description what a bill of materials should contain.
There are different formats available, for example the SBOM of CyCloneDX,
nevertheless most tools have their own SBOM format.
We have decided also to have our own flavor of CycloneDX, see [SBOM](Readme_BOM.md),
focused on the information we need to handle components, releases and projects
on SW360. It is a simple JSON format. CaPyCli reads or writes exactly the
information that is needed.
Conversion support from or to our SBOM format is available.
For converting CycloneDX (XML) to JSON or for converting SPDX SBOMs, we like
to refer you to the open source tools from [CycloneDX](https://cyclonedx.org/).

## Mapping a SBOM to SW360

SBOM mapping is described in an extra file, see [SBOM Mapping](Readme_Mapping.md).

## Project Management

This is a Python project managed using ```Poetry```.

## Installation

### From PyPi

* using `pip`:

  ```shell
  pip install capycli
  ```

## Copyright & License

Copyright 2018-2024 Siemens

This program and the accompanying materials are made
available under the terms of the MIT License.  
SPDX-License-Identifier: MIT

"""^^xsd:string ;
    schema1:name "capycli"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.5.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/cli-support> a software_vocab:Software_package ;
    schema1:description """# CLI Support for Python

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/sw360/clipython/blob/master/License.md)
[![Python Version](https://img.shields.io/badge/python-3.8%2C3.9%2C3.10%2C3.11-yellow?logo=python)](https://www.python.org/doc/versions/)
[![PyPI](https://shields.io/pypi/v/cli-support)](https://pypi.org/project/cli-support)
[![Static checks](https://github.com/sw360/clipython/actions/workflows/python-package.yml/badge.svg)](https://github.com/sw360/clipython/actions/workflows/python-package.yml)
[![Unit tests](https://github.com/sw360/clipython/actions/workflows/unit-test.yml/badge.svg)](https://github.com/sw360/clipython/actions/workflows/unit-test.yml)

Python library to read and write Component License Information (CLI) files. They can be
created by [FOSSology](https://www.fossology.org) and stored in
[SW360](https://www.eclipse.org/sw360/).

For more information about the CLI file format, please have a look at
[ComponentLicenseInformation.md](ComponentLicenseInformation.md).

## Usage

### Installation

This project is available as [Python package on PyPi.org](https://pypi.org/project/cli-support/).  
Install cli_support and required dependencies:

  ```shell
  pip install cli_support
  ```

### Required Packages

* none

## Using the API

* Start using the API:

  ```python
  import cli_support
  clifile = cli_support.CLI.CliFile()
  clifile.read_from_file("cli_filename")
  ```

## Contribute

* All contributions in form of bug reports, feature requests or merge requests are welcome!
* Please use proper [docstrings](https://realpython.com/documenting-python-code/) to document
  functions and classes.
* Extend the testsuite **poetry run pytest** with the new functions/classes

## Build

### Building Python package

For building the library, you need [Poetry](https://python-poetry.org/). Build is then
simply triggered using

```shell
poetry build
```

This creates the source and wheel files in ```dist/``` subdirectory -- which can then
be uploaded or installed locally using ```pip```.

## Test

Start the complete test suite or a specific test case (and generate coverage report):

```shell
poetry run pytest
```

or

```shell
poetry run coverage run -m pytest
poetry run coverage report -m --omit "*/site-packages/*.py"
poetry run coverage html --omit "*/site-packages/*.py"
```

## Demo

The script ``show_licenses.py`` shows how to use the library to retrieve some information
of a given CLI file.

```shell
python ./show_licenses.py ./test/testfiles/CLIXML_MIT_simple.xml
```

## License

The project is licensed under the MIT license.  
SPDX-License-Identifier: MIT

"""^^xsd:string ;
    schema1:name "cli-support"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.0.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/cyclonedx-bom> a software_vocab:Software_package ;
    schema1:description """# CycloneDX Python SBOM Generation Tool

[![shield_pypi-version]][link_pypi]
[![shield_docker-version]][link_docker]
[![shield_rtfd]][link_rtfd]
[![shield_gh-workflow-test]][link_gh-workflow-test]
[![shield_coverage]][link_codacy]
[![shield_license]][license_file]  
[![shield_website]][link_website]
[![shield_slack]][link_slack]
[![shield_groups]][link_discussion]
[![shield_twitter-follow]][link_twitter]

----

This project provides a runnable Python-based application for generating CycloneDX bill-of-material documents from either:

* Your current Python Environment
* Your project's manifest (e.g. `Pipfile.lock`, `poetry.lock` or `requirements.txt`)
* Conda as a Package Manager

The BOM will contain an aggregate of all your current project's dependencies, or those defined by the manifest you supply.

[CycloneDX](https://cyclonedx.org/) is a lightweight BOM specification that is easily created, human-readable, and simple to parse.

Read the full [documentation][link_rtfd] for more details.

## Installation

Install this from [PyPi.org][link_pypi] using your preferred Python package manager.

Example using `pip`:

```shell
pip install cyclonedx-bom
```

Example using `poetry`:

```shell
poetry add cyclonedx-bom
```

## Usage

Call via one of commands:

```shell
cyclonedx-py
python3 -m cyclonedx_py
```

## Basic usage

```shellSession
$ cyclonedx-py --help
usage: cyclonedx-py [-h] (-c | -cj | -e | -p | -pip | -r) [-i FILE_PATH]
                 [--format {json,xml}] [--schema-version {1.4,1.3,1.2,1.1,1.0}]
                 [-o FILE_PATH] [-F] [-X]

CycloneDX SBOM Generator

optional arguments:
  -h, --help            show this help message and exit
  -c, --conda           Build a SBOM based on the output from `conda list
                        --explicit` or `conda list --explicit --md5`
  -cj, --conda-json     Build a SBOM based on the output from `conda list
                        --json`
  -e, --e, --environment
                        Build a SBOM based on the packages installed in your
                        current Python environment (default)
  -p, --p, --poetry     Build a SBOM based on a Poetry poetry.lock's contents.
                        Use with -i to specify absolute path to a `poetry.lock`
                        you wish to use, else we'll look for one in the
                        current working directory.
  -pip, --pip           Build a SBOM based on a PipEnv Pipfile.lock's
                        contents. Use with -i to specify absolute path to a
                        `Pipfile.lock` you wish to use, else we'll look for
                        one in the current working directory.
  -r, --r, --requirements
                        Build a SBOM based on a requirements.txt's contents.
                        Use with -i to specify absolute path to a
                        `requirements.txt` you wish to use, else we'll look
                        for one in the current working directory.
  -X                    Enable debug output

Input Method:
  Flags to determine how this tool obtains its input

  -i FILE_PATH, --in-file FILE_PATH
                        File to read input from. Use "-" to read from STDIN.

SBOM Output Configuration:
  Choose the output format and schema version

  --format {json,xml}   The output format for your SBOM (default: xml)
  --schema-version {1.4,1.3,1.2,1.1,1.0}
                        The CycloneDX schema version for your SBOM (default:
                        1.4)
  -o FILE_PATH, --o FILE_PATH, --output FILE_PATH
                        Output file path for your SBOM (set to '-' to output
                        to STDOUT)
  -F, --force           If outputting to a file and the stated file already
                        exists, it will be overwritten.
  -pb, --purl-bom-ref   Use a component's PURL for the bom-ref value, instead
                        of a random UUID
```

### Advanced usage and details

See the full [documentation][link_rtfd] for advanced usage and details on input formats, switches and options.

## Python Support

We endeavour to support all functionality for all [current actively supported Python versions](https://www.python.org/downloads/).
However, some features may not be possible/present in older Python versions due to their lack of support.

## Contributing

Feel free to open issues, bugreports or pull requests.  
See the [CONTRIBUTING][contributing_file] file for details.

## Copyright & License

CycloneDX BOM is Copyright (c) OWASP Foundation. All Rights Reserved.  
Permission to modify and redistribute is granted under the terms of the Apache 2.0 license.  
See the [LICENSE][license_file] file for the full license.

[license_file]: https://github.com/CycloneDX/cyclonedx-python/blob/main/LICENSE
[contributing_file]: https://github.com/CycloneDX/cyclonedx-python/blob/main/CONTRIBUTING.md
[link_rtfd]: https://cyclonedx-bom-tool.readthedocs.io/

[shield_gh-workflow-test]: https://img.shields.io/github/actions/workflow/status/CycloneDX/cyclonedx-python/python.yml?branch=main&logo=GitHub&logoColor=white "build"
[shield_rtfd]: https://img.shields.io/readthedocs/cyclonedx-bom-tool?logo=readthedocs&logoColor=white
[shield_pypi-version]: https://img.shields.io/pypi/v/cyclonedx-bom?logo=Python&logoColor=white&label=PyPI "PyPI"
[shield_docker-version]: https://img.shields.io/docker/v/cyclonedx/cyclonedx-python?logo=docker&logoColor=white&label=docker "docker"
[shield_license]: https://img.shields.io/github/license/CycloneDX/cyclonedx-python?logo=open%20source%20initiative&logoColor=white "license"
[shield_website]: https://img.shields.io/badge/https://-cyclonedx.org-blue.svg "homepage"
[shield_slack]: https://img.shields.io/badge/slack-join-blue?logo=Slack&logoColor=white "slack join"
[shield_groups]: https://img.shields.io/badge/discussion-groups.io-blue.svg "groups discussion"
[shield_twitter-follow]: https://img.shields.io/badge/Twitter-follow-blue?logo=Twitter&logoColor=white "twitter follow"
[link_gh-workflow-test]: https://github.com/CycloneDX/cyclonedx-python/actions/workflows/python.yml?query=branch%3Amain
[shield_coverage]: https://img.shields.io/codacy/coverage/682ceda9a1044832a087afb95ae280fe?logo=Codacy&logoColor=white "test coverage"
[link_pypi]: https://pypi.org/project/cyclonedx-bom/
[link_docker]: https://hub.docker.com/r/cyclonedx/cyclonedx-python
[link_codacy]: https://app.codacy.com/gh/CycloneDX/cyclonedx-python
[link_website]: https://cyclonedx.org/
[link_slack]: https://cyclonedx.org/slack/invite
[link_discussion]: https://groups.io/g/CycloneDX
[link_twitter]: https://twitter.com/CycloneDX_Spec

"""^^xsd:string ;
    schema1:name "cyclonedx-bom"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.11.7"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/cyclonedx-python-lib> a software_vocab:Software_package ;
    schema1:description """# Python Library for generating CycloneDX

[![shield_gh-workflow-test]][link_gh-workflow-test]
[![shield_rtfd]][link_rtfd]
[![shield_pypi-version]][link_pypi]
[![shield_conda-forge-version]][link_conda-forge]
[![shield_license]][license_file]  
[![shield_website]][link_website]
[![shield_slack]][link_slack]
[![shield_groups]][link_discussion]
[![shield_twitter-follow]][link_twitter]

----

This CycloneDX module for Python can generate valid CycloneDX bill-of-material document containing an aggregate of all
project dependencies.

This module is not designed for standalone use.

If you're looking for a CycloneDX tool to run to generate (SBOM) software bill-of-materials documents, why not checkout: [CycloneDX Python][cyclonedx-python]

Additionally, the following tool can be used as well (and this library was written to help improve it) [Jake][jake].

Additionally, you can use this module yourself in your application to programmatically generate SBOMs.

CycloneDX is a lightweight BOM specification that is easily created, human-readable, and simple to parse.

View our documentation [here](https://cyclonedx-python-library.readthedocs.io/).

## Python Support

We endeavour to support all functionality for all [current actively supported Python versions](https://www.python.org/downloads/).
However, some features may not be possible/present in older Python versions due to their lack of support.

## Changelog

See our [CHANGELOG][chaneglog_file].

## Contributing

Feel free to open issues, bugreports or pull requests.  
See the [CONTRIBUTING][contributing_file] file for details.

## Copyright & License

CycloneDX Python Lib is Copyright (c) OWASP Foundation. All Rights Reserved.  
Permission to modify and redistribute is granted under the terms of the Apache 2.0 license.  
See the [LICENSE][license_file] file for the full license.

[cyclonedx-python]: https://github.com/CycloneDX/cyclonedx-python
[jake]: https://github.com/sonatype-nexus-community/jake

[license_file]: https://github.com/CycloneDX/cyclonedx-python-lib/blob/master/LICENSE
[chaneglog_file]: https://github.com/CycloneDX/cyclonedx-python-lib/blob/master/CHANGELOG.md
[contributing_file]: https://github.com/CycloneDX/cyclonedx-python-lib/blob/master/CONTRIBUTING.md

[shield_gh-workflow-test]: https://img.shields.io/github/actions/workflow/status/CycloneDX/cyclonedx-python-lib/poetry.yml?branch=main&logo=GitHub&logoColor=white "build"
[shield_pypi-version]: https://img.shields.io/pypi/v/cyclonedx-python-lib?logo=pypi&logoColor=white&label=PyPI "PyPI"
[shield_conda-forge-version]: https://img.shields.io/conda/vn/conda-forge/cyclonedx-python-lib?logo=anaconda&logoColor=white&label=conda-forge "conda-forge"
[shield_rtfd]: https://img.shields.io/readthedocs/cyclonedx-python-library?logo=readthedocs&logoColor=white "Read the Docs"
[shield_license]: https://img.shields.io/github/license/CycloneDX/cyclonedx-python-lib?logo=open%20source%20initiative&logoColor=white "license"
[shield_website]: https://img.shields.io/badge/https://-cyclonedx.org-blue.svg "homepage"
[shield_slack]: https://img.shields.io/badge/slack-join-blue?logo=Slack&logoColor=white "slack join"
[shield_groups]: https://img.shields.io/badge/discussion-groups.io-blue.svg "groups discussion"
[shield_twitter-follow]: https://img.shields.io/badge/Twitter-follow-blue?logo=Twitter&logoColor=white "twitter follow"
[link_gh-workflow-test]: https://github.com/CycloneDX/cyclonedx-python-lib/actions/workflows/poetry.yml?query=branch%3Amain
[link_pypi]: https://pypi.org/project/cyclonedx-python-lib/
[link_conda-forge]: https://anaconda.org/conda-forge/cyclonedx-python-lib
[link_rtfd]: https://cyclonedx-python-library.readthedocs.io/en/latest/?badge=latest
[link_website]: https://cyclonedx.org/
[link_slack]: https://cyclonedx.org/slack/invite
[link_discussion]: https://groups.io/g/CycloneDX
[link_twitter]: https://twitter.com/CycloneDX_Spec

[PEP-508]: https://www.python.org/dev/peps/pep-0508/

"""^^xsd:string ;
    schema1:name "cyclonedx-python-lib"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.1.5"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/decorator> a software_vocab:Software_package ;
    schema1:description """Decorators for Humans
=====================

The goal of the decorator module is to make it easy to define
signature-preserving function decorators and decorator factories.
It also includes an implementation of multiple dispatch and other niceties
(please check the docs). It is released under a two-clauses
BSD license, i.e. basically you can do whatever you want with it but I am not
responsible.

Installation
-------------

If you are lazy, just perform

 ``$ pip install decorator``

which will install just the module on your system.

If you prefer to install the full distribution from source, including
the documentation, clone the `GitHub repo`_ or download the tarball_, unpack it and run

 ``$ pip install .``

in the main directory, possibly as superuser.

.. _tarball: https://pypi.org/project/decorator/#files
.. _GitHub repo: https://github.com/micheles/decorator

Testing
--------

If you have the source code installation you can run the tests with

 `$ python src/tests/test.py -v`

or (if you have setuptools installed)

 `$ python setup.py test`

Notice that you may run into trouble if in your system there
is an older version of the decorator module; in such a case remove the
old version. It is safe even to copy the module `decorator.py` over
an existing one, since we kept backward-compatibility for a long time.

Repository
---------------

The project is hosted on GitHub. You can look at the source here:

 https://github.com/micheles/decorator

Documentation
---------------

The documentation has been moved to https://github.com/micheles/decorator/blob/master/docs/documentation.md

From there you can get a PDF version by simply using the print
functionality of your browser.

Here is the documentation for previous versions of the module:

https://github.com/micheles/decorator/blob/4.3.2/docs/tests.documentation.rst
https://github.com/micheles/decorator/blob/4.2.1/docs/tests.documentation.rst
https://github.com/micheles/decorator/blob/4.1.2/docs/tests.documentation.rst
https://github.com/micheles/decorator/blob/4.0.0/documentation.rst
https://github.com/micheles/decorator/blob/3.4.2/documentation.rst

For the impatient
-----------------

Here is an example of how to define a family of decorators tracing slow
operations:

.. code-block:: python

   from decorator import decorator

   @decorator
   def warn_slow(func, timelimit=60, *args, **kw):
       t0 = time.time()
       result = func(*args, **kw)
       dt = time.time() - t0
       if dt > timelimit:
           logging.warn('%s took %d seconds', func.__name__, dt)
       else:
           logging.info('%s took %d seconds', func.__name__, dt)
       return result

   @warn_slow  # warn if it takes more than 1 minute
   def preprocess_input_files(inputdir, tempdir):
       ...

   @warn_slow(timelimit=600)  # warn if it takes more than 10 minutes
   def run_calculation(tempdir, outdir):
       ...

Enjoy!


"""^^xsd:string ;
    schema1:name "decorator"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "5.1.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/defusedxml> a software_vocab:Software_package ;
    schema1:description """===================================================
defusedxml -- defusing XML bombs and other exploits
===================================================

.. image:: https://img.shields.io/pypi/v/defusedxml.svg
    :target: https://pypi.org/project/defusedxml/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/pyversions/defusedxml.svg
    :target: https://pypi.org/project/defusedxml/
    :alt: Supported Python versions

.. image:: https://travis-ci.org/tiran/defusedxml.svg?branch=master
    :target: https://travis-ci.org/tiran/defusedxml
    :alt: Travis CI

.. image:: https://codecov.io/github/tiran/defusedxml/coverage.svg?branch=master
    :target: https://codecov.io/github/tiran/defusedxml?branch=master
    :alt: codecov

.. image:: https://img.shields.io/pypi/dm/defusedxml.svg
    :target: https://pypistats.org/packages/defusedxml
    :alt: PyPI downloads

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black
    :alt: Code style: black

..

    "It's just XML, what could probably go wrong?"

Christian Heimes <christian@python.org>

Synopsis
========

The results of an attack on a vulnerable XML library can be fairly dramatic.
With just a few hundred **Bytes** of XML data an attacker can occupy several
**Gigabytes** of memory within **seconds**. An attacker can also keep
CPUs busy for a long time with a small to medium size request. Under some
circumstances it is even possible to access local files on your
server, to circumvent a firewall, or to abuse services to rebound attacks to
third parties.

The attacks use and abuse less common features of XML and its parsers. The
majority of developers are unacquainted with features such as processing
instructions and entity expansions that XML inherited from SGML. At best
they know about ``<!DOCTYPE>`` from experience with HTML but they are not
aware that a document type definition (DTD) can generate an HTTP request
or load a file from the file system.

None of the issues is new. They have been known for a long time. Billion
laughs was first reported in 2003. Nevertheless some XML libraries and
applications are still vulnerable and even heavy users of XML are
surprised by these features. It's hard to say whom to blame for the
situation. It's too short sighted to shift all blame on XML parsers and
XML libraries for using insecure default settings. After all they
properly implement XML specifications. Application developers must not rely
that a library is always configured for security and potential harmful data
by default.


.. contents:: Table of Contents
   :depth: 2


Attack vectors
==============

billion laughs / exponential entity expansion
---------------------------------------------

The `Billion Laughs`_ attack -- also known as exponential entity expansion --
uses multiple levels of nested entities. The original example uses 9 levels
of 10 expansions in each level to expand the string ``lol`` to a string of
3 * 10 :sup:`9` bytes, hence the name "billion laughs". The resulting string
occupies 3 GB (2.79 GiB) of memory; intermediate strings require additional
memory. Because most parsers don't cache the intermediate step for every
expansion it is repeated over and over again. It increases the CPU load even
more.

An XML document of just a few hundred bytes can disrupt all services on a
machine within seconds.

Example XML::

    <!DOCTYPE xmlbomb [
    <!ENTITY a "1234567890" >
    <!ENTITY b "&a;&a;&a;&a;&a;&a;&a;&a;">
    <!ENTITY c "&b;&b;&b;&b;&b;&b;&b;&b;">
    <!ENTITY d "&c;&c;&c;&c;&c;&c;&c;&c;">
    ]>
    <bomb>&d;</bomb>


quadratic blowup entity expansion
---------------------------------

A quadratic blowup attack is similar to a `Billion Laughs`_ attack; it abuses
entity expansion, too. Instead of nested entities it repeats one large entity
with a couple of thousand chars over and over again. The attack isn't as
efficient as the exponential case but it avoids triggering countermeasures of
parsers against heavily nested entities. Some parsers limit the depth and
breadth of a single entity but not the total amount of expanded text
throughout an entire XML document.

A medium-sized XML document with a couple of hundred kilobytes can require a
couple of hundred MB to several GB of memory. When the attack is combined
with some level of nested expansion an attacker is able to achieve a higher
ratio of success.

::

    <!DOCTYPE bomb [
    <!ENTITY a "xxxxxxx... a couple of ten thousand chars">
    ]>
    <bomb>&a;&a;&a;... repeat</bomb>


external entity expansion (remote)
----------------------------------

Entity declarations can contain more than just text for replacement. They can
also point to external resources by public identifiers or system identifiers.
System identifiers are standard URIs. When the URI is a URL (e.g. a
``http://`` locator) some parsers download the resource from the remote
location and embed them into the XML document verbatim.

Simple example of a parsed external entity::

    <!DOCTYPE external [
    <!ENTITY ee SYSTEM "http://www.python.org/some.xml">
    ]>
    <root>&ee;</root>

The case of parsed external entities works only for valid XML content. The
XML standard also supports unparsed external entities with a
``NData declaration``.

External entity expansion opens the door to plenty of exploits. An attacker
can abuse a vulnerable XML library and application to rebound and forward
network requests with the IP address of the server. It highly depends
on the parser and the application what kind of exploit is possible. For
example:

* An attacker can circumvent firewalls and gain access to restricted
  resources as all the requests are made from an internal and trustworthy
  IP address, not from the outside.
* An attacker can abuse a service to attack, spy on or DoS your servers but
  also third party services. The attack is disguised with the IP address of
  the server and the attacker is able to utilize the high bandwidth of a big
  machine.
* An attacker can exhaust additional resources on the machine, e.g. with
  requests to a service that doesn't respond or responds with very large
  files.
* An attacker may gain knowledge, when, how often and from which IP address
  an XML document is accessed.
* An attacker could send mail from inside your network if the URL handler
  supports ``smtp://`` URIs.


external entity expansion (local file)
--------------------------------------

External entities with references to local files are a sub-case of external
entity expansion. It's listed as an extra attack because it deserves extra
attention. Some XML libraries such as lxml disable network access by default
but still allow entity expansion with local file access by default. Local
files are either referenced with a ``file://`` URL or by a file path (either
relative or absolute).

An attacker may be able to access and download all files that can be read by
the application process. This may include critical configuration files, too.

::

    <!DOCTYPE external [
    <!ENTITY ee SYSTEM "file:///PATH/TO/simple.xml">
    ]>
    <root>&ee;</root>


DTD retrieval
-------------

This case is similar to external entity expansion, too. Some XML libraries
like Python's xml.dom.pulldom retrieve document type definitions from remote
or local locations. Several attack scenarios from the external entity case
apply to this issue as well.

::

    <?xml version="1.0" encoding="utf-8"?>
    <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
      "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
    <html>
        <head/>
        <body>text</body>
    </html>


Python XML Libraries
====================

.. csv-table:: vulnerabilities and features
   :header: "kind", "sax", "etree", "minidom", "pulldom", "xmlrpc", "lxml", "genshi"
   :widths: 24, 7, 8, 8, 7, 8, 8, 8
   :stub-columns: 0

   "billion laughs", "**True**", "**True**", "**True**", "**True**", "**True**", "False (1)", "False (5)"
   "quadratic blowup", "**True**", "**True**", "**True**", "**True**", "**True**", "**True**", "False (5)"
   "external entity expansion (remote)", "**True**", "False (3)", "False (4)", "**True**", "false", "False (1)", "False (5)"
   "external entity expansion (local file)", "**True**", "False (3)", "False (4)", "**True**", "false", "**True**", "False (5)"
   "DTD retrieval", "**True**", "False", "False", "**True**", "false", "False (1)", "False"
   "gzip bomb", "False", "False", "False", "False", "**True**", "**partly** (2)", "False"
   "xpath support (7)", "False", "False", "False", "False", "False", "**True**", "False"
   "xsl(t) support (7)", "False", "False", "False", "False", "False", "**True**", "False"
   "xinclude support (7)", "False", "**True** (6)", "False", "False", "False", "**True** (6)", "**True**"
   "C library", "expat", "expat", "expat", "expat", "expat", "libxml2", "expat"

1. Lxml is protected against billion laughs attacks and doesn't do network
   lookups by default.
2. libxml2 and lxml are not directly vulnerable to gzip decompression bombs
   but they don't protect you against them either.
3. xml.etree doesn't expand entities and raises a ParserError when an entity
   occurs.
4. minidom doesn't expand entities and simply returns the unexpanded entity
   verbatim.
5. genshi.input of genshi 0.6 doesn't support entity expansion and raises a
   ParserError when an entity occurs.
6. Library has (limited) XInclude support but requires an additional step to
   process inclusion.
7. These are features but they may introduce exploitable holes, see
   `Other things to consider`_


Settings in standard library
----------------------------


xml.sax.handler Features
........................

feature_external_ges (http://xml.org/sax/features/external-general-entities)
  disables external entity expansion

feature_external_pes (http://xml.org/sax/features/external-parameter-entities)
  the option is ignored and doesn't modify any functionality

DOM xml.dom.xmlbuilder.Options
..............................

external_parameter_entities
  ignored

external_general_entities
  ignored

external_dtd_subset
  ignored

entities
  unsure


defusedxml
==========

The `defusedxml package`_ (`defusedxml on PyPI`_)
contains several Python-only workarounds and fixes
for denial of service and other vulnerabilities in Python's XML libraries.
In order to benefit from the protection you just have to import and use the
listed functions / classes from the right defusedxml module instead of the
original module. Merely `defusedxml.xmlrpc`_ is implemented as monkey patch.

Instead of::

   >>> from xml.etree.ElementTree import parse
   >>> et = parse(xmlfile)

alter code to::

   >>> from defusedxml.ElementTree import parse
   >>> et = parse(xmlfile)

Additionally the package has an **untested** function to monkey patch
all stdlib modules with ``defusedxml.defuse_stdlib()``.

All functions and parser classes accept three additional keyword arguments.
They return either the same objects as the original functions or compatible
subclasses.

forbid_dtd (default: False)
  disallow XML with a ``<!DOCTYPE>`` processing instruction and raise a
  *DTDForbidden* exception when a DTD processing instruction is found.

forbid_entities (default: True)
  disallow XML with ``<!ENTITY>`` declarations inside the DTD and raise an
  *EntitiesForbidden* exception when an entity is declared.

forbid_external (default: True)
  disallow any access to remote or local resources in external entities
  or DTD and raising an *ExternalReferenceForbidden* exception when a DTD
  or entity references an external resource.


defusedxml (package)
--------------------

DefusedXmlException, DTDForbidden, EntitiesForbidden,
ExternalReferenceForbidden, NotSupportedError

defuse_stdlib() (*experimental*)


defusedxml.cElementTree
-----------------------

**NOTE** ``defusedxml.cElementTree`` is deprecated and will be removed in a
future release. Import from ``defusedxml.ElementTree`` instead.

parse(), iterparse(), fromstring(), XMLParser


defusedxml.ElementTree
-----------------------

parse(), iterparse(), fromstring(), XMLParser


defusedxml.expatreader
----------------------

create_parser(), DefusedExpatParser


defusedxml.sax
--------------

parse(), parseString(), make_parser()


defusedxml.expatbuilder
-----------------------

parse(), parseString(), DefusedExpatBuilder, DefusedExpatBuilderNS


defusedxml.minidom
------------------

parse(), parseString()


defusedxml.pulldom
------------------

parse(), parseString()


defusedxml.xmlrpc
-----------------

The fix is implemented as monkey patch for the stdlib's xmlrpc package (3.x)
or xmlrpclib module (2.x). The function `monkey_patch()` enables the fixes,
`unmonkey_patch()` removes the patch and puts the code in its former state.

The monkey patch protects against XML related attacks as well as
decompression bombs and excessively large requests or responses. The default
setting is 30 MB for requests, responses and gzip decompression. You can
modify the default by changing the module variable `MAX_DATA`. A value of
`-1` disables the limit.


defusedxml.lxml
---------------

**DEPRECATED** The module is deprecated and will be removed in a future
release.

The module acts as an *example* how you could protect code that uses
lxml.etree. It implements a custom Element class that filters out
Entity instances, a custom parser factory and a thread local storage for
parser instances. It also has a check_docinfo() function which inspects
a tree for internal or external DTDs and entity declarations. In order to
check for entities lxml > 3.0 is required.

parse(), fromstring()
RestrictedElement, GlobalParserTLS, getDefaultParser(), check_docinfo()


defusedexpat
============

The `defusedexpat package`_ (`defusedexpat on PyPI`_)
comes with binary extensions and a
`modified expat`_ library instead of the standard `expat parser`_. It's
basically a stand-alone version of the patches for Python's standard
library C extensions.

Modifications in expat
----------------------

new definitions::

  XML_BOMB_PROTECTION
  XML_DEFAULT_MAX_ENTITY_INDIRECTIONS
  XML_DEFAULT_MAX_ENTITY_EXPANSIONS
  XML_DEFAULT_RESET_DTD

new XML_FeatureEnum members::

  XML_FEATURE_MAX_ENTITY_INDIRECTIONS
  XML_FEATURE_MAX_ENTITY_EXPANSIONS
  XML_FEATURE_IGNORE_DTD

new XML_Error members::

  XML_ERROR_ENTITY_INDIRECTIONS
  XML_ERROR_ENTITY_EXPANSION

new API functions::

  int XML_GetFeature(XML_Parser parser,
                     enum XML_FeatureEnum feature,
                     long *value);
  int XML_SetFeature(XML_Parser parser,
                     enum XML_FeatureEnum feature,
                     long value);
  int XML_GetFeatureDefault(enum XML_FeatureEnum feature,
                            long *value);
  int XML_SetFeatureDefault(enum XML_FeatureEnum feature,
                            long value);

XML_FEATURE_MAX_ENTITY_INDIRECTIONS
   Limit the amount of indirections that are allowed to occur during the
   expansion of a nested entity. A counter starts when an entity reference
   is encountered. It resets after the entity is fully expanded. The limit
   protects the parser against exponential entity expansion attacks (aka
   billion laughs attack). When the limit is exceeded the parser stops and
   fails with `XML_ERROR_ENTITY_INDIRECTIONS`.
   A value of 0 disables the protection.

   Supported range
     0 .. UINT_MAX
   Default
     40

XML_FEATURE_MAX_ENTITY_EXPANSIONS
   Limit the total length of all entity expansions throughout the entire
   document. The lengths of all entities are accumulated in a parser variable.
   The setting protects against quadratic blowup attacks (lots of expansions
   of a large entity declaration). When the sum of all entities exceeds
   the limit, the parser stops and fails with `XML_ERROR_ENTITY_EXPANSION`.
   A value of 0 disables the protection.

   Supported range
     0 .. UINT_MAX
   Default
     8 MiB

XML_FEATURE_RESET_DTD
   Reset all DTD information after the <!DOCTYPE> block has been parsed. When
   the flag is set (default: false) all DTD information after the
   endDoctypeDeclHandler has been called. The flag can be set inside the
   endDoctypeDeclHandler. Without DTD information any entity reference in
   the document body leads to `XML_ERROR_UNDEFINED_ENTITY`.

   Supported range
     0, 1
   Default
     0


How to avoid XML vulnerabilities
================================

Best practices
--------------

* Don't allow DTDs
* Don't expand entities
* Don't resolve externals
* Limit parse depth
* Limit total input size
* Limit parse time
* Favor a SAX or iterparse-like parser for potential large data
* Validate and properly quote arguments to XSL transformations and
  XPath queries
* Don't use XPath expression from untrusted sources
* Don't apply XSL transformations that come untrusted sources

(based on Brad Hill's `Attacking XML Security`_)


Other things to consider
========================

XML, XML parsers and processing libraries have more features and possible
issue that could lead to DoS vulnerabilities or security exploits in
applications. I have compiled an incomplete list of theoretical issues that
need further research and more attention. The list is deliberately pessimistic
and a bit paranoid, too. It contains things that might go wrong under daffy
circumstances.


attribute blowup / hash collision attack
----------------------------------------

XML parsers may use an algorithm with quadratic runtime O(n :sup:`2`) to
handle attributes and namespaces. If it uses hash tables (dictionaries) to
store attributes and namespaces the implementation may be vulnerable to
hash collision attacks, thus reducing the performance to O(n :sup:`2`) again.
In either case an attacker is able to forge a denial of service attack with
an XML document that contains thousands upon thousands of attributes in
a single node.

I haven't researched yet if expat, pyexpat or libxml2 are vulnerable.


decompression bomb
------------------

The issue of decompression bombs (aka `ZIP bomb`_) apply to all XML libraries
that can parse compressed XML stream like gzipped HTTP streams or LZMA-ed
files. For an attacker it can reduce the amount of transmitted data by three
magnitudes or more. Gzip is able to compress 1 GiB zeros to roughly 1 MB,
lzma is even better::

    $ dd if=/dev/zero bs=1M count=1024 | gzip > zeros.gz
    $ dd if=/dev/zero bs=1M count=1024 | lzma -z > zeros.xy
    $ ls -sh zeros.*
    1020K zeros.gz
     148K zeros.xy

None of Python's standard XML libraries decompress streams except for
``xmlrpclib``. The module is vulnerable <https://bugs.python.org/issue16043>
to decompression bombs.

lxml can load and process compressed data through libxml2 transparently.
libxml2 can handle even very large blobs of compressed data efficiently
without using too much memory. But it doesn't protect applications from
decompression bombs. A carefully written SAX or iterparse-like approach can
be safe.


Processing Instruction
----------------------

`PI`_'s like::

  <?xml-stylesheet type="text/xsl" href="style.xsl"?>

may impose more threats for XML processing. It depends if and how a
processor handles processing instructions. The issue of URL retrieval with
network or local file access apply to processing instructions, too.


Other DTD features
------------------

`DTD`_ has more features like ``<!NOTATION>``. I haven't researched how
these features may be a security threat.


XPath
-----

XPath statements may introduce DoS vulnerabilities. Code should never execute
queries from untrusted sources. An attacker may also be able to create an XML
document that makes certain XPath queries costly or resource hungry.


XPath injection attacks
-----------------------

XPath injeciton attacks pretty much work like SQL injection attacks.
Arguments to XPath queries must be quoted and validated properly, especially
when they are taken from the user. The page `Avoid the dangers of XPath injection`_
list some ramifications of XPath injections.

Python's standard library doesn't have XPath support. Lxml supports
parameterized XPath queries which does proper quoting. You just have to use
its xpath() method correctly::

   # DON'T
   >>> tree.xpath("/tag[@id='%s']" % value)

   # instead do
   >>> tree.xpath("/tag[@id=$tagid]", tagid=name)


XInclude
--------

`XML Inclusion`_ is another way to load and include external files::

   <root xmlns:xi="http://www.w3.org/2001/XInclude">
     <xi:include href="filename.txt" parse="text" />
   </root>

This feature should be disabled when XML files from an untrusted source are
processed. Some Python XML libraries and libxml2 support XInclude but don't
have an option to sandbox inclusion and limit it to allowed directories.


XMLSchema location
------------------

A validating XML parser may download schema files from the information in a
``xsi:schemaLocation`` attribute.

::

  <ead xmlns="urn:isbn:1-931666-22-9"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="urn:isbn:1-931666-22-9 http://www.loc.gov/ead/ead.xsd">
  </ead>


XSL Transformation
------------------

You should keep in mind that XSLT is a Turing complete language. Never
process XSLT code from unknown or untrusted source! XSLT processors may
allow you to interact with external resources in ways you can't even imagine.
Some processors even support extensions that allow read/write access to file
system, access to JRE objects or scripting with Jython.

Example from `Attacking XML Security`_ for Xalan-J::

    <xsl:stylesheet version="1.0"
     xmlns:xsl="http://www.w3.org/1999/XSL/Transform"
     xmlns:rt="http://xml.apache.org/xalan/java/java.lang.Runtime"
     xmlns:ob="http://xml.apache.org/xalan/java/java.lang.Object"
     exclude-result-prefixes= "rt ob">
     <xsl:template match="/">
       <xsl:variable name="runtimeObject" select="rt:getRuntime()"/>
       <xsl:variable name="command"
         select="rt:exec($runtimeObject, &apos;c:\\Windows\\system32\\cmd.exe&apos;)"/>
       <xsl:variable name="commandAsString" select="ob:toString($command)"/>
       <xsl:value-of select="$commandAsString"/>
     </xsl:template>
    </xsl:stylesheet>


Related CVEs
============

CVE-2013-1664
  Unrestricted entity expansion induces DoS vulnerabilities in Python XML
  libraries (XML bomb)

CVE-2013-1665
  External entity expansion in Python XML libraries inflicts potential
  security flaws and DoS vulnerabilities


Other languages / frameworks
=============================

Several other programming languages and frameworks are vulnerable as well. A
couple of them are affected by the fact that libxml2 up to 2.9.0 has no
protection against quadratic blowup attacks. Most of them have potential
dangerous default settings for entity expansion and external entities, too.

Perl
----

Perl's XML::Simple is vulnerable to quadratic entity expansion and external
entity expansion (both local and remote).


Ruby
----

Ruby's REXML document parser is vulnerable to entity expansion attacks
(both quadratic and exponential) but it doesn't do external entity
expansion by default. In order to counteract entity expansion you have to
disable the feature::

  REXML::Document.entity_expansion_limit = 0

libxml-ruby and hpricot don't expand entities in their default configuration.


PHP
---

PHP's SimpleXML API is vulnerable to quadratic entity expansion and loads
entities from local and remote resources. The option ``LIBXML_NONET`` disables
network access but still allows local file access. ``LIBXML_NOENT`` seems to
have no effect on entity expansion in PHP 5.4.6.


C# / .NET / Mono
----------------

Information in `XML DoS and Defenses (MSDN)`_ suggest that .NET is
vulnerable with its default settings. The article contains code snippets
how to create a secure XML reader::

  XmlReaderSettings settings = new XmlReaderSettings();
  settings.ProhibitDtd = false;
  settings.MaxCharactersFromEntities = 1024;
  settings.XmlResolver = null;
  XmlReader reader = XmlReader.Create(stream, settings);


Java
----

Untested. The documentation of Xerces and its `Xerces SecurityMananger`_
sounds like Xerces is also vulnerable to billion laugh attacks with its
default settings. It also does entity resolving when an
``org.xml.sax.EntityResolver`` is configured. I'm not yet sure about the
default setting here.

Java specialists suggest to have a custom builder factory::

  DocumentBuilderFactory builderFactory = DocumentBuilderFactory.newInstance();
  builderFactory.setXIncludeAware(False);
  builderFactory.setExpandEntityReferences(False);
  builderFactory.setFeature(XMLConstants.FEATURE_SECURE_PROCESSING, True);
  # either
  builderFactory.setFeature("http://apache.org/xml/features/disallow-doctype-decl", True);
  # or if you need DTDs
  builderFactory.setFeature("http://xml.org/sax/features/external-general-entities", False);
  builderFactory.setFeature("http://xml.org/sax/features/external-parameter-entities", False);
  builderFactory.setFeature("http://apache.org/xml/features/nonvalidating/load-external-dtd", False);
  builderFactory.setFeature("http://apache.org/xml/features/nonvalidating/load-dtd-grammar", False);


TODO
====

* DOM: Use xml.dom.xmlbuilder options for entity handling
* SAX: take feature_external_ges and feature_external_pes (?) into account
* test experimental monkey patching of stdlib modules
* improve documentation


License
=======

Copyright (c) 2013-2017 by Christian Heimes <christian@python.org>

Licensed to PSF under a Contributor Agreement.

See https://www.python.org/psf/license for licensing details.


Acknowledgements
================

Brett Cannon (Python Core developer)
  review and code cleanup

Antoine Pitrou (Python Core developer)
  code review

Aaron Patterson, Ben Murphy and Michael Koziarski (Ruby community)
  Many thanks to Aaron, Ben and Michael from the Ruby community for their
  report and assistance.

Thierry Carrez (OpenStack)
  Many thanks to Thierry for his report to the Python Security Response
  Team on behalf of the OpenStack security team.

Carl Meyer (Django)
  Many thanks to Carl for his report to PSRT on behalf of the Django security
  team.

Daniel Veillard (libxml2)
  Many thanks to Daniel for his insight and assistance with libxml2.

semantics GmbH (https://www.semantics.de/)
  Many thanks to my employer semantics for letting me work on the issue
  during working hours as part of semantics's open source initiative.


References
==========

* `XML DoS and Defenses (MSDN)`_
* `Billion Laughs`_ on Wikipedia
* `ZIP bomb`_ on Wikipedia
* `Configure SAX parsers for secure processing`_
* `Testing for XML Injection`_

.. _defusedxml package: https://github.com/tiran/defusedxml
.. _defusedxml on PyPI: https://pypi.python.org/pypi/defusedxml
.. _defusedexpat package: https://github.com/tiran/defusedexpat
.. _defusedexpat on PyPI: https://pypi.python.org/pypi/defusedexpat
.. _modified expat: https://github.com/tiran/expat
.. _expat parser: http://expat.sourceforge.net/
.. _Attacking XML Security: https://www.isecpartners.com/media/12976/iSEC-HILL-Attacking-XML-Security-bh07.pdf
.. _Billion Laughs: https://en.wikipedia.org/wiki/Billion_laughs
.. _XML DoS and Defenses (MSDN): https://msdn.microsoft.com/en-us/magazine/ee335713.aspx
.. _ZIP bomb: https://en.wikipedia.org/wiki/Zip_bomb
.. _DTD: https://en.wikipedia.org/wiki/Document_Type_Definition
.. _PI: https://en.wikipedia.org/wiki/Processing_Instruction
.. _Avoid the dangers of XPath injection: http://www.ibm.com/developerworks/xml/library/x-xpathinjection/index.html
.. _Configure SAX parsers for secure processing: http://www.ibm.com/developerworks/xml/library/x-tipcfsx/index.html
.. _Testing for XML Injection: https://www.owasp.org/index.php/Testing_for_XML_Injection_(OWASP-DV-008)
.. _Xerces SecurityMananger: https://xerces.apache.org/xerces2-j/javadocs/xerces2/org/apache/xerces/util/SecurityManager.html
.. _XML Inclusion: https://www.w3.org/TR/xinclude/#include_element

Changelog
=========

defusedxml 0.7.1
---------------------

*Release date: 08-Mar-2021*

- Fix regression ``defusedxml.ElementTree.ParseError`` (#63)
  The ``ParseError`` exception is now the same class object as
  ``xml.etree.ElementTree.ParseError`` again.


defusedxml 0.7.0
----------------

*Release date: 4-Mar-2021*

- No changes


defusedxml 0.7.0rc2
-------------------

*Release date: 12-Jan-2021*

- Re-add and deprecate ``defusedxml.cElementTree``
- Use GitHub Actions instead of TravisCI
- Restore ``ElementTree`` attribute of ``xml.etree`` module after patching

defusedxml 0.7.0rc1
-------------------

*Release date: 04-May-2020*

- Add support for Python 3.9
- ``defusedxml.cElementTree`` is not available with Python 3.9.
- Python 2 is deprecate. Support for Python 2 will be removed in 0.8.0.


defusedxml 0.6.0
----------------

*Release date: 17-Apr-2019*

- Increase test coverage.
- Add badges to README.


defusedxml 0.6.0rc1
-------------------

*Release date: 14-Apr-2019*

- Test on Python 3.7 stable and 3.8-dev
- Drop support for Python 3.4
- No longer pass *html* argument to XMLParse. It has been deprecated and
  ignored for a long time. The DefusedXMLParser still takes a html argument.
  A deprecation warning is issued when the argument is False and a TypeError
  when it's True.
- defusedxml now fails early when pyexpat stdlib module is not available or
  broken.
- defusedxml.ElementTree.__all__ now lists ParseError as public attribute.
- The defusedxml.ElementTree and defusedxml.cElementTree modules had a typo
  and used XMLParse instead of XMLParser as an alias for DefusedXMLParser.
  Both the old and fixed name are now available.


defusedxml 0.5.0
----------------

*Release date: 07-Feb-2017*

- No changes


defusedxml 0.5.0.rc1
--------------------

*Release date: 28-Jan-2017*

- Add compatibility with Python 3.6
- Drop support for Python 2.6, 3.1, 3.2, 3.3
- Fix lxml tests (XMLSyntaxError: Detected an entity reference loop)


defusedxml 0.4.1
----------------

*Release date: 28-Mar-2013*

- Add more demo exploits, e.g. python_external.py and Xalan XSLT demos.
- Improved documentation.


defusedxml 0.4
--------------

*Release date: 25-Feb-2013*

- As per http://seclists.org/oss-sec/2013/q1/340 please REJECT
  CVE-2013-0278, CVE-2013-0279 and CVE-2013-0280 and use CVE-2013-1664,
  CVE-2013-1665 for OpenStack/etc.
- Add missing parser_list argument to sax.make_parser(). The argument is
  ignored, though. (thanks to Florian Apolloner)
- Add demo exploit for external entity attack on Python's SAX parser, XML-RPC
  and WebDAV.


defusedxml 0.3
--------------

*Release date: 19-Feb-2013*

- Improve documentation


defusedxml 0.2
--------------

*Release date: 15-Feb-2013*

- Rename ExternalEntitiesForbidden to ExternalReferenceForbidden
- Rename defusedxml.lxml.check_dtd() to check_docinfo()
- Unify argument names in callbacks
- Add arguments and formatted representation to exceptions
- Add forbid_external argument to all functions and classes
- More tests
- LOTS of documentation
- Add example code for other languages (Ruby, Perl, PHP) and parsers (Genshi)
- Add protection against XML and gzip attacks to xmlrpclib

defusedxml 0.1
--------------

*Release date: 08-Feb-2013*

- Initial and internal release for PSRT review


"""^^xsd:string ;
    schema1:name "defusedxml"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.7.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/docopt> a software_vocab:Software_package ;
    schema1:description """``docopt`` creates *beautiful* command-line interfaces
======================================================================

Video introduction to **docopt**: `PyCon UK 2012: Create *beautiful*
command-line interfaces with Python <http://youtu.be/pXhcPJK5cMc>`_

    New in version 0.6.1:

    - Fix issue `#85 <https://github.com/docopt/docopt/issues/85>`_
      which caused improper handling of ``[options]`` shortcut
      if it was present several times.

    New in version 0.6.0:

    - New argument ``options_first``, disallows interspersing options
      and arguments.  If you supply ``options_first=True`` to
      ``docopt``, it will interpret all arguments as positional
      arguments after first positional argument.

    - If option with argument could be repeated, its default value
      will be interpreted as space-separated list. E.g. with
      ``[default: ./here ./there]`` will be interpreted as
      ``['./here', './there']``.

    Breaking changes:

    - Meaning of ``[options]`` shortcut slightly changed. Previously
      it ment *"any known option"*. Now it means *"any option not in
      usage-pattern"*.  This avoids the situation when an option is
      allowed to be repeated unintentionaly.

    - ``argv`` is ``None`` by default, not ``sys.argv[1:]``.
      This allows ``docopt`` to always use the *latest* ``sys.argv``,
      not ``sys.argv`` during import time.

Isn't it awesome how ``optparse`` and ``argparse`` generate help
messages based on your code?!

*Hell no!*  You know what's awesome?  It's when the option parser *is*
generated based on the beautiful help message that you write yourself!
This way you don't need to write this stupid repeatable parser-code,
and instead can write only the help message--*the way you want it*.

**docopt** helps you create most beautiful command-line interfaces
*easily*:

.. code:: python

    \"\"\"Naval Fate.

    Usage:
      naval_fate.py ship new <name>...
      naval_fate.py ship <name> move <x> <y> [--speed=<kn>]
      naval_fate.py ship shoot <x> <y>
      naval_fate.py mine (set|remove) <x> <y> [--moored | --drifting]
      naval_fate.py (-h | --help)
      naval_fate.py --version

    Options:
      -h --help     Show this screen.
      --version     Show version.
      --speed=<kn>  Speed in knots [default: 10].
      --moored      Moored (anchored) mine.
      --drifting    Drifting mine.

    \"\"\"
    from docopt import docopt


    if __name__ == '__main__':
        arguments = docopt(__doc__, version='Naval Fate 2.0')
        print(arguments)

Beat that! The option parser is generated based on the docstring above
that is passed to ``docopt`` function.  ``docopt`` parses the usage
pattern (``"Usage: ..."``) and option descriptions (lines starting
with dash "``-``") and ensures that the program invocation matches the
usage pattern; it parses options, arguments and commands based on
that. The basic idea is that *a good help message has all necessary
information in it to make a parser*.

Also, `PEP 257 <http://www.python.org/dev/peps/pep-0257/>`_ recommends
putting help message in the module docstrings.

Installation
======================================================================

Use `pip <http://pip-installer.org>`_ or easy_install::

    pip install docopt==0.6.2

Alternatively, you can just drop ``docopt.py`` file into your
project--it is self-contained.

**docopt** is tested with Python 2.5, 2.6, 2.7, 3.2, 3.3 and PyPy.

API
======================================================================

.. code:: python

    from docopt import docopt

.. code:: python

    docopt(doc, argv=None, help=True, version=None, options_first=False)

``docopt`` takes 1 required and 4 optional arguments:

- ``doc`` could be a module docstring (``__doc__``) or some other
  string that contains a **help message** that will be parsed to
  create the option parser.  The simple rules of how to write such a
  help message are given in next sections.  Here is a quick example of
  such a string:

.. code:: python

    \"\"\"Usage: my_program.py [-hso FILE] [--quiet | --verbose] [INPUT ...]

    -h --help    show this
    -s --sorted  sorted output
    -o FILE      specify output file [default: ./test.txt]
    --quiet      print less text
    --verbose    print more text

    \"\"\"

- ``argv`` is an optional argument vector; by default ``docopt`` uses
  the argument vector passed to your program (``sys.argv[1:]``).
  Alternatively you can supply a list of strings like ``['--verbose',
  '-o', 'hai.txt']``.

- ``help``, by default ``True``, specifies whether the parser should
  automatically print the help message (supplied as ``doc``) and
  terminate, in case ``-h`` or ``--help`` option is encountered
  (options should exist in usage pattern, more on that below). If you
  want to handle ``-h`` or ``--help`` options manually (as other
  options), set ``help=False``.

- ``version``, by default ``None``, is an optional argument that
  specifies the version of your program. If supplied, then, (assuming
  ``--version`` option is mentioned in usage pattern) when parser
  encounters the ``--version`` option, it will print the supplied
  version and terminate.  ``version`` could be any printable object,
  but most likely a string, e.g. ``"2.1.0rc1"``.

    Note, when ``docopt`` is set to automatically handle ``-h``,
    ``--help`` and ``--version`` options, you still need to mention
    them in usage pattern for this to work. Also, for your users to
    know about them.

- ``options_first``, by default ``False``.  If set to ``True`` will
  disallow mixing options and positional argument.  I.e. after first
  positional argument, all arguments will be interpreted as positional
  even if the look like options.  This can be used for strict
  compatibility with POSIX, or if you want to dispatch your arguments
  to other programs.

The **return** value is a simple dictionary with options, arguments
and commands as keys, spelled exactly like in your help message.  Long
versions of options are given priority. For example, if you invoke the
top example as::

    naval_fate.py ship Guardian move 100 150 --speed=15

the return dictionary will be:

.. code:: python

    {'--drifting': False,    'mine': False,
     '--help': False,        'move': True,
     '--moored': False,      'new': False,
     '--speed': '15',        'remove': False,
     '--version': False,     'set': False,
     '<name>': ['Guardian'], 'ship': True,
     '<x>': '100',           'shoot': False,
     '<y>': '150'}

Help message format
======================================================================

Help message consists of 2 parts:

- Usage pattern, e.g.::

    Usage: my_program.py [-hso FILE] [--quiet | --verbose] [INPUT ...]

- Option descriptions, e.g.::

    -h --help    show this
    -s --sorted  sorted output
    -o FILE      specify output file [default: ./test.txt]
    --quiet      print less text
    --verbose    print more text

Their format is described below; other text is ignored.

Usage pattern format
----------------------------------------------------------------------

**Usage pattern** is a substring of ``doc`` that starts with
``usage:`` (case *insensitive*) and ends with a *visibly* empty line.
Minimum example:

.. code:: python

    \"\"\"Usage: my_program.py

    \"\"\"

The first word after ``usage:`` is interpreted as your program's name.
You can specify your program's name several times to signify several
exclusive patterns:

.. code:: python

    \"\"\"Usage: my_program.py FILE
              my_program.py COUNT FILE

    \"\"\"

Each pattern can consist of the following elements:

- **<arguments>**, **ARGUMENTS**. Arguments are specified as either
  upper-case words, e.g. ``my_program.py CONTENT-PATH`` or words
  surrounded by angular brackets: ``my_program.py <content-path>``.
- **--options**.  Options are words started with dash (``-``), e.g.
  ``--output``, ``-o``.  You can "stack" several of one-letter
  options, e.g. ``-oiv`` which will be the same as ``-o -i -v``. The
  options can have arguments, e.g.  ``--input=FILE`` or ``-i FILE`` or
  even ``-iFILE``. However it is important that you specify option
  descriptions if you want for option to have an argument, a default
  value, or specify synonymous short/long versions of option (see next
  section on option descriptions).
- **commands** are words that do *not* follow the described above
  conventions of ``--options`` or ``<arguments>`` or ``ARGUMENTS``,
  plus two special commands: dash "``-``" and double dash "``--``"
  (see below).

Use the following constructs to specify patterns:

- **[ ]** (brackets) **optional** elements.  e.g.: ``my_program.py
  [-hvqo FILE]``
- **( )** (parens) **required** elements.  All elements that are *not*
  put in **[ ]** are also required, e.g.: ``my_program.py
  --path=<path> <file>...`` is the same as ``my_program.py
  (--path=<path> <file>...)``.  (Note, "required options" might be not
  a good idea for your users).
- **|** (pipe) **mutualy exclusive** elements. Group them using **(
  )** if one of the mutually exclusive elements is required:
  ``my_program.py (--clockwise | --counter-clockwise) TIME``. Group
  them using **[ ]** if none of the mutually-exclusive elements are
  required: ``my_program.py [--left | --right]``.
- **...** (ellipsis) **one or more** elements. To specify that
  arbitrary number of repeating elements could be accepted, use
  ellipsis (``...``), e.g.  ``my_program.py FILE ...`` means one or
  more ``FILE``-s are accepted.  If you want to accept zero or more
  elements, use brackets, e.g.: ``my_program.py [FILE ...]``. Ellipsis
  works as a unary operator on the expression to the left.
- **[options]** (case sensitive) shortcut for any options.  You can
  use it if you want to specify that the usage pattern could be
  provided with any options defined below in the option-descriptions
  and do not want to enumerate them all in usage-pattern.  -
  "``[--]``". Double dash "``--``" is used by convention to separate
  positional arguments that can be mistaken for options. In order to
  support this convention add "``[--]``" to you usage patterns.  -
  "``[-]``". Single dash "``-``" is used by convention to signify that
  ``stdin`` is used instead of a file. To support this add "``[-]``"
  to you usage patterns. "``-``" act as a normal command.

If your pattern allows to match argument-less option (a flag) several
times::

    Usage: my_program.py [-v | -vv | -vvv]

then number of occurences of the option will be counted. I.e.
``args['-v']`` will be ``2`` if program was invoked as ``my_program
-vv``. Same works for commands.

If your usage patterns allows to match same-named option with argument
or positional argument several times, the matched arguments will be
collected into a list::

    Usage: my_program.py <file> <file> --path=<path>...

I.e. invoked with ``my_program.py file1 file2 --path=./here
--path=./there`` the returned dict will contain ``args['<file>'] ==
['file1', 'file2']`` and ``args['--path'] == ['./here', './there']``.


Option descriptions format
----------------------------------------------------------------------

**Option descriptions** consist of a list of options that you put
below your usage patterns.

It is necessary to list option descriptions in order to specify:

- synonymous short and long options,
- if an option has an argument,
- if option's argument has a default value.

The rules are as follows:

- Every line in ``doc`` that starts with ``-`` or ``--`` (not counting
  spaces) is treated as an option description, e.g.::

    Options:
      --verbose   # GOOD
      -o FILE     # GOOD
    Other: --bad  # BAD, line does not start with dash "-"

- To specify that option has an argument, put a word describing that
  argument after space (or equals "``=``" sign) as shown below. Follow
  either <angular-brackets> or UPPER-CASE convention for options'
  arguments.  You can use comma if you want to separate options. In
  the example below, both lines are valid, however you are recommended
  to stick to a single style.::

    -o FILE --output=FILE       # without comma, with "=" sign
    -i <file>, --input <file>   # with comma, wihtout "=" sing

- Use two spaces to separate options with their informal description::

    --verbose More text.   # BAD, will be treated as if verbose option had
                           # an argument "More", so use 2 spaces instead
    -q        Quit.        # GOOD
    -o FILE   Output file. # GOOD
    --stdout  Use stdout.  # GOOD, 2 spaces

- If you want to set a default value for an option with an argument,
  put it into the option-description, in form ``[default:
  <my-default-value>]``::

    --coefficient=K  The K coefficient [default: 2.95]
    --output=FILE    Output file [default: test.txt]
    --directory=DIR  Some directory [default: ./]

- If the option is not repeatable, the value inside ``[default: ...]``
  will be interpeted as string.  If it *is* repeatable, it will be
  splited into a list on whitespace::

    Usage: my_program.py [--repeatable=<arg> --repeatable=<arg>]
                         [--another-repeatable=<arg>]...
                         [--not-repeatable=<arg>]

    # will be ['./here', './there']
    --repeatable=<arg>          [default: ./here ./there]

    # will be ['./here']
    --another-repeatable=<arg>  [default: ./here]

    # will be './here ./there', because it is not repeatable
    --not-repeatable=<arg>      [default: ./here ./there]

Examples
----------------------------------------------------------------------

We have an extensive list of `examples
<https://github.com/docopt/docopt/tree/master/examples>`_ which cover
every aspect of functionality of **docopt**.  Try them out, read the
source if in doubt.

Subparsers, multi-level help and *huge* applications (like git)
----------------------------------------------------------------------

If you want to split your usage-pattern into several, implement
multi-level help (whith separate help-screen for each subcommand),
want to interface with existing scripts that don't use **docopt**, or
you're building the next "git", you will need the new ``options_first``
parameter (described in API section above). To get you started quickly
we implemented a subset of git command-line interface as an example:
`examples/git
<https://github.com/docopt/docopt/tree/master/examples/git>`_


Data validation
----------------------------------------------------------------------

**docopt** does one thing and does it well: it implements your
command-line interface.  However it does not validate the input data.
On the other hand there are libraries like `python schema
<https://github.com/halst/schema>`_ which make validating data a
breeze.  Take a look at `validation_example.py
<https://github.com/docopt/docopt/tree/master/examples/validation_example.py>`_
which uses **schema** to validate data and report an error to the
user.

Development
======================================================================

We would *love* to hear what you think about **docopt** on our `issues
page <http://github.com/docopt/docopt/issues>`_

Make pull requrests, report bugs, suggest ideas and discuss
**docopt**. You can also drop a line directly to
<vladimir@keleshev.com>.

Porting ``docopt`` to other languages
======================================================================

We think **docopt** is so good, we want to share it beyond the Python
community!

The follosing ports are available:

- `Ruby port <http://github.com/docopt/docopt.rb>`_
- `CoffeeScript port <http://github.com/docopt/docopt.coffee>`_
- `Lua port <http://github.com/docopt/docopt.lua>`_
- `PHP port <http://github.com/docopt/docopt.php>`_

But you can always create a port for your favorite language!  You are
encouraged to use the Python version as a reference implementation.  A
Language-agnostic test suite is bundled with `Python implementation
<http://github.com/docopt/docopt>`_.

Porting discussion is on `issues page
<http://github.com/docopt/docopt/issues>`_.

Changelog
======================================================================

**docopt** follows `semantic versioning <http://semver.org>`_.  The
first release with stable API will be 1.0.0 (soon).  Until then, you
are encouraged to specify explicitly the version in your dependency
tools, e.g.::

    pip install docopt==0.6.2

- 0.6.2 `Wheel <http://pythonwheels.com/>`_ support.
- 0.6.1 Bugfix release.
- 0.6.0 ``options_first`` parameter.
  **Breaking changes**: Corrected ``[options]`` meaning.
  ``argv`` defaults to ``None``.
- 0.5.0 Repeated options/commands are counted or accumulated into a
  list.
- 0.4.2 Bugfix release.
- 0.4.0 Option descriptions become optional,
  support for "``--``" and "``-``" commands.
- 0.3.0 Support for (sub)commands like `git remote add`.
  Introduce ``[options]`` shortcut for any options.
  **Breaking changes**: ``docopt`` returns dictionary.
- 0.2.0 Usage pattern matching. Positional arguments parsing based on
  usage patterns.
  **Breaking changes**: ``docopt`` returns namespace (for arguments),
  not list. Usage pattern is formalized.
- 0.1.0 Initial release. Options-parsing only (based on options
  description).
"""^^xsd:string ;
    schema1:name "docopt"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.6.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/doxypypy> a software_vocab:Software_package ;
    schema1:description """doxypypy
========

*A more Pythonic version of doxypy, a Doxygen filter for Python.*

Intent
------

For now Doxygen_ has limited support for Python.  It recognizes Python comments,
but otherwise treats the language as being more or less like Java.  It doesn't
understand basic Python syntax constructs like docstrings, keyword arguments,
generators, nested functions, decorators, or lambda expressions.  It likewise
doesn't understand conventional constructs like doctests or ZOPE-style
interfaces.  It does however support inline filters that can be used to make
input source code a little more like what it's expecting.

The excellent doxypy_ makes it possible to embed Doxygen commands in Python
docstrings, and have those docstrings converted to Doxygen-recognized comments
on the fly per Doxygen's regular input filtering process.  It however does not
address any of the other previously mentioned areas of difficulty.

This project started off as a fork of doxypy but quickly became quite distinct.
It shares little (if any) of the same code at this point (but maintains the
original license just in case).  It is meant to support all the same command
line options as doxypy, but handle additional Python syntax beyond docstrings.

Additional Syntax Supported
---------------------------

Python can have functions and classes within both functions and classes.
Doxygen best understands this concept via its notion of namespaces.  This filter
thus can supply Doxygen tags marking namespaces on every function and class.
This addresses the issue of Doxygen merging inner functions' documentation with
the documentation of the parent.

Python class members whose names begin with a double-underscore are mangled
and kept private by the language.  Doxygen does not understand this natively
yet, so this filter additionally provides Doxygen tags to label such variables
as private.

Python frequently embeds doctests within docstrings.  This filter makes it
trivial to mark off such sections of the docstring so they get displayed as
code.

ZOPE-style interfaces overload class definitions to be interface definitions,
use embedded variable assignments to identify attributes, and use specific
function calls to indicate interface adherence.  Furthermore, they frequently
don't have any code beyond their docstrings, so naively removing docstrings
would result in broken Python.  This filter has basic understanding of these
interfaces and treats them accordingly, supplying Doxygen tags as appropriate.

Fundamentally Python docstrings are meant for humans and not machines, and ought
not to have special mark-up beyond conventional structured text.  This filter
heuristically examines Python docstrings, and ones like the sample for complex
in `PEP 257`_ or that generally follow the stricter `Google Python Style Guide`_
will get appropriate Doxygen tags automatically added.

How It Works
------------

This project takes a radically different approach than doxypy.  Rather than use
regular expressions tied to a state machine to figure out syntax, Python's own
Abstract Syntax Tree module is used to extract items of interest.  If the
`autobrief` option is enabled, docstrings are parsed via a set of regular
expressions and a producer / consumer pair of coroutines.

Example
-------

This filter will correctly process code like the following working (albeit
contrived) example:

.. code-block:: python

    def myfunction(arg1, arg2, kwarg='whatever.'):
        \"\"\"
        Does nothing more than demonstrate syntax.

        This is an example of how a Pythonic human-readable docstring can
        get parsed by doxypypy and marked up with Doxygen commands as a
        regular input filter to Doxygen.

        Args:
            arg1:   A positional argument.
            arg2:   Another positional argument.

        Kwargs:
            kwarg:  A keyword argument.

        Returns:
            A string holding the result.

        Raises:
            ZeroDivisionError, AssertionError, & ValueError.

        Examples:
            >>> myfunction(2, 3)
            '5 - 0, whatever.'
            >>> myfunction(5, 0, 'oops.')
            Traceback (most recent call last):
                ...
            ZeroDivisionError: integer division or modulo by zero
            >>> myfunction(4, 1, 'got it.')
            '5 - 4, got it.'
            >>> myfunction(23.5, 23, 'oh well.')
            Traceback (most recent call last):
                ...
            AssertionError
            >>> myfunction(5, 50, 'too big.')
            Traceback (most recent call last):
                ...
            ValueError
        \"\"\"
        assert isinstance(arg1, int)
        if arg2 > 23:
            raise ValueError
        return '{0} - {1}, {2}'.format(arg1 + arg2, arg1 / arg2, kwarg)

There are a few points to note:

1.  No special tags are used.  Best practice human-readable section headers
are enough.

2.  Some flexibility is allowed.  Most common names for sections are accepted,
and items and descriptions may be separated by either colons or dashes.

3.  The brief must be the first item and be no longer than one
line.

4.  Everything thrown into an examples section will be treated as code, so it's
the perfect place for doctests.

Additional more comprehensive examples can be found in the test area.

Installing doxypypy
-------------------

One can use either :code:`pip` or :code:`easy_install` for installation.
Running either:

.. code-block:: shell

    pip install doxypypy

or:

.. code-block:: shell

    easy_install doxypypy

with administrator privileges should do the trick.

Many Linux distributions have packages for doxypypy, so if you are
using Linux you may find it more convenient to use :code:`aptitude`,
:code:`apt`, :code:`apt-get`, :code:`yum`, :code:`dnf`, etc. as
appropriate for your system to install the version tested by the
distribution maintainer. It will often be available as separate
packages for both Python 3 and Python 2.


Previewing doxypypy Output
--------------------------

After successful installation, doxypypy can be run from the command line to
preview the filtered results with:

.. code-block:: shell

    doxypypy -a -c file.py

Typically you'll want to redirect output to a file for viewing in a text editor:

.. code-block:: shell

    doxypypy -a -c file.py > file.py.out

Invoking doxypypy from Doxygen
------------------------------

To make Doxygen run your Python code through doxypypy, set the FILTER\\_PATTERNS
tag in your Doxyfile as follows:

.. code-block:: shell

    FILTER_PATTERNS        = *.py=py_filter

`py_filter` must be available in your path as a shell script (or Windows batch
file).  If you wish to run `py_filter` in a particular directory you can include
the full or relative path.

For Unix-like operating systems, `py_filter` should like something like this:

.. code-block:: shell

    #!/bin/bash
    doxypypy -a -c $1

In Windows, the batch file should be named `py_filter.bat`, and need only
contain the one line:

.. code-block:: shell

    doxypypy -a -c %1

Running Doxygen as usual should now run all Python code through doxypypy.  Be
sure to carefully browse the Doxygen output the first time to make sure that
Doxygen properly found and executed doxypypy.

.. _Doxygen: http://www.stack.nl/~dimitri/doxygen/
.. _doxypy: https://github.com/Feneric/doxypy
.. _PEP 257: http://www.python.org/dev/peps/pep-0257/
.. _Google Python Style Guide: https://google.github.io/styleguide/pyguide.html?showone=Comments#Comments

"""^^xsd:string ;
    schema1:name "doxypypy"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.8.8.7"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/et-xmlfile> a software_vocab:Software_package ;
    schema1:description """et_xmfile
=========

et_xmlfile is a low memory library for creating large XML files.

It is based upon the `xmlfile module from lxml <http://lxml.de/api.html#incremental-xml-generation>`_ with the aim of allowing code to be developed that will work with both libraries. It was developed initially for the openpyxl project but is now a standalone module.

The code was written by Elias Rabel as part of the `Python DÃ¼sseldorf <http://pyddf.de>`_ openpyxl sprint in September 2014.


Note on performance
-------------------

The code was not developed with performance in mind but turned out to be faster than the existing SAX-based implementation but is significantly slower than lxml's xmlfile. There is one area where an optimisation for lxml will negatively affect the performance of et_xmfile and that is when using the `.element()` method on an xmlfile context manager. It is, therefore, recommended not to use this, though the method is provided for code compatibility.


"""^^xsd:string ;
    schema1:name "et-xmlfile"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.1.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/fqdn> a software_vocab:Software_package ;
    schema1:description """UNKNOWN


"""^^xsd:string ;
    schema1:name "fqdn"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.5.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/ipython> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/jedi>,
        <https://data.siemens-energy.com/cybersec/package/matplotlib>,
        <https://data.siemens-energy.com/cybersec/package/numpy>,
        <https://data.siemens-energy.com/cybersec/package/pexpect>,
        <https://data.siemens-energy.com/cybersec/package/prompt-toolkit>,
        <https://data.siemens-energy.com/cybersec/package/pygments>,
        <https://data.siemens-energy.com/cybersec/package/pytest>,
        <https://data.siemens-energy.com/cybersec/package/setuptools>,
        <https://data.siemens-energy.com/cybersec/package/sphinx>,
        <https://data.siemens-energy.com/cybersec/package/traitlets> ;
    schema1:description """IPython provides a rich toolkit to help you make the most out of using Python
interactively.  Its main components are:

 * A powerful interactive Python shell
 * A `Jupyter <https://jupyter.org/>`_ kernel to work with Python code in Jupyter
   notebooks and other interactive frontends.

The enhanced interactive Python shells have the following main features:

 * Comprehensive object introspection.

 * Input history, persistent across sessions.

 * Caching of output results during a session with automatically generated
   references.

 * Extensible tab completion, with support by default for completion of python
   variables and keywords, filenames and function keywords.

 * Extensible system of 'magic' commands for controlling the environment and
   performing many tasks related either to IPython or the operating system.

 * A rich configuration system with easy switching between different setups
   (simpler than changing $PYTHONSTARTUP environment variables every time).

 * Session logging and reloading.

 * Extensible syntax processing for special purpose situations.

 * Access to the system shell with user-extensible alias system.

 * Easily embeddable in other Python programs and GUIs.

 * Integrated access to the pdb debugger and the Python profiler.

The latest development version is always available from IPython's `GitHub
site <http://github.com/ipython>`_.
"""^^xsd:string ;
    schema1:name "ipython"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "8.12.3"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/isoduration> a software_vocab:Software_package ;
    schema1:description """# isoduration: Operations with ISO 8601 durations.

[![PyPI Package](https://img.shields.io/pypi/v/isoduration?style=flat-square)](https://pypi.org/project/isoduration/)

## What is this.

ISO 8601 is most commonly known as a way to exchange datetimes in textual format. A
lesser known aspect of the standard is the representation of durations. They have a
shape similar to this:

```
P3Y6M4DT12H30M5S
```

This string represents a duration of 3 years, 6 months, 4 days, 12 hours, 30 minutes,
and 5 seconds.

The state of the art of ISO 8601 duration handling in Python is more or less limited to
what's offered by [`isodate`](https://pypi.org/project/isodate/). What we are trying to
achieve here is to address the shortcomings of `isodate` (as described in their own
[_Limitations_](https://github.com/gweis/isodate/#limitations) section), and a few of
our own annoyances with their interface, such as the lack of uniformity in their
handling of types, and the use of regular expressions for parsing.

## How to use it.

This package revolves around the [`Duration`](src/isoduration/types.py) type.

Given a ISO duration string we can produce such a type by using the `parse_duration()`
function:

```py
>>> from isoduration import parse_duration
>>> duration = parse_duration("P3Y6M4DT12H30M5S")
>>> duration.date
DateDuration(years=Decimal('3'), months=Decimal('6'), days=Decimal('4'), weeks=Decimal('0'))
>>> duration.time
TimeDuration(hours=Decimal('12'), minutes=Decimal('30'), seconds=Decimal('5'))
```

The `date` and `time` portions of the parsed duration are just regular
[dataclasses](https://docs.python.org/3/library/dataclasses.html), so their members can
be accessed in a non-surprising way.

Besides just parsing them, a number of additional operations are available:

- Durations can be compared and negated:
  ```py
  >>> parse_duration("P3Y4D") == parse_duration("P3Y4DT0H")
  True
  >>> -parse_duration("P3Y4D")
  Duration(DateDuration(years=Decimal('-3'), months=Decimal('0'), days=Decimal('-4'), weeks=Decimal('0')), TimeDuration(hours=Decimal('0'), minutes=Decimal('0'), seconds=Decimal('0')))
  ```
- Durations can be added to, or subtracted from, Python datetimes:
  ```py
  >>> from datetime import datetime
  >>> datetime(2020, 3, 15) + parse_duration("P2Y")
  datetime.datetime(2022, 3, 15, 0, 0)
  >>> datetime(2020, 3, 15) - parse_duration("P33Y1M4D")
  datetime.datetime(1987, 2, 11, 0, 0)
  ```
- Durations are hashable, so they can be used as dictionary keys or as part of sets.
- Durations can be formatted back to a ISO 8601-compliant duration string:
  ```py
  >>> from isoduration import parse_duration, format_duration
  >>> format_duration(parse_duration("P11YT2H"))
  'P11YT2H'
  >>> str(parse_duration("P11YT2H"))
  'P11YT2H'
  ```

## How to improve it.

These steps, in this order, should land you in a development environment:

```sh
git clone git@github.com:bolsote/isoduration.git
cd isoduration/
python -m venv ve
. ve/bin/activate
pip install -U pip
pip install -e .
pip install -r requirements/dev.txt
```

Adapt to your own likings and/or needs.

Testing is driven by [tox](https://tox.readthedocs.io). The output of `tox -l` and a
careful read of [tox.ini](tox.ini) should get you there.

## FAQs.

### How come `P1Y != P365D`?
Some years have 366 days. If it's not always the same, then it's not the same.

### Why do you create your own types, instead of somewhat shoehorning a `timedelta`?
`timedelta` cannot represent certain durations, such as those involving years or months.
Since it cannot represent all possible durations without dangerous arithmetic, then it
must not be the right type.

### Why don't you use regular expressions to parse duration strings?
[Regular expressions should only be used to parse regular languages.](https://stackoverflow.com/a/1732454)

### Why is parsing the inverse of formatting, but the converse is not true?
Because this wonderful representation is not unique.

### Why do you support `<insert here a weird case>`?
Probably because the standard made me to.

### Why do you not support `<insert here a weird case>`?
Probably because the standard doesn't allow me to.

### Why is it not possible to subtract a datetime from a duration?
I'm confused.

### Why should I use this over some other thing?
You shouldn't do what people on the Internet tell you to do.

### Why are ISO standards so strange?
Yes.

## References.

- [XML Schema Part 2: Datatypes, Appendix D](https://www.w3.org/TR/xmlschema-2/#isoformats):
  This excitingly named document contains more details about ISO 8601 than any human
  should be allowed to understand.
- [`isodate`](https://pypi.org/project/isodate/): The original implementation of ISO
  durations in Python. Worth a look. But ours is cooler.


"""^^xsd:string ;
    schema1:name "isoduration"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "20.11.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/jmespath> a software_vocab:Software_package ;
    schema1:description """JMESPath
========


.. image:: https://badges.gitter.im/Join Chat.svg
   :target: https://gitter.im/jmespath/chat


JMESPath (pronounced "james path") allows you to declaratively specify how to
extract elements from a JSON document.

For example, given this document::

    {"foo": {"bar": "baz"}}

The jmespath expression ``foo.bar`` will return "baz".

JMESPath also supports:

Referencing elements in a list.  Given the data::

    {"foo": {"bar": ["one", "two"]}}

The expression: ``foo.bar[0]`` will return "one".
You can also reference all the items in a list using the ``*``
syntax::

   {"foo": {"bar": [{"name": "one"}, {"name": "two"}]}}

The expression: ``foo.bar[*].name`` will return ["one", "two"].
Negative indexing is also supported (-1 refers to the last element
in the list).  Given the data above, the expression
``foo.bar[-1].name`` will return "two".

The ``*`` can also be used for hash types::

   {"foo": {"bar": {"name": "one"}, "baz": {"name": "two"}}}

The expression: ``foo.*.name`` will return ["one", "two"].


Installation
============

You can install JMESPath from pypi with:

.. code:: bash

    pip install jmespath


API
===

The ``jmespath.py`` library has two functions
that operate on python data structures.  You can use ``search``
and give it the jmespath expression and the data:

.. code:: python

    >>> import jmespath
    >>> path = jmespath.search('foo.bar', {'foo': {'bar': 'baz'}})
    'baz'

Similar to the ``re`` module, you can use the ``compile`` function
to compile the JMESPath expression and use this parsed expression
to perform repeated searches:

.. code:: python

    >>> import jmespath
    >>> expression = jmespath.compile('foo.bar')
    >>> expression.search({'foo': {'bar': 'baz'}})
    'baz'
    >>> expression.search({'foo': {'bar': 'other'}})
    'other'

This is useful if you're going to use the same jmespath expression to
search multiple documents.  This avoids having to reparse the
JMESPath expression each time you search a new document.

Options
-------

You can provide an instance of ``jmespath.Options`` to control how
a JMESPath expression is evaluated.  The most common scenario for
using an ``Options`` instance is if you want to have ordered output
of your dict keys.  To do this you can use either of these options:

.. code:: python

    >>> import jmespath
    >>> jmespath.search('{a: a, b: b}',
    ...                 mydata,
    ...                 jmespath.Options(dict_cls=collections.OrderedDict))


    >>> import jmespath
    >>> parsed = jmespath.compile('{a: a, b: b}')
    >>> parsed.search(mydata,
    ...               jmespath.Options(dict_cls=collections.OrderedDict))


Custom Functions
~~~~~~~~~~~~~~~~

The JMESPath language has numerous
`built-in functions
<http://jmespath.org/specification.html#built-in-functions>`__, but it is
also possible to add your own custom functions.  Keep in mind that
custom function support in jmespath.py is experimental and the API may
change based on feedback.

**If you have a custom function that you've found useful, consider submitting
it to jmespath.site and propose that it be added to the JMESPath language.**
You can submit proposals
`here <https://github.com/jmespath/jmespath.site/issues>`__.

To create custom functions:

* Create a subclass of ``jmespath.functions.Functions``.
* Create a method with the name ``_func_<your function name>``.
* Apply the ``jmespath.functions.signature`` decorator that indicates
  the expected types of the function arguments.
* Provide an instance of your subclass in a ``jmespath.Options`` object.

Below are a few examples:

.. code:: python

    import jmespath
    from jmespath import functions

    # 1. Create a subclass of functions.Functions.
    #    The function.Functions base class has logic
    #    that introspects all of its methods and automatically
    #    registers your custom functions in its function table.
    class CustomFunctions(functions.Functions):

        # 2 and 3.  Create a function that starts with _func_
        # and decorate it with @signature which indicates its
        # expected types.
        # In this example, we're creating a jmespath function
        # called "unique_letters" that accepts a single argument
        # with an expected type "string".
        @functions.signature({'types': ['string']})
        def _func_unique_letters(self, s):
            # Given a string s, return a sorted
            # string of unique letters: 'ccbbadd' ->  'abcd'
            return ''.join(sorted(set(s)))

        # Here's another example.  This is creating
        # a jmespath function called "my_add" that expects
        # two arguments, both of which should be of type number.
        @functions.signature({'types': ['number']}, {'types': ['number']})
        def _func_my_add(self, x, y):
            return x + y

    # 4. Provide an instance of your subclass in a Options object.
    options = jmespath.Options(custom_functions=CustomFunctions())

    # Provide this value to jmespath.search:
    # This will print 3
    print(
        jmespath.search(
            'my_add(`1`, `2`)', {}, options=options)
    )

    # This will print "abcd"
    print(
        jmespath.search(
            'foo.bar | unique_letters(@)',
            {'foo': {'bar': 'ccbbadd'}},
            options=options)
    )

Again, if you come up with useful functions that you think make
sense in the JMESPath language (and make sense to implement in all
JMESPath libraries, not just python), please let us know at
`jmespath.site <https://github.com/jmespath/jmespath.site/issues>`__.


Specification
=============

If you'd like to learn more about the JMESPath language, you can check out
the `JMESPath tutorial <http://jmespath.org/tutorial.html>`__.  Also check
out the `JMESPath examples page <http://jmespath.org/examples.html>`__ for
examples of more complex jmespath queries.

The grammar is specified using ABNF, as described in
`RFC4234 <http://www.ietf.org/rfc/rfc4234.txt>`_.
You can find the most up to date
`grammar for JMESPath here <http://jmespath.org/specification.html#grammar>`__.

You can read the full
`JMESPath specification here <http://jmespath.org/specification.html>`__.


Testing
=======

In addition to the unit tests for the jmespath modules,
there is a ``tests/compliance`` directory that contains
.json files with test cases.  This allows other implementations
to verify they are producing the correct output.  Each json
file is grouped by feature.


Discuss
=======

Join us on our `Gitter channel <https://gitter.im/jmespath/chat>`__
if you want to chat or if you have any questions.


"""^^xsd:string ;
    schema1:name "jmespath"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.0.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/jupyter_client> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/importlib-metadata>,
        <https://data.siemens-energy.com/cybersec/package/ipykernel>,
        <https://data.siemens-energy.com/cybersec/package/jupyter-core>,
        <https://data.siemens-energy.com/cybersec/package/pytest>,
        <https://data.siemens-energy.com/cybersec/package/python-dateutil>,
        <https://data.siemens-energy.com/cybersec/package/pyzmq>,
        <https://data.siemens-energy.com/cybersec/package/sphinx>,
        <https://data.siemens-energy.com/cybersec/package/tornado>,
        <https://data.siemens-energy.com/cybersec/package/traitlets> ;
    schema1:description """# Jupyter Client

[![Build Status](https://github.com/jupyter/jupyter_client/workflows/CI/badge.svg)](https://github.com/jupyter/jupyter_client/actions)
[![Documentation Status](https://readthedocs.org/projects/jupyter-client/badge/?version=latest)](http://jupyter-client.readthedocs.io/en/latest/?badge=latest)

`jupyter_client` contains the reference implementation of the [Jupyter protocol].
It also provides client and kernel management APIs for working with kernels.

It also provides the `jupyter kernelspec` entrypoint
for installing kernelspecs for use with Jupyter frontends.

## Development Setup

The [Jupyter Contributor Guides](https://jupyter.readthedocs.io/en/latest/contributing/content-contributor.html) provide extensive information on contributing code or documentation to Jupyter projects. The limited instructions below for setting up a development environment are for your convenience.

## Coding

You'll need Python and `pip` on the search path. Clone the Jupyter Client git repository to your computer, for example in `/my/project/jupyter_client`

```bash
cd /my/projects/
git clone git@github.com:jupyter/jupyter_client.git
```

Now create an [editable install](https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs)
and download the dependencies of code and test suite by executing:

```bash
cd /my/projects/jupyter_client/
pip install -e ".[test]"
pytest
```

The last command runs the test suite to verify the setup. During development, you can pass filenames to `pytest`, and it will execute only those tests.

## Documentation

The documentation of Jupyter Client is generated from the files in `docs/` using Sphinx. Instructions for setting up Sphinx with a selection of optional modules are in the [Documentation Guide](https://jupyter.readthedocs.io/en/latest/contributing/docs-contributions/index.html). You'll also need the `make` command.
For a minimal Sphinx installation to process the Jupyter Client docs, execute:

```bash
pip install ".[doc]"
```

The following commands build the documentation in HTML format and check for broken links:

```bash
cd /my/projects/jupyter_client/docs/
make html linkcheck
```

Point your browser to the following URL to access the generated documentation:

_file:///my/projects/jupyter_client/docs/\\_build/html/index.html_

## Contributing

`jupyter-client` has adopted automatic code formatting so you shouldn't
need to worry too much about your code style.
As long as your code is valid,
the pre-commit hook should take care of how it should look.
You can invoke the pre-commit hook by hand at any time with:

```bash
pre-commit run
```

which should run any autoformatting on your code
and tell you about any errors it couldn't fix automatically.
You may also install [black integration](https://black.readthedocs.io/en/stable/integrations/editors.html)
into your text editor to format code automatically.

If you have already committed files before setting up the pre-commit
hook with `pre-commit install`, you can fix everything up using
`pre-commit run --all-files`. You need to make the fixing commit
yourself after that.

Some of the hooks only run on CI by default, but you can invoke them by
running with the `--hook-stage manual` argument.

## About the Jupyter Development Team

The Jupyter Development Team is the set of all contributors to the Jupyter project.
This includes all of the Jupyter subprojects.

The core team that coordinates development on GitHub can be found here:
https://github.com/jupyter/.

## Our Copyright Policy

Jupyter uses a shared copyright model. Each contributor maintains copyright
over their contributions to Jupyter. But, it is important to note that these
contributions are typically only changes to the repositories. Thus, the Jupyter
source code, in its entirety is not the copyright of any single person or
institution. Instead, it is the collective copyright of the entire Jupyter
Development Team. If individual contributors want to maintain a record of what
changes/contributions they have specific copyright on, they should indicate
their copyright in the commit message of the change, when they commit the
change to one of the Jupyter repositories.

With this in mind, the following banner should be used in any source code file
to indicate the copyright and license terms:

```
# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.
```

[jupyter protocol]: https://jupyter-client.readthedocs.io/en/latest/messaging.html
"""^^xsd:string ;
    schema1:name "jupyter_client"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "8.6.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/jupyter_core> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/platformdirs>,
        <https://data.siemens-energy.com/cybersec/package/pytest>,
        <https://data.siemens-energy.com/cybersec/package/pywin32>,
        <https://data.siemens-energy.com/cybersec/package/traitlets> ;
    schema1:description "There is no reason to install this package on its own."^^xsd:string ;
    schema1:name "jupyter_core"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "5.7.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/jupyterlab_pygments> a software_vocab:Software_package ;
    schema1:description """# JupyterLab Pygments Theme

This package contains a syntax coloring theme for [pygments](http://pygments.org/) making use of
the JupyterLab CSS variables.

The goal is to enable the use of JupyterLab's themes with pygments-generated HTML.

## Screencast

In the following screencast, we demonstrate how Pygments-highlighted code can make use of the JupyterLab theme.

![pygments screencast](pygments.gif)

## Installation

`jupyterlab_pygments` can be installed with the conda package manager

```
conda install -c conda-forge jupyterlab_pygments
```

or from pypi

```
pip install jupyterlab_pygments
```

## Dependencies

- `jupyterlab_pygments` requires [pygments](http://pygments.org) version `2.4.1`.
- The CSS variables used by the theme correspond to the CodeMirror syntex coloring
  theme defined in the NPM package [@jupyterlab/codemirror](https://www.npmjs.com/package/@jupyterlab/codemirror). Supported versions for `@jupyterlab/codemirror`'s CSS include `0.19.1`, `^1.0`, and, `^2.0`.

## Limitations

Pygments-generated HTML and CSS classes are not granular enough to reproduce
all of the details of codemirror (the JavaScript text editor used by JupyterLab).

This includes the ability to differentiate properties from general names.

## License

`jupyterlab_pygments` uses a shared copyright model that enables all contributors to maintain the
copyright on their contributions. All code is licensed under the terms of the revised [BSD license](LICENSE).
"""^^xsd:string ;
    schema1:name "jupyterlab_pygments"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.3.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/lib4sbom> a software_vocab:Software_package ;
    schema1:description """# Lib4SBOM

Lib4SBOM is a library to parse and generate Software Bill of Materials (SBOMs). It supports SBOMs created in both
[SPDX](https://www.spdx.org) and [CycloneDX](https://www.cyclonedx.org) formats.

It has been developed on the assumption that having a generic abstraction of SBOM regardless of the underlying format will be useful to developers.

The following facilities are provided:

- Generate SPDX SBOM in TagValue, JSON and YAML formats
- Generate CycloneDX SBOM in JSON format
- Parse SPDX SBOM in TagValue, JSON, YAML, XML and RDF formats
- Parse CycloneDX SBOM in JSON and XMLformat
- Create and manipulate a SBOM file object
- Create and manipulate a SBOM package object
- Create and manipulate a SBOM dependency relationship object
- Create and manipulate a Vulnerability object
- Create and manipulate a Software Service object
- Generated SBOM can be output to a file or to the console

## Installation

To install use the following command:

`pip install lib4sbom`

Alternatively, just clone the repo and install dependencies using the following command:

`pip install -U -r requirements.txt`

The tool requires Python 3 (3.7+). It is recommended to use a virtual python environment especially
if you are using different versions of python. `virtualenv` is a tool for setting up virtual python environments which
allows you to have all the dependencies for the tool set up in a single environment, or have different environments set
up for testing using different versions of Python.

## API

### SBOMParser

The SBOMParser module provides methods for parsing a SBOM in either SPDX or CycloneDX format and
returns the file, package and relationship information from within the SBOM.

The focus of the implementation is on providing a common set of SBOM data regardless of the SBOM format.

SBOMs are supported in the following formats

| SBOM Type | Version | Format   |
| --------- |---------|----------|
| SPDX      | 2.2     | TagValue |
| SPDX      | 2.2     | JSON     |
| SPDX      | 2.2     | YAML     |
| SPDX      | 2.2     | RDF      |
| SPDX      | 2.2     | XML      |
| SPDX      | 2.3     | TagValue |
| SPDX      | 2.3     | JSON     |
| SPDX      | 2.3     | YAML     |
| SPDX      | 2.3     | RDF       |
| SPDX      | 2.3     | XML      |
| CycloneDX | 1.4     | JSON     |
| CycloneDX | 1.4     | XML      |
| CycloneDX | 1.5     | JSON     |
| CycloneDX | 1.5     | XML      |
| CycloneDX | 1.6     | JSON     |
| CycloneDX | 1.6     | XML      |

**Note** that support for SPDX RDF and XML formats is limited to a few package attributes.

_class_ **SBOMParser**(_sbom_type='auto_')

This creates a simple SBOM Parser object. A single optional parameter, _sbom_type_, can be specified
which represents the type of SBOM (either spdx, cyclonedx or auto). The default is auto in
which case the parser will automatically work out the SBOM type using the
following filename conventions.

| SBOM      | Format   | Filename extension |
| --------- |----------|--------------------|
| SPDX      | TagValue | .spdx              |
| SPDX      | JSON     | .spdx.json         |
| SPDX      | YAML     | .spdx.yaml         |
| SPDX      | YAML     | .spdx.yml          |
| SPDX      | RDF      | .spdx.rdf          |
| SPDX      | XML      | .spdx.xml          |
| CycloneDX | JSON     | .json              |
| CycloneDX | XML      | .xml               |

The parser will check that the correct JSON files is being processed by the correct parser.
A SPDX JSON file submitted to the CycloneDX parser will result in no data being processed.

**Methods**

parse_file(filename)
Parses the SBOM file. If the file does not exist, a FileNotFoundError exception is raised.

get_files()
Returns a list of file elements from within a parsed SBOM

get_packages()
Returns a list of packages elements from within a parsed SBOM

get_relationships()
Returns the relationship elements from within a parsed SBOM

get_vulnerabilities()
Returns the vulnerability elements from within a parsed SBOM

get_services()
Returns the software service elements from within a parsed SBOM

get_type()
Returns the type of SBOM (either spdx or cyclonedx)

**Example**

A test SBOM file (test_sbom.spdx) is used in the following example.

```bash
SPDXVersion: SPDX-2.2
DataLicense: CC0-1.0
SPDXID: SPDXRef-DOCUMENT
DocumentName: virtualenv
DocumentNamespace: http://spdx.org/spdxdocs/virtualenv-b7ac9cce-efe8-4fe7-a544-100e6a5664e6
LicenseListVersion: 3.18
Creator: Tool: sbom4python-0.4.0
Created: 2022-11-16T10:14:26Z
CreatorComment: <text>This document has been automatically generated.</text>
##### 

PackageName: virtualenv
SPDXID: SPDXRef-Package-1-virtualenv
PackageSupplier: Person: Bernat_Gabor
PackageVersion: 20.16.7
PackageDownloadLocation: NOASSERTION
FilesAnalyzed: false
PackageLicenseConcluded: MIT
PackageLicenseDeclared: MIT
PackageCopyrightText: NOASSERTION
ExternalRef: PACKAGE-MANAGER purl pkg:pypi/virtualenv@20.16.7
##### 

PackageName: distlib
SPDXID: SPDXRef-Package-2-distlib
PackageSupplier: Person: Vinay_Sajip
PackageVersion: 0.3.6
PackageDownloadLocation: NOASSERTION
FilesAnalyzed: false
PackageLicenseConcluded: NOASSERTION
PackageLicenseDeclared: NOASSERTION
PackageCopyrightText: NOASSERTION
ExternalRef: PACKAGE-MANAGER purl pkg:pypi/distlib@0.3.6
##### 

PackageName: filelock
SPDXID: SPDXRef-Package-3-filelock
PackageSupplier: Person: Benedikt_Schmitt
PackageVersion: 3.8.0
PackageDownloadLocation: NOASSERTION
FilesAnalyzed: false
PackageLicenseConcluded: Unlicense
PackageLicenseDeclared: Unlicense
PackageCopyrightText: NOASSERTION
ExternalRef: PACKAGE-MANAGER purl pkg:pypi/filelock@3.8.0
##### 

PackageName: platformdirs
SPDXID: SPDXRef-Package-4-platformdirs
PackageSupplier: NOASSERTION
PackageVersion: 2.5.4
PackageDownloadLocation: NOASSERTION
FilesAnalyzed: false
PackageLicenseConcluded: NOASSERTION
PackageLicenseDeclared: NOASSERTION
PackageCopyrightText: NOASSERTION
ExternalRef: PACKAGE-MANAGER purl pkg:pypi/platformdirs@2.5.4

Relationship: SPDXRef-DOCUMENT DESCRIBES SPDXRef-Package-1-virtualenv
Relationship: SPDXRef-Package-1-virtualenv CONTAINS SPDXRef-Package-2-distlib
Relationship: SPDXRef-Package-1-virtualenv CONTAINS SPDXRef-Package-3-filelock
Relationship: SPDXRef-Package-1-virtualenv CONTAINS SPDXRef-Package-4-platformdirs
```

The following code sample shows the use of the SBOMParser module.

```python
>>> from lib4sbom.parser import SBOMParser
>>> test_parser = SBOMParser()
>>> print (f"SBOM type {test_parser.get_type()}")                                                                                                                                             
SBOM type auto                                                                                                                                                                                
>>> test_parser.parse_file("test_sbom.spdx")                                                                                                                                                                                                                                                                                            
>>> print (f"SBOM type {test_parser.get_type()}")                                                                                                                                             
SBOM type spdx                                                                                                                                                                                
>>> sbom_files = test_parser.get_files()
>>> print (sbom_files)                                                                                                                                                                        
[]                                                                                                                                                                                            
>>> sbom_packages = test_parser.get_packages()
>>> print (sbom_packages)
[{'name': 'virtualenv', 'type': 'library', 'id': 'SPDXRef-Package-1-virtualenv', 'supplier_type': 'Person', 'supplier': 'Bernat_Gabor', 'version': '20.16.7', 'downloadlocation': 'NOASSERTION', 'filesanalysis': 'false', 'licenseconcluded': 'MIT', 'licensedeclared': 'MIT', 'externalreference': [['PACKAGE-MANAGER', 'purl', 'pkg:pypi/virtualenv@20.16.7']]}, {'name': 'distlib', 'type': 'library', 'id': 'SPDXRef-Package-2-distlib', 'supplier_type': 'Person', 'supplier': 'Vinay_Sajip', 'version': '0.3.6', 'downloadlocation': 'NOASSERTION', 'filesanalysis': 'false', 'licenseconcluded': 'NOASSERTION', 'licensedeclared': 'NOASSERTION', 'externalreference': [['PACKAGE-MANAGER', 'purl', 'pkg:pypi/distlib@0.3.6']]}, {'name': 'filelock', 'type': 'library', 'id': 'SPDXRef-Package-3-filelock', 'supplier_type': 'Person', 'supplier': 'Benedikt_Schmitt', 'version': '3.8.0', 'downloadlocation': 'NOASSERTION', 'filesanalysis': 'false', 'licenseconcluded': 'Unlicense', 'licensedeclared': 'Unlicense', 'externalreference': [['PACKAGE-MANAGER', 'purl', 'pkg:pypi/filelock@3.8.0']]}, {'name': 'platformdirs', 'type': 'library', 'id': 'SPDXRef-Package-4-platformdirs', 'supplier_type': 'Organization', 'supplier': 'Unknown', 'version': '2.5.4', 'downloadlocation': 'NOASSERTION', 'filesanalysis': 'false', 'licenseconcluded': 'NOASSERTION', 'licensedeclared': 'NOASSERTION', 'externalreference': [['PACKAGE-MANAGER', 'purl', 'pkg:pypi/platformdirs@2.5.4']]}]
>>> print (len(sbom_packages))
4
>>> sbom_packages[0]
{'name': 'virtualenv', 'type': 'library', 'id': 'SPDXRef-Package-1-virtualenv', 'supplier_type': 'Person', 'supplier': 'Bernat_Gabor', 'version': '20.16.7', 'downloadlocation': 'NOASSERTION', 'filesanalysis': 'false', 'licenseconcluded': 'MIT', 'licensedeclared': 'MIT', 'externalreference': [['PACKAGE-MANAGER', 'purl', 'pkg:pypi/virtualenv@20.16.7']]}
>>> sbom_relationships = test_parser.get_relationships()
>>> print (sbom_relationships)
[{'source': 'TestDocument', 'type': 'DESCRIBES', 'target': 'virtualenv', 'source_id': 'SPDXRef-DOCUMENT', 'target_id': 'SPDXRef-Package-1-virtualenv'}, {'source': 'virtualenv', 'type': 'CONTAINS', 'target': 'distlib', 'source_id': 'SPDXRef-Package-1-virtualenv', 'target_id': 'SPDXRef-Package-2-distlib'}, {'source': 'virtualenv', 'type': 'CONTAINS', 'target': 'filelock', 'source_id': 'SPDXRef-Package-1-virtualenv', 'target_id': 'SPDXRef-Package-3-filelock'}, {'source': 'virtualenv', 'type': 'CONTAINS', 'target': 'platformdirs', 'source_id': 'SPDXRef-Package-1-virtualenv', 'target_id': 'SPDXRef-Package-4-platformdirs'}]
>>> sbom_relationships[2]
{'source': 'virtualenv', 'type': 'CONTAINS', 'target': 'filelock', 'source_id': 'SPDXRef-Package-1-virtualenv', 'target_id': 'SPDXRef-Package-3-filelock'}
>>> 
```

### SBOMGenerator

The SBOMGenerator module provides methods for generating a SBOM in either SPDX or CycloneDX format.

The focus of the implementation is on providing a single interface regardless of the SBOM type and format.

SBOMs can be generated in the following formats

| SBOM Type | Version | Format    |
| --------- |---------| ----------|
| SPDX      | 2.2     | Tag       |
| SPDX      | 2.2     | JSON      |
| SPDX      | 2.2     | YAML      |
| SPDX      | 2.3     | Tag       |
| SPDX      | 2.3     | JSON      |
| SPDX      | 2.3     | YAML      |
| CycloneDX | 1.4     | JSON      |
| CycloneDX | 1.5     | JSON      |
| CycloneDX | 1.6     | JSON      |

The default version for CycloneDX is version 1.6. However, the version can be overridden by setting the environment variable LIB4SBOM_CYCLONEDX_VERSION to "1.4" if required.

The default version for SPDX is version 2.3. However, the version can be overridden by setting the environment variable LIB4SBOM_SPDX_VERSION to "SPDX-2.2" if required.

_class_ **SBOMGenerator**(_validate_license: True, sbom_type="spdx", format="tag", application="lib4sbom", version="0.1"_)

This creates a simple SBOM Generator object. The following optional parameters can be specified:

_validate_license_ indicates if license information is validated against the set of [SPDX license identifiers](https://spdx.org/licenses/). This option only applies for SPDX SBOMs
as this is mandatory for CycloneDX SBOMs.

_sbom_type_ indicates the type of SBOM to be generated. Valid options are spdx or cyclonedx

_format_ indicates the format that the SBOM is to be generated in. Valid options are Tag, JSON or YAML. If an invalid format is specified,
a default format of JSON will be assumed. If an unsupported format is specified for the type of SBOM (e.g. Tag or YAML for CycloneDX), a default
format is assumed (Tag for SPDX, JSON for CycloneDX)

_application_ and _version_ specify the name and version of the tool which created the SBOM. If these are not specified, the application name is 'lib4sbom' and the version is '0.1'

**Methods**

_generate(project_name, sbom_data, filename = "", send_to_output = True)_

The method generates the SBOM file. The parameters are

_project_name_ specifies the name of the project

_sbom_data_ contain the SBOM data to be used in the generation. It contains details of the packages, files and relationships to be included in the SBOM.

_filename_ is the name of the file to be generated. The default is for the output to be generated to a console.

_send_to_output_ indicates if the output is to be sent to the filename.

get_sbom()
Returns the generated SBOM in the requested format

get_type()
Returns the type of the generated SBOM (either spdx or cyclonedx)

get_format()
Returns the format of the generated SBOM (one of tag, json or yaml)

**Example**

The following code sample shows the use of the SBOMGenerator module being used
in the conversion of a SBOM from the Tag Value format to YAML format. The output is sent ot the console.

```python
>>> from lib4sbom.parser import SBOMParser
>>> test_parser = SBOMParser()
>>> test_parser.parse_file("test_sbom.spdx")
>>> from lib4sbom.generator import SBOMGenerator
>>> test_generator = SBOMGenerator(format="yaml")
>>> test_generator.get_type()
'spdx'
>>> test_generator.get_format()
'yaml'
>>> test_generator.generate("TestApp",test_parser.get_sbom())
SPDXID: SPDXRef-DOCUMENT
creationInfo:
  comment: This document has been automatically generated.
  created: '2023-01-24T13:51:36Z'
  creators:
  - 'Tool: lib4sbom-0.1.0'
  licenseListVersion: '3.18'
dataLicense: CC0-1.0
documentNamespace: http://spdx.org/spdxdocs/TestDocument-817c4e4c-eac4-49d9-bc41-65f0972edce8
name: TestDocument
...
- relatedSpdxElement: SPDXRef-Package-4-platformdirs
  relationshipType: DESCRIBES
  spdxElementId: SPDXRef-DOCUMENT
- relatedSpdxElement: SPDXRef-Package-2-distlib
  relationshipType: CONTAINS
  spdxElementId: SPDXRef-Package-1-virtualenv
- relatedSpdxElement: SPDXRef-Package-3-filelock
  relationshipType: CONTAINS
  spdxElementId: SPDXRef-Package-1-virtualenv
- relatedSpdxElement: SPDXRef-Package-4-platformdirs
  relationshipType: CONTAINS
  spdxElementId: SPDXRef-Package-1-virtualenv
spdxVersion: SPDX-2.3
>>> test_generator.get_sbom()
{'SPDXID': 'SPDXRef-DOCUMENT', 'spdxVersion': 'SPDX-2.3', 'creationInfo': {'comment': 'This document has been automatically generated.', 'creators': ['Tool: lib4sbom-0.1.0'], 'created': '2023-01-24T13:51:36Z', 'licenseListVersion': '3.18'}, 'name': 'TestDocument', 'dataLicense': 'CC0-1.0', 'documentNamespace': 'http://spdx.org/spdxdocs/TestDocument-817c4e4c-eac4-49d9-bc41-65f0972edce8', 'packages': [{'SPDXID': 'SPDXRef-Package-1-virtualenv', 'name': 'virtualenv', 'versionInfo': '20.16.7', 'supplier': 'Person: Bernat_Gabor', 'downloadLocation': 'NONE', 'filesAnalyzed': 'false', 'licenseConcluded': 'MIT', 'licenseDeclared': 'MIT', 'copyrightText': 'NOASSERTION', 'externalRefs': [{'referenceCategory': 'PACKAGE-MANAGER', 'referenceType': 'purl', 'referenceLocator': 'pkg:pypi/virtualenv@20.16.7'}]}, {'SPDXID': 'SPDXRef-Package-2-distlib', 'name': 'distlib', 'versionInfo': '0.3.6', 'supplier': 'Person: Vinay_Sajip', 'downloadLocation': 'NONE', 'filesAnalyzed': 'false', 'licenseConcluded': 'NOASSERTION', 'licenseDeclared': 'NOASSERTION', 'copyrightText': 'NOASSERTION', 'externalRefs': [{'referenceCategory': 'PACKAGE-MANAGER', 'referenceType': 'purl', 'referenceLocator': 'pkg:pypi/distlib@0.3.6'}]}, {'SPDXID': 'SPDXRef-Package-3-filelock', 'name': 'filelock', 'versionInfo': '3.8.0', 'supplier': 'Person: Benedikt_Schmitt', 'downloadLocation': 'NONE', 'filesAnalyzed': 'false', 'licenseConcluded': 'Unlicense', 'licenseDeclared': 'Unlicense', 'copyrightText': 'NOASSERTION', 'externalRefs': [{'referenceCategory': 'PACKAGE-MANAGER', 'referenceType': 'purl', 'referenceLocator': 'pkg:pypi/filelock@3.8.0'}]}, {'SPDXID': 'SPDXRef-Package-4-platformdirs', 'name': 'platformdirs', 'versionInfo': '2.5.4', 'supplier': 'Organization: Unknown', 'downloadLocation': 'NONE', 'filesAnalyzed': 'false', 'licenseConcluded': 'NOASSERTION', 'licenseDeclared': 'NOASSERTION', 'copyrightText': 'NOASSERTION', 'externalRefs': [{'referenceCategory': 'PACKAGE-MANAGER', 'referenceType': 'purl', 'referenceLocator': 'pkg:pypi/platformdirs@2.5.4'}]}], 'relationships': [{'spdxElementId': 'SPDXRef-DOCUMENT', 'relatedSpdxElement': 'SPDXRef-Package-1-virtualenv', 'relationshipType': 'DESCRIBES'}, {'spdxElementId': 'SPDXRef-DOCUMENT', 'relatedSpdxElement': 'SPDXRef-Package-2-distlib', 'relationshipType': 'DESCRIBES'}, {'spdxElementId': 'SPDXRef-DOCUMENT', 'relatedSpdxElement': 'SPDXRef-Package-3-filelock', 'relationshipType': 'DESCRIBES'}, {'spdxElementId': 'SPDXRef-DOCUMENT', 'relatedSpdxElement': 'SPDXRef-Package-4-platformdirs', 'relationshipType': 'DESCRIBES'}, {'spdxElementId': 'SPDXRef-Package-1-virtualenv', 'relatedSpdxElement': 'SPDXRef-Package-2-distlib', 'relationshipType': 'CONTAINS'}, {'spdxElementId': 'SPDXRef-Package-1-virtualenv', 'relatedSpdxElement': 'SPDXRef-Package-3-filelock', 'relationshipType': 'CONTAINS'}, {'spdxElementId': 'SPDXRef-Package-1-virtualenv', 'relatedSpdxElement': 'SPDXRef-Package-4-platformdirs', 'relationshipType': 'CONTAINS'}]}
>>> 
```

### SBOMOutput

_class_ **SBOMOutput**(_filename="", output_format="tag"_)

This creates a simple SBOM Output object. The following optional parameters can be specified:

_filename_ indicates the output destination of the SBOM to be generated. If a valid filename path is provided and a file can be created, then the output will be to a file otherwise
it will be output to the console.

_output_format_ indicates the format that the SBOM is to be generated in. Valid options are Tag, JSON or YAML. If an invalid format is specified,
a default format of Tag will be assumed.

**Methods**

generate_output(dataset) Outputs a SBOM file. The parameters are

_dataset_ contains SBOM data in the output format. If the SBOM data is NOT in the format specified by the output_format parameter, no output will be generated.

**NOTE A valid dataset will normally be generated by the SBOMGenerator class and obtained by a call to the get_sbom() method.**

get_type()
Return the destination of the generated SBOM. Either file or console

get_format()
Return the format of the generated SBOM. One of Tag, JSON or YAML.

**Example**

The following code sample shows the use of the SBOMOutput module.

```python
>>> from lib4sbom.parser import SBOMParser
>>> test_parser = SBOMParser()
>>> test_parser.parse_file("test_sbom.spdx")
>>> from lib4sbom.generator import SBOMGenerator
>>> test_generator = SBOMGenerator(format="json")
>>> test_generator.generate("TestApp",test_parser.get_sbom())
>>> from lib4sbom.output import SBOMOutput
>>> sbom_output = SBOMOutput(filename="testapp.json", output_format="json")
>>> sbom_output.generate_output(test_generator.get_sbom())
>>> 
```

### SBOM Object 

_class_ **SBOM**()

This creates a simple SBOM object. This object contains all the items to be contained within the SBOM including
components and relationships. It is left to the application manipulating the SBOM object to apply validation as appropriate
for the presence of each attribute.

**Methods**

**_Setter Methods_**

For the following attributes, a method **_set_attribute(value)_** is provided. Note that the attribute name is always in _lowercase_.
e.g. set_type(). Unless indicated, the method just takes a single parameter for the value. Where indicated, multiple instances of the attribute may be defined.

| Attribute         | Multiple | Note |
|-------------------|----------|------|
| Version           | No       | (1)  |
| Type              | No       | (2)  |
| Uuid              | No       | (3)  |
| Bom_Version       | No       |      |

**Note**

1. This relates to the version of the specification of SBOM specified by the type atrribute. e.g. 1.4 for CycloneDX, SPDX-2.3 for SPDX.

2. This refers to the type of SBOM either SPDX or CycloneDX.

3. This relates to the unique identifier for the SBOM.

**_Getter Methods_**

get_sbom()
Returns the SBOM object as a dictionary.

**Example**

```python
>>> from lib4sbom.sbom import SBOM
>>> sbom = SBOM()
>>> sbom.set_type(sbom_type='cyclonedx')
>>> sbom.set_version("1.4")
>>> sbom.set_uuid("urn:uuid:My_uuid_1234")
>>> sbom.set_bom_version("2")
>>> sbom.get_type()
'cyclonedx'
>>> from lib4sbom.data.document import SBOMDocument
>>> my_doc = SBOMDocument()
>>> my_doc.set_metadata_type("firmware")
>>> sbom.add_document(my_doc.get_document())
```

### SBOMDocument Object 

_class_ **SBOMDocument**()

This creates a simple SBOMDocument object. This object contains the values of the attributes
that can be associated with a SBOM. This includes attributes such as name, identifier, type of file,
checksum and licence information. As each of the attributes are optional, it is left to the application manipulating the
SBOMFile object to apply validation as appropriate for the presence of each attribute.

**Methods**

**_Setter Methods_**

For the following attributes, a method **_set_attribute(value)_** is provided. Note that the attribute name is always in _lowercase_.
e.g. set_filetype(). The attribute names are aligned with the attributes of the File Object in the SPDX Specification. Unless
indicated, the method just takes a single parameter for the value. Where indicated, multiple instances of the attribute may be defined.

| Attribute         | Multiple | Note |
|-------------------|----------|------|
| Name              | No       |      |
| Id                | No       |      |
| DataLicense       | No       |      |
| Metadata_Type     | No       | (1)  |
| Metadata_Supplier | No       |      |
| Metadata_Version  | No       |      |
| Bom_Version       | No       |      |

**Note**

1. This relates to the type of component which the SBOM is describing. This is attribute is only used for CycloneDX SBOMs.

There is an additional setter method, **set_value**(_attribute, value_) which allows the setting of any attribute.

`set_value("language", "Rust")`

**_Getter Methods_**

get_document()
Returns the SBOMDocument object as a list.

get_name()
Returns the name of the SBOMDocument object or a default value if the attribute does not exist within the instance of the SBOMDocument object.

get_value(attribute)
Returns the value of the attribute. A default value is returned if the attribute does not exist within the instance of the SBOMDocument object.

**Example**

```python
>>> from lib4sbom.data.document import SBOMDocument
>>> sbom_document = SBOMDocument()
>>> sbom_document.set_name("test_file")
>>> sbom_document.set_metadata_type("firmware")
>>> sbom_document.get_name()
'test_file'
>>> from lib4sbom.sbom import SBOM
>>> my_sbom = SBOM()
>>> my_sbom.add_document(sbom_document.get_document())
```

### SBOMFile Object

_class_ **SBOMFile**()

This creates a simple SBOM File object. This object contains the values of the attributes
that can be associated with a file artefact within an SBOM. This includes attributes such as name, identifier, type of file,
checksum and licence information. As each of the attributes are optional, it is left to the application manipulating the
SBOMFile object to apply validation as appropriate for the presence of each attribute.

**Methods**

**_Setter Methods_**

For the following attributes, a method **_set_attribute(value)_** is provided. Note that the attribute name is always in _lowercase_.
e.g. set_filetype(). The attribute names are aligned with the attributes of the File Object in the SPDX Specification. Unless
indicated, the method just takes a single parameter for the value. Where indicated, multiple instances of the attribute may be defined.

| Attribute         | Multiple | Note |
|-------------------|----------|------|
| Name              | No       |      |
| Id                | No       |      |
| FileType          | Yes      |      |
| Checksum          | Yes      | (1)  |
| LicenseConcluded  | No       |      |
| LicenceInfoInFile | Yes      |      |
| LicenceComment    | No       |      |
| CopyrightText     | No       |      |
| Comment           | No       |      |
| Notice            | No       |      |
| Contributor       | Yes      |      |
| Attribution       | No       |      |

**Note**

1. The set_checksum method takes two parameters, the checksum algorithm (e.g. SHA256) and the actual checksum value (as a string)

There is an additional setter method, **set_value**(_attribute, value_) which allows the setting of any attribute.

`set_value("language", "Rust")`

**_Getter Methods_**

get_file()
Returns the SBOMFile object as a dictionary. The value of an attribute is returned as a string except where multiple instances of an attribute are allowed in which
case the value of the attribute is returned as a List.

get_name()
Returns the name of the SBOMFile object or None if the 'name' attribute does not exist within the instance of the SBOMFile object.

get_value(attribute)
Returns the value of the attribute. A default value is returned if the attribute does not exist within the instance of the SBOMFile object.

_**Utility Methods**_

initialise() Reinitialises a SBOMFile Object. All data associated with the object is deleted.

**Example**

```python
>>> from lib4sbom.data.file import SBOMFile
>>> sbom_file = SBOMFile()
>>> sbom_files = {}
>>> sbom_file.initialise()
>>> sbom_file.set_name("test_file.c")
>>> sbom_file.set_licenseconcluded("MIT")
>>> file_hash = <<< some calculation >>>
>>> sbom_file.set_checksum("SHA1", file_hash)
>>> sbom_file.set_id("SPDXRef-File-0001")
>>> sbom_files[sbom_file.get_name()] = sbom_file.get_file()
>>> sbom_file.initialise()                                  
>>> sbom_file.set_name("makefile")                       
>>> sbom_file.set_licenseconcluded("NOASSERTION")                    
>>> sbom_file.set_id("SPDXRef-File-0002")                   
>>> sbom_files[sbom_file.get_name()] = sbom_file.get_file()
>>> from lib4sbom.sbom import SBOM
>>> my_sbom = SBOM()
>>> my_sbom.add_files(sbom_files)
```

### SBOMPackage Object
                
_class_ **SBOMPackage**()

This creates a simple SBOM Package object. This object contains the values of the attributes
that can be associated with a package or component artefact within an SBOM. This includes attributes such as name, identifier, supplier,
version and licence information. As each of the attributes are optional, it is left to the application manipulating the
SBOMPackage object to apply validation as appropriate for the presence of each attribute.

**_Setter Methods_**

For the following attributes, a method **_set_attribute(value)_** is provided. Note that the attribute name is always in _lowercase_.
e.g. set_version(). The attribute names are aligned with the attributes of the Package Object in the SPDX Specification. Unless
indicated, the method just takes a single parameter for the value. Where indicated, multiple instances of the attribute may be defined.

| Attribute         | Multiple | Note |
|-------------------|----------|------|
| Name              | No       |      |
| Id                | No       |      |
| Type              | No       | (1)  |
| Checksum          | Yes      | (2)  |
| LicenseConcluded  | No       |      |
| LicenseDeclared   | No       | (3)  |
| LicenceInfoInFile | Yes      |      |
| LicenceComments   | No       |      |
| FilesAnalysis     | No       |      |
| CopyrightText     | No       |      |
| Comment           | No       |      |
| Originator        | No       |      |
| Supplier          | No       |      |
| Version           | No       |      |
| Homepage          | No       |      |
| Property          | Yes      | (4)  |
| DownloadLocation  | No       |      |
| Description       | No       |      |
| ExternalReference | Yes      | (5)  |
| Cpe               | No       |      |
| Purl              | No       | (6)  |
| Summary           | No       |      |
| SourceInfo        | No       |      |
| Filename          | No       |      |

**Note**

1. The set_type method is used to indicate the purpose of the package (e.g. Application, Library, Operating-System).

2. The set_checksum method takes two parameters, the checksum algorithm (e.g. SHA256) and the actual checksum value (as a string)

3. The set_licensedeclared method takes an optional second parameter which is the license name. In this case the first parameter, license, is assumed to be the license text rather than the license identity.

4. The set_property method takes two parameters which are the property name and value.

5. The set_externalreference method takes three parameters which are the category (SECURITY or PACKAGE_MANAGER), type (cpe22Type, cpe23Type or purl) and the element corresponding to the tyoe.

6. The set_cpe takes an optional second parameter which is the CPE type (default is cpeType23).

There is an additional setter method, **set_value**(_attribute, value_) which allows the setting of any attribute.

`set_value("language", "Rust")`

**_Getter Methods_**

get_package()
Returns the SBOMPackage object as a dictionary. The value of an attribute is returned as a string except where multiple instances of an attribute are allowed in which
case the value of the attribute is returned as a List.

get_name()
Returns the name of the SBOMPackage object or None if the 'name' attribute does not exist within the instance of the SBOMPackage object.

get_value(attribute)
Returns the value of the attribute. A default value is returned if the attribute does not exist within the instance of the SBOMPackage object.

get_purl()
Returns the PURL identifier as a string for the package or None if no PURL element is defined.

get_cpe()
Returns the CPE identifier as a string for the package or None if no CPE element is defined.

**_Utility Methods_**

initialise() Reinitialises a SBOMPackage Object. All data associated with the object is deleted.

**Example**

```python
>>> from lib4sbom.data.package import SBOMPackage
>>> sbom_packages = {}
>>> my_package = SBOMPackage()
>>> my_package.set_name("glibc")
>>> my_package.set_version("2.15")
>>> my_package.set_supplier("organisation","gnu")
>>> my_package.set_licensedeclared("GPL3")
>>> sbom_packages[(my_package.get_name(), my_package.get_value('version'))] = my_package.get_package()
>>> my_package.initialise()
>>> my_package.set_name("tomcat")
>>> my_package.set_version("9.0.46")
>>> my_package.set_supplier("organisation","apache")
>>> my_package.set_licensedeclared("Apache-2.0")
>>> sbom_packages[(my_package.get_name(), my_package.get_value('version'))] = my_package.get_package()
>>> from lib4sbom.sbom import SBOM
>>> my_sbom = SBOM()
>>> my_sbom.add_packages(sbom_packages)
```

### SBOMRelationship Object

_class_ **SBOMRelationship**()

This creates a simple SBOMRelationship object which is used to show the relationship between two items within an SBOM.
As there are multiple types of relationships, it is left to the application manipulating the
SBOMRelationship object to apply validation as appropriate to ensure the semantics of the relationship are correct.

**_Setter Methods_**

set_relationship (source, type, target)

_source_ and _target_ are the unique identifiers of the components for which the relationship is being defined.

_type_ is the type of relationship being defined.

**_Getter Methods_**

get_relationship()
Returns the SBOMRelationship object as a dictionary.

**Example**

```python
>>> from lib4sbom.data.relationship import SBOMRelationship
>>> sbom_relationships = []
>>> my_relationship = SBOMRelationship()
>>> my_relationship.set_relationship("Package-1","CONTAINS", "Package-2")
>>> sbom_relationships.append(my_relationship)
>>> from lib4sbom.sbom import SBOM
>>> my_sbom = SBOM()
>>> my_sbom.add_relationships(sbom_relationships)
```

### Vulnerability Object

_class_ **Vulnerability**(validation = None)

This creates a simple vulnerability object which is used to define the details of a vulnerability typically for a component
specified within an SBOM. As there are multiple ways of specifying the status of a vulnerability, it is left to the
application manipulating the Vulnerability object to apply validation as appropriate to ensure the semantics of the vulnerability are correct.

The following optional parameter can be specified:

_validation_ indicates that the status field is to be validated against the [OpenVEX](https://openvex.dev), [CycloneDX](https://www.cyclonedx.org) or [CSAF](https://docs.oasis-open.org/csaf/csaf/v2.0/csaf-v2.0.html) specifcaitions.

**NOTE** Vulnerability objects are only included in CyclonedDX SBOMs

**_Setter Methods_**

For the following attributes, a method **_set_attribute(value)_** is provided. Note that the attribute name is always in _lowercase_.
e.g. set_release(). Each method takes a single parameter for the value. Multiple instances of the attribute are not allowed.


| Attribute         | Multiple | Note |
|-------------------|----------|------|
| Name              | No       |      |
| Id                | No       | (1)  |
| Release           | No       |      |
| Status            | No       | (2)  |
| Comment           | No       | (3)  |
| Description       | No       | (4)  |


**Note**

1. The set_id method is used to indicate the identity of the vulnerability e.g. CVE-2021-44228

2. The set_status is used to indicate the status of the vulnerability. Validation of the value of status may be optionally performed as determined by
the optional parameter _validation_ specified in the creation of the Vulnerability object. An invalid status is indicated by a value of None.

3. The set_comment method is used to provide additional information to support the status value e.g. a brief justification

4. The set_description method is used to describe the vulnerability.

There is an additional setter method, **set_value**(_attribute, value_) which allows the setting of any attribute.

`set_value("bom-ref", "rust@1.2.3")`

**_Getter Methods_**

get_vulnerability()
Returns the vulnerability object as a dictionary.

**Example**

```python
>>> from lib4sbom.data.vulnerability import Vulnerability
>>> vulnerabilities = []
>>> vulnerability = Vulnerability(validation="cyclonedx")
>>> vulnerability.set_id("CVE-2023-1235")
>>> vulnerability.set_name("rust")
>>> vulnerability.set_release("1.2.3")
>>> vulnerability.set_value("bom-ref", "rust@1.2.3")
>>> vulnerability.set_status("in_triage")
>>> vulnerabilities.append(vulnerability.get_vulnerability())
>>> from lib4sbom.sbom import SBOM
>>> my_sbom = SBOM()
>>> my_sbom.add_vulnerabilities(vulnerabilities)
```

### Services Object

_class_ **SBOMService**(validation = None)

This creates a simple software service object which is used to define the details of a software service.
As there are multiple ways of specifying a service, it is left to the
application manipulating the service object to apply validation as appropriate to ensure the semantics of the service are correct.

**NOTE** Services objects are only included in CyclonedDX SBOMs

**_Setter Methods_**

For the following attributes, a method **_set_attribute(value)_** is provided. Note that the attribute name is always in _lowercase_.
e.g. set_release(). Each method takes a single parameter for the value. Multiple instances of the attribute are not allowed.


| Attribute        | Multiple | Note |
|------------------|----------|------|
| Name             | No       |      |
| Id               | No       | (1)  |
| Version          | No       |      |
| Provider         | No       | (2)  |
| Endpoint         | Yes      |      |
| Data             | Yes      | (3)  |
| Property         | Yes      | (4)  |
| License          | Yes      |      |
| Exernalreference | Yes      | (5)  |
| Description      | No       |      |


**Note**

1. The set_id method is used to indicate the identity of the service. If this is not specified, an id will be automatically generated.

2. The set_provider is used to specify details of the service provider. There are multiple parameters which can be specified (name, url, contactname, email address and phone) at least one must be specified.

3. The set_data method is used to provide additional information to describe the data being exhanged. There are two mandatatory parameters flow type ("Inbound", "Outbound", "Bi-directional" or "Unknown") and classification, and two optional paramters name and description.

4. The set_property method takes two parameters which are the property name and value.

5. The set_externalreference method takes three parameters, the URL, the type of information being referenced and an optional comment.

There is an additional setter method, **set_value**(_attribute, value_) which allows the setting of any attribute.

`set_value("trustzone", "Data_DMZ")`

**_Getter Methods_**

get_service()
Returns the service object as a dictionary.

**Example**

```python
>>> from lib4sbom.data.service import SBOMService
>>> sbom_services = {}
>>> my_service=SBOMService()
>>> my_service.set_name("Microsoft 365")
>>> my_service.set_version("2022.04")
>>> my_service.set_provider(name="Microsoft Inc.", contact="Fred Flintstone", email="fred@micrsoft.com")
>>> my_service.set_description("Business productivity suite")
>>> my_service.set_value("authenticated",True)
>>> my_service.set_endpoint("www.microsoft.com")
>>> my_service.set_endpoint("www.microsoft.com/owa")
>>> my_service.set_data("Bi-directional","None",description="document")
>>> my_service.set_data("outbound","PII",name="User information")
>>> my_service.set_license("Apache-2.0")
>>> my_service.set_license("MIT")
>>> my_service.set_property("Data_Location","EU")
>>> my_service.set_externalreference("https://www.microsoft.com","Website", "Company website")
>>> sbom_services[(my_service.get_name(), my_service.get_value('version'))] = my_service.get_service()
>>> from lib4sbom.sbom import SBOM
>>> my_sbom = SBOM()
>>> my_sbom.add_services(sbom_services)
```

## Examples

A number of example scripts are included in the _examples_ subdirectory.
						
## Implementation Notes

The following design decisions have been made in processing the SBOM files:

1. It is assumed that the SBOM is valid and contains syntactically valid data.

2. In SPDX format, the tool assumes that the name of a package preceeds the version and license of the package.

3. In SPDX format, the current implementation does not currently handle multi-line elements.

4. When processing and validating licenses, the application will use a set of synonyms to attempt to map some license identifiers to the correct [SPDX License Identifiers](https://spdx.org/licenses/). However, the
user of the tool is reminded that they should assert the quality of any data which is provided by the tool particularly where the license identifier has been modified.

## Future Development

1. Support later versions of SPDX (3.0).

2. Enhance validation of SBOM data to check for all mandatory elements.

3. Implement Python typing across modules.

4. Migrate packaging infrastructure away from setup.py.

5. Utilise third-party SPDX and CycloneDX parsers and generators

6. Add further support for SPDX XML and RDF formats

7. Add generator for CycloneDX XML documents.

8. Implement test suite.

## License

Licensed under the Apache 2.0 Licence.

The tool uses a local copy of the [SPDX Licenses List](https://github.com/spdx/license-list-data) which is released under
[Creative Commons Attribution 3.0 (CC-BY-3.0)](http://creativecommons.org/licenses/by/3.0/).

This tool uses information sourced from the [Blue Oak Council's License List](https://blueoakcouncil.org/list) which is released under
[Creative Commons Attribution 1.0 (CC-BY-1.0)](https://creativecommons.org/licenses/by/1.0/).

## Limitations

This tool is meant to support software development. The usefulness of the tool is dependent on the SBOM data
which is provided to the tool. Unfortunately, the tool is unable to determine the validity or completeness of such a SBOM file; users of the tool
are therefore reminded that they should assert the quality of any data which is provided to the tool.

## Feedback and Contributions

Bugs and feature requests can be made via GitHub Issues.
"""^^xsd:string ;
    schema1:name "lib4sbom"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.7.5"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/license-expression> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/Sphinx>,
        <https://data.siemens-energy.com/cybersec/package/doc8>,
        <https://data.siemens-energy.com/cybersec/package/pytest>,
        <https://data.siemens-energy.com/cybersec/package/pytest-xdist>,
        <https://data.siemens-energy.com/cybersec/package/sphinx-reredirects>,
        <https://data.siemens-energy.com/cybersec/package/sphinx-rtd-dark-mode>,
        <https://data.siemens-energy.com/cybersec/package/sphinx-rtd-theme>,
        <https://data.siemens-energy.com/cybersec/package/sphinxcontrib-apidoc> ;
    schema1:description """==================
license-expression
==================

``license-expression`` is a comprehensive utility library to parse, compare,
simplify and normalize license expressions (such as SPDX license expressions)
using boolean logic.

- License: Apache-2.0
- Python: 3.8+
- Homepage: https://github.com/aboutcode-org/license-expression/
- Install: `pip install license-expression` also available in most Linux distro.

Software project licenses are often a combination of several free and open
source software licenses. License expressions -- as specified by SPDX -- provide
a concise and human readable way to express these licenses without having to
read long license texts, while still being machine-readable.

License expressions are used by key FOSS projects such as Linux; several
packages ecosystem use them to document package licensing metadata such as
npm and Rubygems; they are important when exchanging software data (such as with
SPDX and SBOM in general) as a way to express licensing precisely.

``license-expression`` is a comprehensive utility library to parse, compare,
simplify and normalize these license expressions (such as SPDX license expressions)
using boolean logic like in: `GPL-2.0-or-later WITH Classpath-exception-2.0 AND MIT`.

It includes the license keys from SPDX https://spdx.org/licenses/ (version 3.23)
and ScanCode license DB (version 32.0.8, last published on 2023-02-27).
See https://scancode-licensedb.aboutcode.org/ to get started quickly.

``license-expression`` is both powerful and simple to use and is a used as the
license expression engine in several projects and products such as:

- AboutCode-toolkit https://github.com/aboutcode-org/aboutcode-toolkit
- AlekSIS (School Information System) https://edugit.org/AlekSIS/official/AlekSIS-Core
- Barista https://github.com/Optum/barista
- Conda forge tools https://github.com/conda-forge/conda-smithy
- DejaCode https://dejacode.com
- DeltaCode https://github.com/nexB/deltacode
- FenixscanX https://github.com/SmartsYoung/FenixscanX
- FetchCode https://github.com/aboutcode-org/fetchcode
- Flict https://github.com/vinland-technology/flict and https://github.com/vinland-technology
- license.sh https://github.com/webscopeio/license.sh
- liferay_inbound_checker https://github.com/carmenbianca/liferay_inbound_checker
- REUSE https://reuse.software/ and https://github.com/fsfe/reuse-tool
- ScanCode-io https://github.com/aboutcode-org/scancode.io
- ScanCode-toolkit https://github.com/aboutcode-org/scancode-toolkit

See also for details:
- https://spdx.github.io/spdx-spec/appendix-IV-SPDX-license-expressions/

``license-expression`` is also packaged for most Linux distributions. See below.

Alternative:

There is no known alternative library for Python, but there are several similar
libraries in other languages (but not as powerful of course!):

- JavaScript https://github.com/jslicense/spdx-expression-parse.js
- Rust https://github.com/ehuss/license-exprs
- Haskell https://github.com/phadej/spdx
- Go https://github.com/kyoh86/go-spdx
- Ada https://github.com/Fabien-Chouteau/spdx_ada
- Java https://github.com/spdx/tools and https://github.com/aschet/spdx-license-expression-tools

Build and tests status
======================

+--------------------------+------------------------+----------------------------------+
|**Linux & macOS (Travis)**| **Windows (AppVeyor)** |**Linux, Windows & macOS (Azure)**|
+==========================+========================+==================================+
|                          |                        |                                  |
| |travis-badge-icon|      | |appveyor-badge-icon|  |   |azure-badge-icon|             |
|                          |                        |                                  |
+--------------------------+------------------------+----------------------------------+

Source code and download
========================

- GitHub https://github.com/aboutcode-org/license-expression.git
- PyPI https://pypi.python.org/pypi/license-expression

Also available in several Linux distros:

- Arch Linux https://archlinux.org/packages/extra/any/python-license-expression/
- Debian https://packages.debian.org/unstable/source/license-expression
- DragonFly BSD https://github.com/DragonFlyBSD/DPorts/tree/master/textproc/py-license-expression
- Fedora https://src.fedoraproject.org/rpms/python-license-expression/
- FreeBSD https://www.freshports.org/textproc/py-license-expression
- NixOS https://github.com/NixOS/nixpkgs/blob/release-21.05/pkgs/development/python-modules/license-expression/default.nix
- openSUSE https://build.opensuse.org/package/show/openSUSE:Factory/python-license-expression


Support
=======

- Submit bugs and questions at: https://github.com/aboutcode-org/license-expression/issues
- Join the chat at: https://gitter.im/aboutcode-org/discuss

Description
===========

This module defines a mini language to parse, validate, simplify, normalize and
compare license expressions using a boolean logic engine.

This supports SPDX license expressions and also accepts other license naming
conventions and license identifiers aliases to resolve and normalize any license
expressions.

Using boolean logic, license expressions can be tested for equality, containment,
equivalence and can be normalized or simplified.

It also bundles the SPDX License list (3.20 as of now) and the ScanCode license
DB (based on latest ScanCode) to easily parse and validate expressions using
the license symbols.


Usage examples
==============

The main entry point is the ``Licensing`` object that you can use to parse,
validate, compare, simplify and normalize license expressions.

Create an SPDX Licensing and parse expressions::

	>>> from license_expression import get_spdx_licensing
	>>> licensing = get_spdx_licensing()
	>>> expression = ' GPL-2.0 or LGPL-2.1 and mit '
	>>> parsed = licensing.parse(expression)
	>>> print(parsed.pretty())
	OR(
	  LicenseSymbol('GPL-2.0-only'),
	  AND(
	    LicenseSymbol('LGPL-2.1-only'),
	    LicenseSymbol('MIT')
	  )
	)

	>>> str(parsed)
	'GPL-2.0-only OR (LGPL-2.1-only AND MIT)'

	>>> licensing.parse('unknwon with foo', validate=True, strict=True)
	license_expression.ExpressionParseError: A plain license symbol cannot be used
	as an exception in a "WITH symbol" statement. for token: "foo" at position: 13

	>>> licensing.parse('unknwon with foo', validate=True)
	license_expression.ExpressionError: Unknown license key(s): unknwon, foo

	>>> licensing.validate('foo and MIT and GPL-2.0+')
	ExpressionInfo(
	    original_expression='foo and MIT and GPL-2.0+',
	    normalized_expression=None,
	    errors=['Unknown license key(s): foo'],
	    invalid_symbols=['foo']
	)


Create a simple Licensing and parse expressions::

    >>> from license_expression import Licensing, LicenseSymbol
    >>> licensing = Licensing()
    >>> expression = ' GPL-2.0 or LGPL-2.1 and mit '
    >>> parsed = licensing.parse(expression)
    >>> expression = ' GPL-2.0 or LGPL-2.1 and mit '
    >>> expected = 'GPL-2.0-only OR (LGPL-2.1-only AND mit)'
    >>> assert str(parsed) == expected
    >>> assert parsed.render('{symbol.key}') == expected


Create a Licensing with your own license symbols::

    >>> expected = [
    ...   LicenseSymbol('GPL-2.0'),
    ...   LicenseSymbol('LGPL-2.1'),
    ...   LicenseSymbol('mit')
    ... ]
    >>> assert licensing.license_symbols(expression) == expected
    >>> assert licensing.license_symbols(parsed) == expected

    >>> symbols = ['GPL-2.0+', 'Classpath', 'BSD']
    >>> licensing = Licensing(symbols)
    >>> expression = 'GPL-2.0+ with Classpath or (bsd)'
    >>> parsed = licensing.parse(expression)
    >>> expected = 'GPL-2.0+ WITH Classpath OR BSD'
    >>> assert parsed.render('{symbol.key}') == expected

    >>> expected = [
    ...   LicenseSymbol('GPL-2.0+'),
    ...   LicenseSymbol('Classpath'),
    ...   LicenseSymbol('BSD')
    ... ]
    >>> assert licensing.license_symbols(parsed) == expected
    >>> assert licensing.license_symbols(expression) == expected

And expression can be deduplicated, to remove duplicate license subexpressions
without changing the order and without consider license choices as simplifiable::

    >>> expression2 = ' GPL-2.0 or (mit and LGPL 2.1) or bsd Or GPL-2.0  or (mit and LGPL 2.1)'
    >>> parsed2 = licensing.parse(expression2)
    >>> str(parsed2)
    'GPL-2.0 OR (mit AND LGPL 2.1) OR BSD OR GPL-2.0 OR (mit AND LGPL 2.1)'
    >>> assert str(parsed2.simplify()) == 'BSD OR GPL-2.0 OR (LGPL 2.1 AND mit)'

Expression can be simplified, treating them as boolean expressions::

    >>> expression2 = ' GPL-2.0 or (mit and LGPL 2.1) or bsd Or GPL-2.0  or (mit and LGPL 2.1)'
    >>> parsed2 = licensing.parse(expression2)
    >>> str(parsed2)
    'GPL-2.0 OR (mit AND LGPL 2.1) OR BSD OR GPL-2.0 OR (mit AND LGPL 2.1)'
    >>> assert str(parsed2.simplify()) == 'BSD OR GPL-2.0 OR (LGPL 2.1 AND mit)'

Two expressions can be compared for equivalence and containment:

    >>> expr1 = licensing.parse(' GPL-2.0 or (LGPL 2.1 and mit) ')
    >>> expr2 = licensing.parse(' (mit and LGPL 2.1)  or GPL-2.0 ')
    >>> licensing.is_equivalent(expr1, expr2)
    True
    >>> licensing.is_equivalent(' GPL-2.0 or (LGPL 2.1 and mit) ',
    ...                         ' (mit and LGPL 2.1)  or GPL-2.0 ')
    True
    >>> expr1.simplify() == expr2.simplify()
    True
    >>> expr3 = licensing.parse(' GPL-2.0 or mit or LGPL 2.1')
    >>> licensing.is_equivalent(expr2, expr3)
    False
    >>> expr4 = licensing.parse('mit and LGPL 2.1')
    >>> expr4.simplify() in expr2.simplify()
    True
    >>> licensing.contains(expr2, expr4)
    True

Development
===========

- Checkout a clone from https://github.com/aboutcode-org/license-expression.git

- Then run ``./configure --dev`` and then ``source tmp/bin/activate`` on Linux and POSIX.
  This will install all dependencies in a local virtualenv, including
  development deps.

- On Windows run  ``configure.bat --dev`` and then ``Scripts\\bin\\activate`` instead.

- To run the tests, run ``pytest -vvs``


.. |travis-badge-icon| image:: https://api.travis-ci.org/nexB/license-expression.png?branch=master
    :target: https://travis-ci.org/nexB/license-expression
    :alt: Travis tests status
    :align: middle

.. |appveyor-badge-icon| image:: https://ci.appveyor.com/api/projects/status/github/nexB/license-expression?svg=true
    :target: https://ci.appveyor.com/project/nexB/license-expression
    :alt: Appveyor tests status
    :align: middle

.. |azure-badge-icon| image:: https://dev.azure.com/nexB/license-expression/_apis/build/status/nexB.license-expression?branchName=master
    :target: https://dev.azure.com/nexB/license-expression/_build/latest?definitionId=2&branchName=master
    :alt: Azure pipelines tests status
    :align: middle

"""^^xsd:string ;
    schema1:name "license-expression"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "30.3.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/matplotlib-inline> a software_vocab:Software_package ;
    schema1:description """# Matplotlib Inline Back-end for IPython and Jupyter

This package provides support for matplotlib to display figures directly inline in the Jupyter notebook and related clients, as shown below.

## Installation

With conda:

```bash
conda install -c conda-forge matplotlib-inline
```

With pip:

```bash
pip install matplotlib-inline
```

## Usage

Note that in current versions of JupyterLab and Jupyter Notebook, the explicit use of the `%matplotlib inline` directive is not needed anymore, though other third-party clients may still require it.

This will produce a figure immediately below:

```python
%matplotlib inline

import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0, 3*np.pi, 500)
plt.plot(x, np.sin(x**2))
plt.title('A simple chirp');
```

## License

Licensed under the terms of the BSD 3-Clause License, by the IPython Development Team (see `LICENSE` file).
"""^^xsd:string ;
    schema1:name "matplotlib-inline"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.1.7"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/packageurl-python> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/sqlalchemy> ;
    schema1:description """=================
packageurl-python
=================

Python library to parse and build "purl" aka. Package URLs.
See https://github.com/package-url/purl-spec for details.

Join the discussion at https://gitter.im/package-url/Lobby or enter a ticket for support.

License: MIT

Tests and build status
======================

+----------------------+
| **Tests and build**  |
+======================+
| |ci-tests|           |
+----------------------+

Install
=======
::

    pip install packageurl-python

Usage
=====
::

    >>> from packageurl import PackageURL

    >>> purl = PackageURL.from_string("pkg:maven/org.apache.commons/io@1.3.4")
    >>> print(purl.to_dict())
    {'type': 'maven', 'namespace': 'org.apache.commons', 'name': 'io', 'version': '1.3.4', 'qualifiers': None, 'subpath': None}

    >>> print(purl.to_string())
    pkg:maven/org.apache.commons/io@1.3.4

    >>> print(str(purl))
    pkg:maven/org.apache.commons/io@1.3.4

    >>> print(repr(purl))
    PackageURL(type='maven', namespace='org.apache.commons', name='io', version='1.3.4', qualifiers={}, subpath=None)

Utilities
=========

Django models
^^^^^^^^^^^^^

`packageurl.contrib.django.models.PackageURLMixin` is a Django abstract model mixin to
use Package URLs in Django.

SQLAlchemy mixin
^^^^^^^^^^^^^^^^

`packageurl.contrib.sqlalchemy.mixin.PackageURLMixin` is a SQLAlchemy declarative mixin
to use Package URLs in SQLAlchemy models.

URL to PURL
^^^^^^^^^^^

`packageurl.contrib.url2purl.get_purl(url)` returns a Package URL inferred from an URL.

::

    >>> from packageurl.contrib import url2purl
    >>> url2purl.get_purl("https://github.com/package-url/packageurl-python")
    PackageURL(type='github', namespace='package-url', name='packageurl-python', version=None, qualifiers={}, subpath=None)

PURL to URL
^^^^^^^^^^^

- `packageurl.contrib.purl2url.get_repo_url(purl)` returns a repository URL inferred
  from a Package URL.
- `packageurl.contrib.purl2url.get_download_url(purl)` returns a download URL inferred
  from a Package URL.
- `packageurl.contrib.purl2url.get_inferred_urls(purl)` return all inferred URLs
  (repository, download) from a Package URL.

::

    >>> from packageurl.contrib import purl2url

    >>> purl2url.get_repo_url("pkg:gem/bundler@2.3.23")
    "https://rubygems.org/gems/bundler/versions/2.3.23"

    >>> purl2url.get_download_url("pkg:gem/bundler@2.3.23")
    "https://rubygems.org/downloads/bundler-2.3.23.gem"

    >>> purl2url.get_inferred_urls("pkg:gem/bundler@2.3.23")
    ["https://rubygems.org/gems/bundler/versions/2.3.23", "https://rubygems.org/downloads/bundler-2.3.23.gem"]

Run tests
=========

Install test dependencies::

    python3 thirdparty/virtualenv.pyz --never-download --no-periodic-update .
    bin/pip install -e ."[test]"

Run tests::

    bin/py.test tests

Make a new release
==================

- Start a new release branch
- Update the CHANGELOG.rst, AUTHORS.rst, and README.rst if needed
- Bump version in setup.cfg
- Run all tests
- Install restview and validate that all .rst docs are correct
- Commit and push this branch
- Make a PR and merge once approved
- Tag and push that tag. This triggers the pypi-release.yml workflow that takes care of
  building the dist release files and upload those to pypi::

    VERSION=v0.x.x
    git tag -a $VERSION -m "Tag $VERSION"
    git push origin $VERSION

- Review the GitHub release created by the workflow at
  https://github.com/package-url/packageurl-python/releases

.. |ci-tests| image:: https://github.com/package-url/packageurl-python/actions/workflows/ci.yml/badge.svg?branch=main
    :target: https://github.com/package-url/packageurl-python/actions/workflows/ci.yml
    :alt: CI Tests and build status
"""^^xsd:string ;
    schema1:name "packageurl-python"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.15.6"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pandas> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/PyQt5>,
        <https://data.siemens-energy.com/cybersec/package/SQLAlchemy>,
        <https://data.siemens-energy.com/cybersec/package/adbc-driver-postgresql>,
        <https://data.siemens-energy.com/cybersec/package/adbc-driver-sqlite>,
        <https://data.siemens-energy.com/cybersec/package/beautifulsoup4>,
        <https://data.siemens-energy.com/cybersec/package/bottleneck>,
        <https://data.siemens-energy.com/cybersec/package/dataframe-api-compat>,
        <https://data.siemens-energy.com/cybersec/package/fastparquet>,
        <https://data.siemens-energy.com/cybersec/package/fsspec>,
        <https://data.siemens-energy.com/cybersec/package/gcsfs>,
        <https://data.siemens-energy.com/cybersec/package/html5lib>,
        <https://data.siemens-energy.com/cybersec/package/hypothesis>,
        <https://data.siemens-energy.com/cybersec/package/jinja2>,
        <https://data.siemens-energy.com/cybersec/package/lxml>,
        <https://data.siemens-energy.com/cybersec/package/matplotlib>,
        <https://data.siemens-energy.com/cybersec/package/numba>,
        <https://data.siemens-energy.com/cybersec/package/numexpr>,
        <https://data.siemens-energy.com/cybersec/package/numpy>,
        <https://data.siemens-energy.com/cybersec/package/odfpy>,
        <https://data.siemens-energy.com/cybersec/package/openpyxl>,
        <https://data.siemens-energy.com/cybersec/package/pandas-gbq>,
        <https://data.siemens-energy.com/cybersec/package/psycopg2>,
        <https://data.siemens-energy.com/cybersec/package/pyarrow>,
        <https://data.siemens-energy.com/cybersec/package/pymysql>,
        <https://data.siemens-energy.com/cybersec/package/pyreadstat>,
        <https://data.siemens-energy.com/cybersec/package/pytest>,
        <https://data.siemens-energy.com/cybersec/package/pytest-xdist>,
        <https://data.siemens-energy.com/cybersec/package/python-calamine>,
        <https://data.siemens-energy.com/cybersec/package/python-dateutil>,
        <https://data.siemens-energy.com/cybersec/package/pytz>,
        <https://data.siemens-energy.com/cybersec/package/pyxlsb>,
        <https://data.siemens-energy.com/cybersec/package/qtpy>,
        <https://data.siemens-energy.com/cybersec/package/s3fs>,
        <https://data.siemens-energy.com/cybersec/package/scipy>,
        <https://data.siemens-energy.com/cybersec/package/tables>,
        <https://data.siemens-energy.com/cybersec/package/tabulate>,
        <https://data.siemens-energy.com/cybersec/package/tzdata>,
        <https://data.siemens-energy.com/cybersec/package/xarray>,
        <https://data.siemens-energy.com/cybersec/package/xlrd>,
        <https://data.siemens-energy.com/cybersec/package/xlsxwriter>,
        <https://data.siemens-energy.com/cybersec/package/zstandard> ;
    schema1:description """<div align="center">
  <img src="https://pandas.pydata.org/static/img/pandas.svg"><br>
</div>

-----------------

# pandas: powerful Python data analysis toolkit

| | |
| --- | --- |
| Testing | [![CI - Test](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/pandas-dev/pandas/actions/workflows/unit-tests.yml) [![Coverage](https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=main)](https://codecov.io/gh/pandas-dev/pandas) |
| Package | [![PyPI Latest Release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/) [![PyPI Downloads](https://img.shields.io/pypi/dm/pandas.svg?label=PyPI%20downloads)](https://pypi.org/project/pandas/) [![Conda Latest Release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/conda-forge/pandas) [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/pandas.svg?label=Conda%20downloads)](https://anaconda.org/conda-forge/pandas) |
| Meta | [![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3509134.svg)](https://doi.org/10.5281/zenodo.3509134) [![License - BSD 3-Clause](https://img.shields.io/pypi/l/pandas.svg)](https://github.com/pandas-dev/pandas/blob/main/LICENSE) [![Slack](https://img.shields.io/badge/join_Slack-information-brightgreen.svg?logo=slack)](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack) |


## What is it?

**pandas** is a Python package that provides fast, flexible, and expressive data
structures designed to make working with "relational" or "labeled" data both
easy and intuitive. It aims to be the fundamental high-level building block for
doing practical, **real world** data analysis in Python. Additionally, it has
the broader goal of becoming **the most powerful and flexible open source data
analysis / manipulation tool available in any language**. It is already well on
its way towards this goal.

## Table of Contents

- [Main Features](#main-features)
- [Where to get it](#where-to-get-it)
- [Dependencies](#dependencies)
- [Installation from sources](#installation-from-sources)
- [License](#license)
- [Documentation](#documentation)
- [Background](#background)
- [Getting Help](#getting-help)
- [Discussion and Development](#discussion-and-development)
- [Contributing to pandas](#contributing-to-pandas)

## Main Features
Here are just a few of the things that pandas does well:

  - Easy handling of [**missing data**][missing-data] (represented as
    `NaN`, `NA`, or `NaT`) in floating point as well as non-floating point data
  - Size mutability: columns can be [**inserted and
    deleted**][insertion-deletion] from DataFrame and higher dimensional
    objects
  - Automatic and explicit [**data alignment**][alignment]: objects can
    be explicitly aligned to a set of labels, or the user can simply
    ignore the labels and let `Series`, `DataFrame`, etc. automatically
    align the data for you in computations
  - Powerful, flexible [**group by**][groupby] functionality to perform
    split-apply-combine operations on data sets, for both aggregating
    and transforming data
  - Make it [**easy to convert**][conversion] ragged,
    differently-indexed data in other Python and NumPy data structures
    into DataFrame objects
  - Intelligent label-based [**slicing**][slicing], [**fancy
    indexing**][fancy-indexing], and [**subsetting**][subsetting] of
    large data sets
  - Intuitive [**merging**][merging] and [**joining**][joining] data
    sets
  - Flexible [**reshaping**][reshape] and [**pivoting**][pivot-table] of
    data sets
  - [**Hierarchical**][mi] labeling of axes (possible to have multiple
    labels per tick)
  - Robust IO tools for loading data from [**flat files**][flat-files]
    (CSV and delimited), [**Excel files**][excel], [**databases**][db],
    and saving/loading data from the ultrafast [**HDF5 format**][hdfstore]
  - [**Time series**][timeseries]-specific functionality: date range
    generation and frequency conversion, moving window statistics,
    date shifting and lagging


   [missing-data]: https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html
   [insertion-deletion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#column-selection-addition-deletion
   [alignment]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html?highlight=alignment#intro-to-data-structures
   [groupby]: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#group-by-split-apply-combine
   [conversion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe
   [slicing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#slicing-ranges
   [fancy-indexing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#advanced
   [subsetting]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing
   [merging]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging
   [joining]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#joining-on-index
   [reshape]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html
   [pivot-table]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html
   [mi]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#hierarchical-indexing-multiindex
   [flat-files]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#csv-text-files
   [excel]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#excel-files
   [db]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#sql-queries
   [hdfstore]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#hdf5-pytables
   [timeseries]: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-series-date-functionality

## Where to get it
The source code is currently hosted on GitHub at:
https://github.com/pandas-dev/pandas

Binary installers for the latest released version are available at the [Python
Package Index (PyPI)](https://pypi.org/project/pandas) and on [Conda](https://docs.conda.io/en/latest/).

```sh
# conda
conda install -c conda-forge pandas
```

```sh
# or PyPI
pip install pandas
```

The list of changes to pandas between each release can be found
[here](https://pandas.pydata.org/pandas-docs/stable/whatsnew/index.html). For full
details, see the commit logs at https://github.com/pandas-dev/pandas.

## Dependencies
- [NumPy - Adds support for large, multi-dimensional arrays, matrices and high-level mathematical functions to operate on these arrays](https://www.numpy.org)
- [python-dateutil - Provides powerful extensions to the standard datetime module](https://dateutil.readthedocs.io/en/stable/index.html)
- [pytz - Brings the Olson tz database into Python which allows accurate and cross platform timezone calculations](https://github.com/stub42/pytz)

See the [full installation instructions](https://pandas.pydata.org/pandas-docs/stable/install.html#dependencies) for minimum supported versions of required, recommended and optional dependencies.

## Installation from sources
To install pandas from source you need [Cython](https://cython.org/) in addition to the normal
dependencies above. Cython can be installed from PyPI:

```sh
pip install cython
```

In the `pandas` directory (same one where you found this file after
cloning the git repo), execute:

```sh
pip install .
```

or for installing in [development mode](https://pip.pypa.io/en/latest/cli/pip_install/#install-editable):


```sh
python -m pip install -ve . --no-build-isolation --config-settings=editable-verbose=true
```

See the full instructions for [installing from source](https://pandas.pydata.org/docs/dev/development/contributing_environment.html).

## License
[BSD 3](LICENSE)

## Documentation
The official documentation is hosted on [PyData.org](https://pandas.pydata.org/pandas-docs/stable/).

## Background
Work on ``pandas`` started at [AQR](https://www.aqr.com/) (a quantitative hedge fund) in 2008 and
has been under active development since then.

## Getting Help

For usage questions, the best place to go to is [StackOverflow](https://stackoverflow.com/questions/tagged/pandas).
Further, general questions and discussions can also take place on the [pydata mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata).

## Discussion and Development
Most development discussions take place on GitHub in this repo, via the [GitHub issue tracker](https://github.com/pandas-dev/pandas/issues).

Further, the [pandas-dev mailing list](https://mail.python.org/mailman/listinfo/pandas-dev) can also be used for specialized discussions or design issues, and a [Slack channel](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack) is available for quick development related questions.

There are also frequent [community meetings](https://pandas.pydata.org/docs/dev/development/community.html#community-meeting) for project maintainers open to the community as well as monthly [new contributor meetings](https://pandas.pydata.org/docs/dev/development/community.html#new-contributor-meeting) to help support new contributors.

Additional information on the communication channels can be found on the [contributor community](https://pandas.pydata.org/docs/development/community.html) page.

## Contributing to pandas

[![Open Source Helpers](https://www.codetriage.com/pandas-dev/pandas/badges/users.svg)](https://www.codetriage.com/pandas-dev/pandas)

All contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.

A detailed overview on how to contribute can be found in the **[contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)**.

If you are simply looking to start working with the pandas codebase, navigate to the [GitHub "issues" tab](https://github.com/pandas-dev/pandas/issues) and start looking through interesting issues. There are a number of issues listed under [Docs](https://github.com/pandas-dev/pandas/issues?labels=Docs&sort=updated&state=open) and [good first issue](https://github.com/pandas-dev/pandas/issues?labels=good+first+issue&sort=updated&state=open) where you could start out.

You can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to [subscribe to pandas on CodeTriage](https://www.codetriage.com/pandas-dev/pandas).

Or maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking â€˜this can be improvedâ€™...you can do something about it!

Feel free to ask questions on the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata) or on [Slack](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack).

As contributors and maintainers to this project, you are expected to abide by pandas' code of conduct. More information can be found at: [Contributor Code of Conduct](https://github.com/pandas-dev/.github/blob/master/CODE_OF_CONDUCT.md)

<hr>

[Go to Top](#table-of-contents)
"""^^xsd:string ;
    schema1:name "pandas"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.2.3"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/parso> a software_vocab:Software_package ;
    schema1:description """###################################################################
parso - A Python Parser
###################################################################


.. image:: https://github.com/davidhalter/parso/workflows/Build/badge.svg?branch=master
    :target: https://github.com/davidhalter/parso/actions
    :alt: GitHub Actions build status

.. image:: https://coveralls.io/repos/github/davidhalter/parso/badge.svg?branch=master
    :target: https://coveralls.io/github/davidhalter/parso?branch=master
    :alt: Coverage Status

.. image:: https://pepy.tech/badge/parso
    :target: https://pepy.tech/project/parso
    :alt: PyPI Downloads

.. image:: https://raw.githubusercontent.com/davidhalter/parso/master/docs/_static/logo_characters.png

Parso is a Python parser that supports error recovery and round-trip parsing
for different Python versions (in multiple Python versions). Parso is also able
to list multiple syntax errors in your python file.

Parso has been battle-tested by jedi_. It was pulled out of jedi to be useful
for other projects as well.

Parso consists of a small API to parse Python and analyse the syntax tree.

A simple example:

.. code-block:: python

    >>> import parso
    >>> module = parso.parse('hello + 1', version="3.9")
    >>> expr = module.children[0]
    >>> expr
    PythonNode(arith_expr, [<Name: hello@1,0>, <Operator: +>, <Number: 1>])
    >>> print(expr.get_code())
    hello + 1
    >>> name = expr.children[0]
    >>> name
    <Name: hello@1,0>
    >>> name.end_pos
    (1, 5)
    >>> expr.end_pos
    (1, 9)

To list multiple issues:

.. code-block:: python

    >>> grammar = parso.load_grammar()
    >>> module = grammar.parse('foo +\\nbar\\ncontinue')
    >>> error1, error2 = grammar.iter_errors(module)
    >>> error1.message
    'SyntaxError: invalid syntax'
    >>> error2.message
    "SyntaxError: 'continue' not properly in loop"

Resources
=========

- `Testing <https://parso.readthedocs.io/en/latest/docs/development.html#testing>`_
- `PyPI <https://pypi.python.org/pypi/parso>`_
- `Docs <https://parso.readthedocs.org/en/latest/>`_
- Uses `semantic versioning <https://semver.org/>`_

Installation
============

    pip install parso

Future
======

- There will be better support for refactoring and comments. Stay tuned.
- There's a WIP PEP8 validator. It's however not in a good shape, yet.

Known Issues
============

- `async`/`await` are already used as keywords in Python3.6.
- `from __future__ import print_function` is not ignored.


Acknowledgements
================

- Guido van Rossum (@gvanrossum) for creating the parser generator pgen2
  (originally used in lib2to3).
- `Salome Schneider <https://www.crepes-schnaegg.ch/cr%C3%AApes-schn%C3%A4gg/kunst-f%C3%BCrs-cr%C3%AApes-mobil/>`_
  for the extremely awesome parso logo.


.. _jedi: https://github.com/davidhalter/jedi


.. :changelog:

Changelog
---------

Unreleased
++++++++++

0.8.4 (2024-04-05)
++++++++++++++++++

- Add basic support for Python 3.13

0.8.3 (2021-11-30)
++++++++++++++++++

- Add basic support for Python 3.11 and 3.12

0.8.2 (2021-03-30)
++++++++++++++++++

- Various small bugfixes

0.8.1 (2020-12-10)
++++++++++++++++++

- Various small bugfixes

0.8.0 (2020-08-05)
++++++++++++++++++

- Dropped Support for Python 2.7, 3.4, 3.5
- It's possible to use ``pathlib.Path`` objects now in the API
- The stubs are gone, we are now using annotations
- ``namedexpr_test`` nodes are now a proper class called ``NamedExpr``
- A lot of smaller refactorings

0.7.1 (2020-07-24)
++++++++++++++++++

- Fixed a couple of smaller bugs (mostly syntax error detection in
  ``Grammar.iter_errors``)

This is going to be the last release that supports Python 2.7, 3.4 and 3.5.

0.7.0 (2020-04-13)
++++++++++++++++++

- Fix a lot of annoying bugs in the diff parser. The fuzzer did not find
  issues anymore even after running it for more than 24 hours (500k tests).
- Small grammar change: suites can now contain newlines even after a newline.
  This should really not matter if you don't use error recovery. It allows for
  nicer error recovery.

0.6.2 (2020-02-27)
++++++++++++++++++

- Bugfixes
- Add Grammar.refactor (might still be subject to change until 0.7.0)

0.6.1 (2020-02-03)
++++++++++++++++++

- Add ``parso.normalizer.Issue.end_pos`` to make it possible to know where an
  issue ends

0.6.0 (2020-01-26)
++++++++++++++++++

- Dropped Python 2.6/Python 3.3 support
- del_stmt names are now considered as a definition
  (for ``name.is_definition()``)
- Bugfixes

0.5.2 (2019-12-15)
++++++++++++++++++

- Add include_setitem to get_definition/is_definition and get_defined_names (#66)
- Fix named expression error listing (#89, #90)
- Fix some f-string tokenizer issues (#93)

0.5.1 (2019-07-13)
++++++++++++++++++

- Fix: Some unicode identifiers were not correctly tokenized
- Fix: Line continuations in f-strings are now working

0.5.0 (2019-06-20)
++++++++++++++++++

- **Breaking Change** comp_for is now called sync_comp_for for all Python
  versions to be compatible with the Python 3.8 Grammar
- Added .pyi stubs for a lot of the parso API
- Small FileIO changes

0.4.0 (2019-04-05)
++++++++++++++++++

- Python 3.8 support
- FileIO support, it's now possible to use abstract file IO, support is alpha

0.3.4 (2019-02-13)
+++++++++++++++++++

- Fix an f-string tokenizer error

0.3.3 (2019-02-06)
+++++++++++++++++++

- Fix async errors in the diff parser
- A fix in iter_errors
- This is a very small bugfix release

0.3.2 (2019-01-24)
+++++++++++++++++++

- 20+ bugfixes in the diff parser and 3 in the tokenizer
- A fuzzer for the diff parser, to give confidence that the diff parser is in a
  good shape.
- Some bugfixes for f-string

0.3.1 (2018-07-09)
+++++++++++++++++++

- Bugfixes in the diff parser and keyword-only arguments

0.3.0 (2018-06-30)
+++++++++++++++++++

- Rewrote the pgen2 parser generator.

0.2.1 (2018-05-21)
+++++++++++++++++++

- A bugfix for the diff parser.
- Grammar files can now be loaded from a specific path.

0.2.0 (2018-04-15)
+++++++++++++++++++

- f-strings are now parsed as a part of the normal Python grammar. This makes
  it way easier to deal with them.

0.1.1 (2017-11-05)
+++++++++++++++++++

- Fixed a few bugs in the caching layer
- Added support for Python 3.7

0.1.0 (2017-09-04)
+++++++++++++++++++

- Pulling the library out of Jedi. Some APIs will definitely change.


"""^^xsd:string ;
    schema1:name "parso"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.8.4"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pickleshare> a software_vocab:Software_package ;
    schema1:description """PickleShare - a small 'shelve' like datastore with concurrency support

Like shelve, a PickleShareDB object acts like a normal dictionary. Unlike shelve,
many processes can access the database simultaneously. Changing a value in 
database is immediately visible to other processes accessing the same database.

Concurrency is possible because the values are stored in separate files. Hence
the "database" is a directory where *all* files are governed by PickleShare.

Example usage::

    from pickleshare import *
    db = PickleShareDB('~/testpickleshare')
    db.clear()
    print("Should be empty:",db.items())
    db['hello'] = 15
    db['aku ankka'] = [1,2,313]
    db['paths/are/ok/key'] = [1,(5,46)]
    print(db.keys())

This module is certainly not ZODB, but can be used for low-load
(non-mission-critical) situations where tiny code size trumps the 
advanced features of a "real" object database.

Installation guide: pip install pickleshare


"""^^xsd:string ;
    schema1:name "pickleshare"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.7.5"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pip-requirements-parser> a software_vocab:Software_package ;
    schema1:description """pip-requirements-parser - a mostly correct pip requirements parsing library
================================================================================

Copyright (c) nexB Inc. and others.
Copyright (c) The pip developers (see AUTHORS.rst file)
SPDX-License-Identifier: MIT
Homepage: https://github.com/nexB/pip-requirements and https://www.aboutcode.org/


``pip-requirements-parser`` is a mostly correct pip requirements parsing
library ... because it uses pip's own code!

pip is the ``package installer`` for Python that is using "requirements" text
files listing the packages to install.

Per https://pip.pypa.io/en/stable/reference/requirements-file-format/ :

    "The requirements file format is closely tied to a number of internal
    details of pip (e.g., pipâ€™s command line options). The basic format is
    relatively stable and portable but the full syntax, as described here,
    is only intended for consumption by pip, and other tools should take
    that into account before using it for their own purposes."

And per https://pip.pypa.io/en/stable/user_guide/#using-pip-from-your-program :

    "[..] pip is a command line program. While it is implemented in Python, and
    so is available from your Python code via import pip, you must not use pipâ€™s
    internal APIs in this way."
    
    "What this means in practice is that everything inside of pip is considered
    an implementation detail. Even the fact that the import name is pip is
    subject to change without notice. While we do try not to break things as
    much as possible, all the internal APIs can change at any time, for any
    reason. It also means that we generally wonâ€™t fix issues that are a result
    of using pip in an unsupported way."


Because of all this, pip requirements are notoriously difficult to parse right
in all their diversity because:

- pip does not have a public API and therefore cannot be reliably used as a
  stable library. Some libraries attempt to do this though. (See Alternative)

- The pip requirements file syntax is closely aligned with pip's command line
  interface and command line options. In some ways a pip requirements file is a
  list of pip command line options and arguments. Therefore, it is hard to parse
  these short of reproducing the pip command line options parsing. At least one
  other library is using a command line option parser to parse options correctly.


This ``pip-requirements-parser`` Python library is yet another pip requirements
files parser, but this time doing it hopefully correctly and doing as well as
pip does it, because this is using pip's own code.


The ``pip-requirements-parser`` library offers these key advantages:

- Other requirements parsers typically do not work in all the cases that ``pip``
  supports: parsing any requirement as seen in the wild will fail parsing some
  valid pip requirements. Since the ``pip-requirements-parser`` library is based
  on pip's own code, it works **exactly** like pip and will parse all the
  requirements files that pip can parse.

- The ``pip-requirements-parser`` library offers a simple and stable code API
  that will not change without notice.

- The ``pip-requirements-parser`` library is designed to work offline without
  making any external network call, while the original pip code needs network
  access.

- The ``pip-requirements-parser`` library is a single file that can easily be
  copied around as needed for easy vendoring. This is useful as requirements
  parsing is often needed to bootstrap in a constrained environment.

- The ``pip-requirements-parser`` library has only one external dependency on
  the common "packaging" package. Otherwise it uses only the standard library.
  The benefits are the same as being a single file: fewer moving parts helps with
  using it in more cases.

- The ``pip-requirements-parser`` library reuses and passes the full subset of
  the pip test suite that deals with requirements. This is a not really
  surprising since this is pip's own code. The suite suite has been carefully
  ported and adjusted to work with the updated code subset.

- The standard pip requirements parser depends on the ``requests`` HTTP library
  and makes network connection to PyPI and other referenced repositories when
  parsing. The ``pip-requirements-parser`` library works entirely offline and the
  requests dependency and calling has been entirely removed.

- The ``pip-requirements-parser`` library has preserved the complete pip git
  history for the subset of the code we kept. The original pip code was merged
  from multiple modules keeping all the git history at the line/blame level using
  some git fu and git filter repo. The benefit is that we will be able to more
  easily track and merge future pip updates.

- The ``pip-requirements-parser`` library has an extensive test suite  made of:

  - pip's own tests
  - new unit tests
  - new requirements test files (over 40 new test files)
  - the tests suite of some of the main other requirement parsers including:

     - http://github.com/di/pip-api
     - https://github.com/pyupio/dparse
     - https://github.com/landscapeio/requirements-detector
     - https://github.com/madpah/requirements-parser

As a result, it has likely the most comprehensive requiremente parsing test
suite around.


Usage
~~~~~~~~~~

The entry point is the ``RequirementsFile`` object::

    >>> from pip_requirements_parser import RequirementsFile
    >>> rf = RequirementsFile.from_file("requirements.txt")

From there, you can dump to a dict::
    >>> rf.to_dict()

Or access the requirements (either InstallRequirement or EditableRequirement
objects)::

    >>> for req in rf.requirements:
    ...    print(req.to_dict())
    ...    print(req.dumps())

And the various other parsed elements such as options, commenst and invalid lines
that have a parsing error::

    >>> rf.options
    >>> rf.comment_lines
    >>> rf.invalid_lines

Each of these and the ``requirements`` hahve a "requirement_line" attribute
with the original text.

Finally you can get a requirements file back as a string::

    >>> rf.dumps()


Alternative
------------------

There are several other parsers that either:

- Implement their own parsing and can therefore miss some subtle differences
- Or wrap and import pip as a library, working around the lack of pip API

None of these use the approach of reusing and forking the subset of pip that is
needed to parse requirements.  The ones that wrap pip require network access
like pip does. They potentially need updating each time there is a new pip
release. The ones that reimplement pip parsing may not support all pip
specifics.


Implement a new pip parser
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- pip-api https://github.com/di/pip-api does not support hashes and certain pip options.
  It does however use argparse for parsing options and is therefore correctly
  handling most options. The parser is a single script that only depends on
  packaging (that is vendored). It is not designed to be used as a single script
  though and ``pip`` is a dependency.

- requirements-parser https://github.com/madpah/requirements-parse does not
  support hashes and certain pip options

- dparse https://github.com/pyupio/dparse

- https://github.com/GoogleCloudPlatform/django-cloud-deploy/blob/d316b1e45357761e2b124143e6e12ce34ef6f975/django_cloud_deploy/skeleton/requirements_parser.py


Reuse and wrap pip's own parser
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- requirementslib https://github.com/sarugaku/requirementslib uses pip-shim
  https://github.com/sarugaku/pip-shims which is a set of "shims" around each
  pip versions in an attempt to offer an API to pip. Comes with 20+ dependencies,

- micropipenv https://github.com/thoth-station/micropipenv/blob/d0c37c1bf0aadf5149aebe2df0bf1cb12ded4c40/micropipenv.py#L53

- pip-tools https://github.com/jazzband/pip-tools/blob/9e1be05375104c56e07cdb0904e1b50b86f8b550/piptools/_compat/pip_compat.py
"""^^xsd:string ;
    schema1:name "pip-requirements-parser"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "32.0.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pip-tools> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/build>,
        <https://data.siemens-energy.com/cybersec/package/click>,
        <https://data.siemens-energy.com/cybersec/package/flit-core>,
        <https://data.siemens-energy.com/cybersec/package/pip>,
        <https://data.siemens-energy.com/cybersec/package/poetry-core>,
        <https://data.siemens-energy.com/cybersec/package/pytest> ;
    schema1:description """[![jazzband-image]][jazzband]
[![pypi][pypi-image]][pypi]
[![pyversions][pyversions-image]][pyversions]
[![pre-commit][pre-commit-image]][pre-commit]
[![buildstatus-gha][buildstatus-gha-image]][buildstatus-gha]
[![codecov][codecov-image]][codecov]
[![Matrix Room Badge]][Matrix Room]
[![Matrix Space Badge]][Matrix Space]
[![discord-chat-image]][discord-chat]

# pip-tools = pip-compile + pip-sync

A set of command line tools to help you keep your `pip`-based packages fresh,
even when you've pinned them. You do pin them, right? (In building your Python application and its dependencies for production, you want to make sure that your builds are predictable and deterministic.)

[![pip-tools overview for phase II][pip-tools-overview]][pip-tools-overview]

## Installation

Similar to `pip`, `pip-tools` must be installed in each of your project's
[virtual environments](https://packaging.python.org/tutorials/installing-packages/#creating-virtual-environments):

```console
$ source /path/to/venv/bin/activate
(venv) $ python -m pip install pip-tools
```

**Note**: all of the remaining example commands assume you've activated your
project's virtual environment.

## Example usage for `pip-compile`

The `pip-compile` command lets you compile a `requirements.txt` file from
your dependencies, specified in either `pyproject.toml`, `setup.cfg`,
`setup.py`, or `requirements.in`.

Run it with `pip-compile` or `python -m piptools compile` (or
`pipx run --spec pip-tools pip-compile` if `pipx` was installed with the
appropriate Python version). If you use multiple Python versions, you can also
run `py -X.Y -m piptools compile` on Windows and `pythonX.Y -m piptools compile`
on other systems.

`pip-compile` should be run from the same virtual environment as your
project so conditional dependencies that require a specific Python version,
or other environment markers, resolve relative to your project's
environment.

**Note**: If `pip-compile` finds an existing `requirements.txt` file that
fulfils the dependencies then no changes will be made, even if updates are
available. To compile from scratch, first delete the existing
`requirements.txt` file, or see
[Updating requirements](#updating-requirements)
for alternative approaches.

### Requirements from `pyproject.toml`

The `pyproject.toml` file is the
[latest standard](https://peps.python.org/pep-0621/) for configuring
packages and applications, and is recommended for new projects. `pip-compile`
supports both installing your `project.dependencies` as well as your
`project.optional-dependencies`. Thanks to the fact that this is an
official standard, you can use `pip-compile` to pin the dependencies
in projects that use modern standards-adhering packaging tools like
[Setuptools](https://setuptools.pypa.io), [Hatch](https://hatch.pypa.io/)
or [flit](https://flit.pypa.io/).

Suppose you have a 'foobar' Python application that is packaged using `Setuptools`,
and you want to pin it for production. You can declare the project metadata as:

```toml
[build-system]
requires = ["setuptools", "setuptools-scm"]
build-backend = "setuptools.build_meta"

[project]
requires-python = ">=3.9"
name = "foobar"
dynamic = ["dependencies", "optional-dependencies"]

[tool.setuptools.dynamic]
dependencies = { file = ["requirements.in"] }
optional-dependencies.test = { file = ["requirements-test.txt"] }

```

If you have a Django application that is packaged using `Hatch`, and you
want to pin it for production. You also want to pin your development tools
in a separate pin file. You declare `django` as a dependency and create an
optional dependency `dev` that includes `pytest`:

```toml
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "my-cool-django-app"
version = "42"
dependencies = ["django"]

[project.optional-dependencies]
dev = ["pytest"]
```

You can produce your pin files as easily as:

```console
$ pip-compile -o requirements.txt pyproject.toml
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile --output-file=requirements.txt pyproject.toml
#
asgiref==3.6.0
    # via django
django==4.1.7
    # via my-cool-django-app (pyproject.toml)
sqlparse==0.4.3
    # via django

$ pip-compile --extra dev -o dev-requirements.txt pyproject.toml
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile --extra=dev --output-file=dev-requirements.txt pyproject.toml
#
asgiref==3.6.0
    # via django
attrs==22.2.0
    # via pytest
django==4.1.7
    # via my-cool-django-app (pyproject.toml)
exceptiongroup==1.1.1
    # via pytest
iniconfig==2.0.0
    # via pytest
packaging==23.0
    # via pytest
pluggy==1.0.0
    # via pytest
pytest==7.2.2
    # via my-cool-django-app (pyproject.toml)
sqlparse==0.4.3
    # via django
tomli==2.0.1
    # via pytest
```

This is great for both pinning your applications, but also to keep the CI
of your open-source Python package stable.

### Requirements from `setup.py` and `setup.cfg`

`pip-compile` has also full support for `setup.py`- and
`setup.cfg`-based projects that use `setuptools`.

Just define your dependencies and extras as usual and run
`pip-compile` as above.

### Requirements from `requirements.in`

You can also use plain text files for your requirements (e.g. if you don't
want your application to be a package). To use a `requirements.in` file to
declare the Django dependency:

```
# requirements.in
django
```

Now, run `pip-compile requirements.in`:

```console
$ pip-compile requirements.in
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile requirements.in
#
asgiref==3.6.0
    # via django
django==4.1.7
    # via -r requirements.in
sqlparse==0.4.3
    # via django
```

And it will produce your `requirements.txt`, with all the Django dependencies
(and all underlying dependencies) pinned.

(updating-requirements)=

### Updating requirements

`pip-compile` generates a `requirements.txt` file using the latest versions
that fulfil the dependencies you specify in the supported files.

If `pip-compile` finds an existing `requirements.txt` file that fulfils the
dependencies then no changes will be made, even if updates are available.

To force `pip-compile` to update all packages in an existing
`requirements.txt`, run `pip-compile --upgrade`.

To update a specific package to the latest or a specific version use the
`--upgrade-package` or `-P` flag:

```console
# only update the django package
$ pip-compile --upgrade-package django

# update both the django and requests packages
$ pip-compile --upgrade-package django --upgrade-package requests

# update the django package to the latest, and requests to v2.0.0
$ pip-compile --upgrade-package django --upgrade-package requests==2.0.0
```

You can combine `--upgrade` and `--upgrade-package` in one command, to
provide constraints on the allowed upgrades. For example to upgrade all
packages whilst constraining requests to the latest version less than 3.0:

```console
$ pip-compile --upgrade --upgrade-package 'requests<3.0'
```

### Using hashes

If you would like to use _Hash-Checking Mode_ available in `pip` since
version 8.0, `pip-compile` offers `--generate-hashes` flag:

```console
$ pip-compile --generate-hashes requirements.in
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile --generate-hashes requirements.in
#
asgiref==3.6.0 \\
    --hash=sha256:71e68008da809b957b7ee4b43dbccff33d1b23519fb8344e33f049897077afac \\
    --hash=sha256:9567dfe7bd8d3c8c892227827c41cce860b368104c3431da67a0c5a65a949506
    # via django
django==4.1.7 \\
    --hash=sha256:44f714b81c5f190d9d2ddad01a532fe502fa01c4cb8faf1d081f4264ed15dcd8 \\
    --hash=sha256:f2f431e75adc40039ace496ad3b9f17227022e8b11566f4b363da44c7e44761e
    # via -r requirements.in
sqlparse==0.4.3 \\
    --hash=sha256:0323c0ec29cd52bceabc1b4d9d579e311f3e4961b98d174201d5622a23b85e34 \\
    --hash=sha256:69ca804846bb114d2ec380e4360a8a340db83f0ccf3afceeb1404df028f57268
    # via django
```

### Output File

To output the pinned requirements in a filename other than
`requirements.txt`, use `--output-file`. This might be useful for compiling
multiple files, for example with different constraints on django to test a
library with both versions using [tox](https://tox.readthedocs.io/en/latest/):

```console
$ pip-compile --upgrade-package 'django<1.0' --output-file requirements-django0x.txt
$ pip-compile --upgrade-package 'django<2.0' --output-file requirements-django1x.txt
```

Or to output to standard output, use `--output-file=-`:

```console
$ pip-compile --output-file=- > requirements.txt
$ pip-compile - --output-file=- < requirements.in > requirements.txt
```

### Forwarding options to `pip`

Any valid `pip` flags or arguments may be passed on with `pip-compile`'s
`--pip-args` option, e.g.

```console
$ pip-compile requirements.in --pip-args "--retries 10 --timeout 30"
```

### Configuration

You can define project-level defaults for `pip-compile` and `pip-sync` by
writing them to a configuration file in the same directory as your requirements
input files (or the current working directory if piping input from stdin).
By default, both `pip-compile` and `pip-sync` will look first
for a `.pip-tools.toml` file and then in your `pyproject.toml`. You can
also specify an alternate TOML configuration file with the `--config` option.

It is possible to specify configuration values both globally and command-specific.
For example, to by default generate `pip` hashes in the resulting
requirements file output, you can specify in a configuration file:

```toml
[tool.pip-tools]
generate-hashes = true
```

Options to `pip-compile` and `pip-sync` that may be used more than once
must be defined as lists in a configuration file, even if they only have one
value.

`pip-tools` supports default values for [all valid command-line flags](/cli/index.md)
of its subcommands. Configuration keys may contain underscores instead of dashes,
so the above could also be specified in this format:

```toml
[tool.pip-tools]
generate_hashes = true
```

Configuration defaults specific to `pip-compile` and `pip-sync` can be put beneath
separate sections. For example, to by default perform a dry-run with `pip-compile`:

```toml
[tool.pip-tools.compile] # "sync" for pip-sync
dry-run = true
```

This does not affect the `pip-sync` command, which also has a `--dry-run` option.
Note that local settings take preference over the global ones of the same name,
whenever both are declared, thus this would also make `pip-compile` generate hashes,
but discard the global dry-run setting:

```toml
[tool.pip-tools]
generate-hashes = true
dry-run = true

[tool.pip-tools.compile]
dry-run = false
```

You might be wrapping the `pip-compile` command in another script. To avoid
confusing consumers of your custom script you can override the update command
generated at the top of requirements files by setting the
`CUSTOM_COMPILE_COMMAND` environment variable.

```console
$ CUSTOM_COMPILE_COMMAND="./pipcompilewrapper" pip-compile requirements.in
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    ./pipcompilewrapper
#
asgiref==3.6.0
    # via django
django==4.1.7
    # via -r requirements.in
sqlparse==0.4.3
    # via django
```

### Workflow for layered requirements

If you have different environments that you need to install different but
compatible packages for, then you can create layered requirements files and use
one layer to constrain the other.

For example, if you have a Django project where you want the newest `2.1`
release in production and when developing you want to use the Django debug
toolbar, then you can create two `*.in` files, one for each layer:

```
# requirements.in
django<2.2
```

At the top of the development requirements `dev-requirements.in` you use `-c
requirements.txt` to constrain the dev requirements to packages already
selected for production in `requirements.txt`.

```
# dev-requirements.in
-c requirements.txt
django-debug-toolbar<2.2
```

First, compile `requirements.txt` as usual:

```
$ pip-compile
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile
#
django==2.1.15
    # via -r requirements.in
pytz==2023.3
    # via django
```

Now compile the dev requirements and the `requirements.txt` file is used as
a constraint:

```console
$ pip-compile dev-requirements.in
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile dev-requirements.in
#
django==2.1.15
    # via
    #   -c requirements.txt
    #   django-debug-toolbar
django-debug-toolbar==2.1
    # via -r dev-requirements.in
pytz==2023.3
    # via
    #   -c requirements.txt
    #   django
sqlparse==0.4.3
    # via django-debug-toolbar
```

As you can see above, even though a `2.2` release of Django is available, the
dev requirements only include a `2.1` version of Django because they were
constrained. Now both compiled requirements files can be installed safely in
the dev environment.

To install requirements in production stage use:

```console
$ pip-sync
```

You can install requirements in development stage by:

```console
$ pip-sync requirements.txt dev-requirements.txt
```

### Version control integration

You might use `pip-compile` as a hook for the [pre-commit](https://github.com/pre-commit/pre-commit).
See [pre-commit docs](https://pre-commit.com/) for instructions.
Sample `.pre-commit-config.yaml`:

```yaml
repos:
  - repo: https://github.com/jazzband/pip-tools
    rev: 7.4.1
    hooks:
      - id: pip-compile
```

You might want to customize `pip-compile` args by configuring `args` and/or `files`, for example:

```yaml
repos:
  - repo: https://github.com/jazzband/pip-tools
    rev: 7.4.1
    hooks:
      - id: pip-compile
        files: ^requirements/production\\.(in|txt)$
        args: [--index-url=https://example.com, requirements/production.in]
```

If you have multiple requirement files make sure you create a hook for each file.

```yaml
repos:
  - repo: https://github.com/jazzband/pip-tools
    rev: 7.4.1
    hooks:
      - id: pip-compile
        name: pip-compile setup.py
        files: ^(setup\\.py|requirements\\.txt)$
      - id: pip-compile
        name: pip-compile requirements-dev.in
        args: [requirements-dev.in]
        files: ^requirements-dev\\.(in|txt)$
      - id: pip-compile
        name: pip-compile requirements-lint.in
        args: [requirements-lint.in]
        files: ^requirements-lint\\.(in|txt)$
      - id: pip-compile
        name: pip-compile requirements.in
        args: [requirements.in]
        files: ^requirements\\.(in|txt)$
```

### Example usage for `pip-sync`

Now that you have a `requirements.txt`, you can use `pip-sync` to update
your virtual environment to reflect exactly what's in there. This will
install/upgrade/uninstall everything necessary to match the
`requirements.txt` contents.

Run it with `pip-sync` or `python -m piptools sync`. If you use multiple
Python versions, you can also run `py -X.Y -m piptools sync` on Windows and
`pythonX.Y -m piptools sync` on other systems.

`pip-sync` must be installed into and run from the same virtual
environment as your project to identify which packages to install
or upgrade.

**Be careful**: `pip-sync` is meant to be used only with a
`requirements.txt` generated by `pip-compile`.

```console
$ pip-sync
Uninstalling flake8-2.4.1:
    Successfully uninstalled flake8-2.4.1
Collecting click==4.1
    Downloading click-4.1-py2.py3-none-any.whl (62kB)
    100% |................................| 65kB 1.8MB/s
    Found existing installation: click 4.0
    Uninstalling click-4.0:
        Successfully uninstalled click-4.0
Successfully installed click-4.1
```

To sync multiple `*.txt` dependency lists, just pass them in via command
line arguments, e.g.

```console
$ pip-sync dev-requirements.txt requirements.txt
```

Passing in empty arguments would cause it to default to `requirements.txt`.

Any valid `pip install` flags or arguments may be passed with `pip-sync`'s
`--pip-args` option, e.g.

```console
$ pip-sync requirements.txt --pip-args "--no-cache-dir --no-deps"
```

**Note**: `pip-sync` will not upgrade or uninstall packaging tools like
`setuptools`, `pip`, or `pip-tools` itself. Use `python -m pip install --upgrade`
to upgrade those packages.

### Should I commit `requirements.in` and `requirements.txt` to source control?

Generally, yes. If you want a reproducible environment installation available from your source control,
then yes, you should commit both `requirements.in` and `requirements.txt` to source control.

Note that if you are deploying on multiple Python environments (read the section below),
then you must commit a separate output file for each Python environment.
We suggest to use the `{env}-requirements.txt` format
(ex: `win32-py3.7-requirements.txt`, `macos-py3.10-requirements.txt`, etc.).

### Cross-environment usage of `requirements.in`/`requirements.txt` and `pip-compile`

The dependencies of a package can change depending on the Python environment in which it
is installed. Here, we define a Python environment as the combination of Operating
System, Python version (3.7, 3.8, etc.), and Python implementation (CPython, PyPy,
etc.). For an exact definition, refer to the possible combinations of [PEP 508
environment markers][environment-markers].

As the resulting `requirements.txt` can differ for each environment, users must
execute `pip-compile` **on each Python environment separately** to generate a
`requirements.txt` valid for each said environment. The same `requirements.in` can
be used as the source file for all environments, using
[PEP 508 environment markers][environment-markers] as
needed, the same way it would be done for regular `pip` cross-environment usage.

If the generated `requirements.txt` remains exactly the same for all Python
environments, then it can be used across Python environments safely. **But** users
should be careful as any package update can introduce environment-dependent
dependencies, making any newly generated `requirements.txt` environment-dependent too.
As a general rule, it's advised that users should still always execute `pip-compile`
on each targeted Python environment to avoid issues.

### Maximizing reproducibility

`pip-tools` is a great tool to improve the reproducibility of builds.
But there are a few things to keep in mind.

- `pip-compile` will produce different results in different environments as described in the previous section.
- `pip` must be used with the `PIP_CONSTRAINT` environment variable to lock dependencies in build environments as documented in [#8439](https://github.com/pypa/pip/issues/8439).
- Dependencies come from many sources.

Continuing the `pyproject.toml` example from earlier, creating a single lock file could be done like:

```console
$ pip-compile --all-build-deps --all-extras --output-file=constraints.txt --strip-extras pyproject.toml
#
# This file is autogenerated by pip-compile with Python 3.9
# by the following command:
#
#    pip-compile --all-build-deps --all-extras --output-file=constraints.txt --strip-extras pyproject.toml
#
asgiref==3.5.2
    # via django
attrs==22.1.0
    # via pytest
backports-zoneinfo==0.2.1
    # via django
django==4.1
    # via my-cool-django-app (pyproject.toml)
editables==0.3
    # via hatchling
hatchling==1.11.1
    # via my-cool-django-app (pyproject.toml::build-system.requires)
iniconfig==1.1.1
    # via pytest
packaging==21.3
    # via
    #   hatchling
    #   pytest
pathspec==0.10.2
    # via hatchling
pluggy==1.0.0
    # via
    #   hatchling
    #   pytest
py==1.11.0
    # via pytest
pyparsing==3.0.9
    # via packaging
pytest==7.1.2
    # via my-cool-django-app (pyproject.toml)
sqlparse==0.4.2
    # via django
tomli==2.0.1
    # via
    #   hatchling
    #   pytest
```

Some build backends may also request build dependencies dynamically using the `get_requires_for_build_` hooks described in [PEP 517] and [PEP 660].
This will be indicated in the output with one of the following suffixes:

- `(pyproject.toml::build-system.backend::editable)`
- `(pyproject.toml::build-system.backend::sdist)`
- `(pyproject.toml::build-system.backend::wheel)`

### Other useful tools

- [pip-compile-multi](https://pip-compile-multi.readthedocs.io/en/latest/) - pip-compile command wrapper for multiple cross-referencing requirements files.
- [pipdeptree](https://github.com/tox-dev/pipdeptree) to print the dependency tree of the installed packages.
- `requirements.in`/`requirements.txt` syntax highlighting:

  - [requirements.txt.vim](https://github.com/raimon49/requirements.txt.vim) for Vim.
  - [Python extension for VS Code](https://marketplace.visualstudio.com/items?itemName=ms-python.python) for VS Code.
  - [pip-requirements.el](https://github.com/Wilfred/pip-requirements.el) for Emacs.

### Deprecations

This section lists `pip-tools` features that are currently deprecated.

- In the next major release, the `--allow-unsafe` behavior will be enabled by
  default (https://github.com/jazzband/pip-tools/issues/989).
  Use `--no-allow-unsafe` to keep the old behavior. It is recommended
  to pass `--allow-unsafe` now to adapt to the upcoming change.
- The legacy resolver is deprecated and will be removed in future versions.
  The new default is `--resolver=backtracking`.
- In the next major release, the `--strip-extras` behavior will be enabled by
  default (https://github.com/jazzband/pip-tools/issues/1613).
  Use `--no-strip-extras` to keep the old behavior.

### A Note on Resolvers

You can choose from either default backtracking resolver or the deprecated legacy resolver.

The legacy resolver will occasionally fail to resolve dependencies. The
backtracking resolver is more robust, but can take longer to run in general.

You can continue using the legacy resolver with `--resolver=legacy` although
note that it is deprecated and will be removed in a future release.

[jazzband]: https://jazzband.co/
[jazzband-image]: https://jazzband.co/static/img/badge.svg
[pypi]: https://pypi.org/project/pip-tools/
[pypi-image]: https://img.shields.io/pypi/v/pip-tools.svg
[pyversions]: https://pypi.org/project/pip-tools/
[pyversions-image]: https://img.shields.io/pypi/pyversions/pip-tools.svg
[pre-commit]: https://results.pre-commit.ci/latest/github/jazzband/pip-tools/main
[pre-commit-image]: https://results.pre-commit.ci/badge/github/jazzband/pip-tools/main.svg
[buildstatus-gha]: https://github.com/jazzband/pip-tools/actions?query=workflow%3ACI
[buildstatus-gha-image]: https://github.com/jazzband/pip-tools/workflows/CI/badge.svg
[codecov]: https://codecov.io/gh/jazzband/pip-tools
[codecov-image]: https://codecov.io/gh/jazzband/pip-tools/branch/main/graph/badge.svg
[Matrix Room Badge]: https://img.shields.io/matrix/pip-tools:matrix.org?label=Discuss%20on%20Matrix%20at%20%23pip-tools%3Amatrix.org&logo=matrix&server_fqdn=matrix.org&style=flat
[Matrix Room]: https://matrix.to/#/%23pip-tools:matrix.org
[Matrix Space Badge]: https://img.shields.io/matrix/jazzband:matrix.org?label=Discuss%20on%20Matrix%20at%20%23jazzband%3Amatrix.org&logo=matrix&server_fqdn=matrix.org&style=flat
[Matrix Space]: https://matrix.to/#/%23jazzband:matrix.org
[pip-tools-overview]: https://github.com/jazzband/pip-tools/raw/main/img/pip-tools-overview.svg
[environment-markers]: https://peps.python.org/pep-0508/#environment-markers
[PEP 517]: https://peps.python.org/pep-0517/
[PEP 660]: https://peps.python.org/pep-0660/
[discord-chat]: https://discord.gg/pypa
[discord-chat-image]: https://img.shields.io/discord/803025117553754132?label=Discord%20chat%20%23pip-tools&style=flat-square
"""^^xsd:string ;
    schema1:name "pip-tools"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "7.4.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pipreqs> a software_vocab:Software_package ;
    schema1:description """=============================================================================
``pipreqs`` - Generate requirements.txt file for any project based on imports
=============================================================================

.. image:: https://img.shields.io/travis/bndr/pipreqs.svg
        :target: https://travis-ci.org/bndr/pipreqs


.. image:: https://img.shields.io/pypi/v/pipreqs.svg
        :target: https://pypi.python.org/pypi/pipreqs


.. image:: https://codecov.io/gh/bndr/pipreqs/branch/master/graph/badge.svg?token=0rfPfUZEAX
        :target: https://codecov.io/gh/bndr/pipreqs

.. image:: https://img.shields.io/pypi/l/pipreqs.svg
        :target: https://pypi.python.org/pypi/pipreqs



Installation
------------

.. code-block:: sh

    pip install pipreqs

Obs.: if you don't want support for jupyter notebooks, you can install pipreqs without the dependencies that give support to it. 
To do so, run:

.. code-block:: sh

    pip install --no-deps pipreqs
    pip install yarg==0.1.9 docopt==0.6.2

Usage
-----

::

    Usage:
        pipreqs [options] [<path>]

    Arguments:
        <path>                The path to the directory containing the application files for which a requirements file
                              should be generated (defaults to the current working directory)

    Options:
        --use-local           Use ONLY local package info instead of querying PyPI
        --pypi-server <url>   Use custom PyPi server
        --proxy <url>         Use Proxy, parameter will be passed to requests library. You can also just set the
                              environments parameter in your terminal:
                              $ export HTTP_PROXY="http://10.10.1.10:3128"
                              $ export HTTPS_PROXY="https://10.10.1.10:1080"
        --debug               Print debug information
        --ignore <dirs>...    Ignore extra directories, each separated by a comma
        --no-follow-links     Do not follow symbolic links in the project
        --encoding <charset>  Use encoding parameter for file open
        --savepath <file>     Save the list of requirements in the given file
        --print               Output the list of requirements in the standard output
        --force               Overwrite existing requirements.txt
        --diff <file>         Compare modules in requirements.txt to project imports
        --clean <file>        Clean up requirements.txt by removing modules that are not imported in project
        --mode <scheme>       Enables dynamic versioning with <compat>, <gt> or <non-pin> schemes
                              <compat> | e.g. Flask~=1.1.2
                              <gt>     | e.g. Flask>=1.1.2
                              <no-pin> | e.g. Flask
        --scan-notebooks      Look for imports in jupyter notebook files.

Example
-------

::

    $ pipreqs /home/project/location
    Successfully saved requirements file in /home/project/location/requirements.txt

Contents of requirements.txt

::

    wheel==0.23.0
    Yarg==0.1.9
    docopt==0.6.2

Why not pip freeze?
-------------------

- ``pip freeze`` only saves the packages that are installed with ``pip install`` in your environment.
- ``pip freeze`` saves all packages in the environment including those that you don't use in your current project (if you don't have ``virtualenv``).
- and sometimes you just need to create ``requirements.txt`` for a new project without installing modules.

.. :changelog:

History
-------

0.4.11 (2020-03-29)
--------------------

* Implement '--mode' (Jake Teo, Jerome Chan)

0.4.8 (2017-06-30)
--------------------

* Implement '--clean' and '--diff' (kxrd)
* Exclude concurrent{,.futures} from stdlib if py2 (kxrd)

0.4.7 (2017-04-20)
--------------------

* BUG: remove package/version duplicates
* Style: pep8

0.4.5 (2016-12-13)
---------------------

* Fixed the --pypi-server option

0.4.4 (2016-07-14)
---------------------

* Remove Spaces in output
* Add package to output even without version

0.4.2 (2016-02-10)
---------------------

* Fix duplicated lines in requirements.txt (Dmitry Pribysh)

0.4.1 (2016-02-05)
---------------------

* Added ignore option (Nick Rhinehart)

0.4.0 (2016-01-28)
---------------------

* Walk Abstract Syntax Tree to find imports (Kay Sackey)

0.3.9 (2016-01-20)
---------------------

* Fix regex for docstring comments (#35)

0.3.8 (2016-01-12)
---------------------

* Add more package mapping
* fix(pipreqs/mapping): remove pylab reference to matplotlib
* Remove comments \"\"\" before going through imports
* Update proxy documentation

0.3.1 (2015-10-20)
---------------------

* fixed lint warnings (EJ Lee)
* add --encoding parameter for open() (EJ Lee)
* support windows directory separator (EJ Lee)

0.3.0 (2015-09-29)
---------------------

* Add --proxy option
* Add --pypi-server option

0.2.9 (2015-09-24)
---------------------

* Ignore irreverent directory when generating requirement.txt (Lee Wei)
* Modify logging level of "Requirement.txt already exists" to warning (Lee Wei)

0.2.8 (2015-05-11)
---------------------

* Add --force option as a protection for overwrites

0.2.6 (2015-05-11)
---------------------

* Fix exception when 'import' is used inside package name #17
* Add more tests

0.2.5 (2015-05-11)
---------------------

* Fix exception when 'import' is used in comments #17
* Fix duplicate entries in requirements.txt

0.2.4 (2015-05-10)
---------------------

* Refactoring
* fix "import as"

0.2.3 (2015-05-09)
---------------------

* Fix multiple alias imports on the same line (Tiago Costa)
* More package mappings

0.2.2 (2015-05-08)
---------------------

* Add ImportName -> PackageName mapping
* More tests

0.2.1 (2015-05-08)
---------------------

* Fix for TypeError for implicit conversion

0.2.0 (2015-05-06)
---------------------

* Add --use-local option
* Exclude relative imports. (Dongwon Shin)
* Use "latest_release_id" instead of "release_ids[-1]" (Dongwon Shin)

0.1.9 (2015-05-01)
---------------------

* Output tuning (Harri Berglund)
* Use str.partition() to simplify the logic (cclaus)

0.1.8 (2015-04-26)
---------------------

* Fixed problems with local imports (Dongwon Shin)
* Fixed problems with imports with 'as' (Dongwon Shin)
* Fix indentation, pep8 Styling. (Michael Borisov)
* Optimize imports and adding missing import for sys module. (Michael Borisov)

0.1.7 (2015-04-24)
---------------------

* Add more assertions in tests
* Add more verbose output
* Add recursive delete to Makefile clean
* Update Readme

0.1.6 (2015-04-22)
---------------------

* py3 print function

0.1.5 (2015-04-22)
---------------------

* Add Readme, Add Examples
* Add Stdlib into package

0.1.1 (2015-04-22)
---------------------

* Fix regex matching for imports
* Release on Pypi

0.1.0 (2015-04-22)
---------------------

* First release on Github.

"""^^xsd:string ;
    schema1:name "pipreqs"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.5.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/prompt_toolkit> a software_vocab:Software_package ;
    schema1:description """Python Prompt Toolkit
=====================

|AppVeyor|  |PyPI|  |RTD|  |License|  |Codecov|

.. image :: https://github.com/prompt-toolkit/python-prompt-toolkit/raw/master/docs/images/logo_400px.png

``prompt_toolkit`` *is a library for building powerful interactive command line applications in Python.*

Read the `documentation on readthedocs
<http://python-prompt-toolkit.readthedocs.io/en/stable/>`_.


Gallery
*******

`ptpython <http://github.com/prompt-toolkit/ptpython/>`_ is an interactive
Python Shell, build on top of ``prompt_toolkit``.

.. image :: https://github.com/prompt-toolkit/python-prompt-toolkit/raw/master/docs/images/ptpython.png

`More examples <https://python-prompt-toolkit.readthedocs.io/en/stable/pages/gallery.html>`_


prompt_toolkit features
***********************

``prompt_toolkit`` could be a replacement for `GNU readline
<https://tiswww.case.edu/php/chet/readline/rltop.html>`_, but it can be much
more than that.

Some features:

- **Pure Python**.
- Syntax highlighting of the input while typing. (For instance, with a Pygments lexer.)
- Multi-line input editing.
- Advanced code completion.
- Both Emacs and Vi key bindings. (Similar to readline.)
- Even some advanced Vi functionality, like named registers and digraphs.
- Reverse and forward incremental search.
- Works well with Unicode double width characters. (Chinese input.)
- Selecting text for copy/paste. (Both Emacs and Vi style.)
- Support for `bracketed paste <https://cirw.in/blog/bracketed-paste>`_.
- Mouse support for cursor positioning and scrolling.
- Auto suggestions. (Like `fish shell <http://fishshell.com/>`_.)
- Multiple input buffers.
- No global state.
- Lightweight, the only dependencies are Pygments and wcwidth.
- Runs on Linux, OS X, FreeBSD, OpenBSD and Windows systems.
- And much more...

Feel free to create tickets for bugs and feature requests, and create pull
requests if you have nice patches that you would like to share with others.


Installation
************

::

    pip install prompt_toolkit

For Conda, do:

::

    conda install -c https://conda.anaconda.org/conda-forge prompt_toolkit


About Windows support
*********************

``prompt_toolkit`` is cross platform, and everything that you build on top
should run fine on both Unix and Windows systems. Windows support is best on
recent Windows 10 builds, for which the command line window supports vt100
escape sequences. (If not supported, we fall back to using Win32 APIs for color
and cursor movements).

It's worth noting that the implementation is a "best effort of what is
possible". Both Unix and Windows terminals have their limitations. But in
general, the Unix experience will still be a little better.


Getting started
***************

The most simple example of the library would look like this:

.. code:: python

    from prompt_toolkit import prompt

    if __name__ == '__main__':
        answer = prompt('Give me some input: ')
        print('You said: %s' % answer)

For more complex examples, have a look in the ``examples`` directory. All
examples are chosen to demonstrate only one thing. Also, don't be afraid to
look at the source code. The implementation of the ``prompt`` function could be
a good start.

Philosophy
**********

The source code of ``prompt_toolkit`` should be **readable**, **concise** and
**efficient**. We prefer short functions focusing each on one task and for which
the input and output types are clearly specified. We mostly prefer composition
over inheritance, because inheritance can result in too much functionality in
the same object. We prefer immutable objects where possible (objects don't
change after initialization). Reusability is important. We absolutely refrain
from having a changing global state, it should be possible to have multiple
independent instances of the same code in the same process. The architecture
should be layered: the lower levels operate on primitive operations and data
structures giving -- when correctly combined -- all the possible flexibility;
while at the higher level, there should be a simpler API, ready-to-use and
sufficient for most use cases. Thinking about algorithms and efficiency is
important, but avoid premature optimization.


`Projects using prompt_toolkit <PROJECTS.rst>`_
***********************************************

Special thanks to
*****************

- `Pygments <http://pygments.org/>`_: Syntax highlighter.
- `wcwidth <https://github.com/jquast/wcwidth>`_: Determine columns needed for a wide characters.

.. |PyPI| image:: https://img.shields.io/pypi/v/prompt_toolkit.svg
    :target: https://pypi.python.org/pypi/prompt-toolkit/
    :alt: Latest Version

.. |AppVeyor| image:: https://ci.appveyor.com/api/projects/status/32r7s2skrgm9ubva?svg=true
    :target: https://ci.appveyor.com/project/prompt-toolkit/python-prompt-toolkit/

.. |RTD| image:: https://readthedocs.org/projects/python-prompt-toolkit/badge/
    :target: https://python-prompt-toolkit.readthedocs.io/en/master/

.. |License| image:: https://img.shields.io/github/license/prompt-toolkit/python-prompt-toolkit.svg
    :target: https://github.com/prompt-toolkit/python-prompt-toolkit/blob/master/LICENSE

.. |Codecov| image:: https://codecov.io/gh/prompt-toolkit/python-prompt-toolkit/branch/master/graphs/badge.svg?style=flat
    :target: https://codecov.io/gh/prompt-toolkit/python-prompt-toolkit/

"""^^xsd:string ;
    schema1:name "prompt_toolkit"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.0.47"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pure_eval> a software_vocab:Software_package ;
    schema1:description """# `pure_eval`

[![Build Status](https://travis-ci.org/alexmojaki/pure_eval.svg?branch=master)](https://travis-ci.org/alexmojaki/pure_eval) [![Coverage Status](https://coveralls.io/repos/github/alexmojaki/pure_eval/badge.svg?branch=master)](https://coveralls.io/github/alexmojaki/pure_eval?branch=master) [![Supports Python versions 3.7+](https://img.shields.io/pypi/pyversions/pure_eval.svg)](https://pypi.python.org/pypi/pure_eval)

This is a Python package that lets you safely evaluate certain AST nodes without triggering arbitrary code that may have unwanted side effects.

It can be installed from PyPI:

    pip install pure_eval

To demonstrate usage, suppose we have an object defined as follows:

```python
class Rectangle:
    def __init__(self, width, height):
        self.width = width
        self.height = height

    @property
    def area(self):
        print("Calculating area...")
        return self.width * self.height


rect = Rectangle(3, 5)
```

Given the `rect` object, we want to evaluate whatever expressions we can in this source code:

```python
source = "(rect.width, rect.height, rect.area)"
```

This library works with the AST, so let's parse the source code and peek inside:

```python
import ast

tree = ast.parse(source)
the_tuple = tree.body[0].value
for node in the_tuple.elts:
    print(ast.dump(node))
```

Output:

```python
Attribute(value=Name(id='rect', ctx=Load()), attr='width', ctx=Load())
Attribute(value=Name(id='rect', ctx=Load()), attr='height', ctx=Load())
Attribute(value=Name(id='rect', ctx=Load()), attr='area', ctx=Load())
```

Now to actually use the library. First construct an Evaluator:

```python
from pure_eval import Evaluator

evaluator = Evaluator({"rect": rect})
```

The argument to `Evaluator` should be a mapping from variable names to their values. Or if you have access to the stack frame where `rect` is defined, you can instead use:

```python
evaluator = Evaluator.from_frame(frame)
```

Now to evaluate some nodes, using `evaluator[node]`:

```python
print("rect.width:", evaluator[the_tuple.elts[0]])
print("rect:", evaluator[the_tuple.elts[0].value])
```

Output:

```
rect.width: 3
rect: <__main__.Rectangle object at 0x105b0dd30>
```

OK, but you could have done the same thing with `eval`. The useful part is that it will refuse to evaluate the property `rect.area` because that would trigger unknown code. If we try, it'll raise a `CannotEval` exception.

```python
from pure_eval import CannotEval

try:
    print("rect.area:", evaluator[the_tuple.elts[2]])  # fails
except CannotEval as e:
    print(e)  # prints CannotEval
```

To find all the expressions that can be evaluated in a tree:

```python
for node, value in evaluator.find_expressions(tree):
    print(ast.dump(node), value)
```

Output:

```python
Attribute(value=Name(id='rect', ctx=Load()), attr='width', ctx=Load()) 3
Attribute(value=Name(id='rect', ctx=Load()), attr='height', ctx=Load()) 5
Name(id='rect', ctx=Load()) <__main__.Rectangle object at 0x105568d30>
Name(id='rect', ctx=Load()) <__main__.Rectangle object at 0x105568d30>
Name(id='rect', ctx=Load()) <__main__.Rectangle object at 0x105568d30>
```

Note that this includes `rect` three times, once for each appearance in the source code. Since all these nodes are equivalent, we can group them together:

```python
from pure_eval import group_expressions

for nodes, values in group_expressions(evaluator.find_expressions(tree)):
    print(len(nodes), "nodes with value:", values)
```

Output:

```
1 nodes with value: 3
1 nodes with value: 5
3 nodes with value: <__main__.Rectangle object at 0x10d374d30>
```

If we want to list all the expressions in a tree, we may want to filter out certain expressions whose values are obvious. For example, suppose we have a function `foo`:

```python
def foo():
    pass
```

If we refer to `foo` by its name as usual, then that's not interesting:

```python
from pure_eval import is_expression_interesting

node = ast.parse('foo').body[0].value
print(ast.dump(node))
print(is_expression_interesting(node, foo))
```

Output:

```python
Name(id='foo', ctx=Load())
False
```

But if we refer to it by a different name, then it's interesting:

```python
node = ast.parse('bar').body[0].value
print(ast.dump(node))
print(is_expression_interesting(node, foo))
```

Output:

```python
Name(id='bar', ctx=Load())
True
```

In general `is_expression_interesting` returns False for the following values:
- Literals (e.g. `123`, `'abc'`, `[1, 2, 3]`, `{'a': (), 'b': ([1, 2], [3])}`)
- Variables or attributes whose name is equal to the value's `__name__`, such as `foo` above or `self.foo` if it was a method.
- Builtins (e.g. `len`) referred to by their usual name.

To make things easier, you can combine finding expressions, grouping them, and filtering out the obvious ones with:

```python
evaluator.interesting_expressions_grouped(root)
```

To get the source code of an AST node, I recommend [asttokens](https://github.com/gristlabs/asttokens).

Here's a complete example that brings it all together:

```python
from asttokens import ASTTokens
from pure_eval import Evaluator

source = \"\"\"
x = 1
d = {x: 2}
y = d[x]
\"\"\"

names = {}
exec(source, names)
atok = ASTTokens(source, parse=True)
for nodes, value in Evaluator(names).interesting_expressions_grouped(atok.tree):
    print(atok.get_text(nodes[0]), "=", value)
```

Output:

```python
x = 1
d = {1: 2}
y = 2
d[x] = 2
```
"""^^xsd:string ;
    schema1:name "pure_eval"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.2.3"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/py-serializable> a software_vocab:Software_package ;
    schema1:description """# py-serializable

[![shield_pypi-version]][link_pypi]
[![shield_conda-forge-version]][link_conda-forge]
[![shield_rtfd]][link_rtfd]
[![shield_gh-workflow-test]][link_gh-workflow-test]
[![shield_license]][license_file]
[![shield_twitter-follow]][link_twitter]

----

This Pythonic library provides a framework for serializing/deserializing Python classes to and from JSON and XML.

It relies upon the use of 
[Python Properties](https://docs.python.org/3/library/functions.html?highlight=property#property) in your Python
classes.

Read the full [documentation][link_rtfd] for more details.

## Installation

Install this from [PyPi.org][link_pypi] using your preferred Python package manager.

Example using `pip`:

```shell
pip install py-serializable
```

Example using `poetry`:

```shell
poetry add py-serializable
```

## Usage

See the full [documentation][link_rtfd] or our [unit tests][link_unit_tests] for usage and details.

## Python Support

We endeavour to support all functionality for all [current actively supported Python versions](https://www.python.org/downloads/).
However, some features may not be possible/present in older Python versions due to their lack of support.

## Contributing

Feel free to open issues, bugreports or pull requests.  
See the [CONTRIBUTING][contributing_file] file for details.

## Copyright & License

`py-serializable` is Copyright (c) Paul Horton 2022. All Rights Reserved.

Permission to modify and redistribute is granted under the terms of the Apache 2.0 license.  
See the [LICENSE][license_file] file for the full license.

[license_file]: https://github.com/madpah/serializable/blob/main/LICENSE
[contributing_file]: https://github.com/madpah/serializable/blob/main/CONTRIBUTING.md
[link_rtfd]: https://py-serializable.readthedocs.io/

[shield_gh-workflow-test]: https://img.shields.io/github/actions/workflow/status/madpah/serializable/python.yml?branch=main "build"
[shield_rtfd]: https://img.shields.io/readthedocs/py-serializable?logo=readthedocs&logoColor=white
[shield_pypi-version]: https://img.shields.io/pypi/v/py-serializable?logo=Python&logoColor=white&label=PyPI "PyPI"
[shield_conda-forge-version]: https://img.shields.io/conda/vn/conda-forge/py-serializable?logo=anaconda&logoColor=white&label=conda-forge "conda-forge"
[shield_license]: https://img.shields.io/github/license/madpah/serializable?logo=open%20source%20initiative&logoColor=white "license"
[shield_twitter-follow]: https://img.shields.io/badge/Twitter-follow-blue?logo=Twitter&logoColor=white "twitter follow"
[link_gh-workflow-test]: https://github.com/madpah/serializable/actions/workflows/python.yml?query=branch%3Amain
[link_pypi]: https://pypi.org/project/py-serializable/
[link_conda-forge]: https://anaconda.org/conda-forge/py-serializable
[link_twitter]: https://twitter.com/madpah
[link_unit_tests]: https://github.com/madpah/serializable/blob/main/tests

"""^^xsd:string ;
    schema1:name "py-serializable"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.1.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pydantic> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/annotated-types>,
        <https://data.siemens-energy.com/cybersec/package/email-validator>,
        <https://data.siemens-energy.com/cybersec/package/pydantic-core>,
        <https://data.siemens-energy.com/cybersec/package/typing-extensions> ;
    schema1:description """# Pydantic
[![CI](https://img.shields.io/github/actions/workflow/status/pydantic/pydantic/ci.yml?branch=main&logo=github&label=CI)](https://github.com/pydantic/pydantic/actions?query=event%3Apush+branch%3Amain+workflow%3ACI)
[![Coverage](https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic.svg)](https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic)
[![pypi](https://img.shields.io/pypi/v/pydantic.svg)](https://pypi.python.org/pypi/pydantic)
[![CondaForge](https://img.shields.io/conda/v/conda-forge/pydantic.svg)](https://anaconda.org/conda-forge/pydantic)
[![downloads](https://static.pepy.tech/badge/pydantic/month)](https://pepy.tech/project/pydantic)
[![versions](https://img.shields.io/pypi/pyversions/pydantic.svg)](https://github.com/pydantic/pydantic)
[![license](https://img.shields.io/github/license/pydantic/pydantic.svg)](https://github.com/pydantic/pydantic/blob/main/LICENSE)
[![Pydantic v2](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json)](https://docs.pydantic.dev/latest/contributing/#badges)

Data validation using Python type hints.

Fast and extensible, Pydantic plays nicely with your linters/IDE/brain.
Define how data should be in pure, canonical Python 3.8+; validate it with Pydantic.

## Pydantic Company :rocket:

We've started a company based on the principles that I believe have led to Pydantic's success.
Learn more from the [Company Announcement](https://blog.pydantic.dev/blog/2023/02/16/company-announcement--pydantic/).

## Pydantic V1.10 vs. V2

Pydantic V2 is a ground-up rewrite that offers many new features, performance improvements, and some breaking changes compared to Pydantic V1.

If you're using Pydantic V1 you may want to look at the
[pydantic V1.10 Documentation](https://docs.pydantic.dev/) or,
[`1.10.X-fixes` git branch](https://github.com/pydantic/pydantic/tree/1.10.X-fixes). Pydantic V2 also ships with the latest version of Pydantic V1 built in so that you can incrementally upgrade your code base and projects: `from pydantic import v1 as pydantic_v1`.

## Help

See [documentation](https://docs.pydantic.dev/) for more details.

## Installation

Install using `pip install -U pydantic` or `conda install pydantic -c conda-forge`.
For more installation options to make Pydantic even faster,
see the [Install](https://docs.pydantic.dev/install/) section in the documentation.

## A Simple Example

```py
from datetime import datetime
from typing import List, Optional
from pydantic import BaseModel

class User(BaseModel):
    id: int
    name: str = 'John Doe'
    signup_ts: Optional[datetime] = None
    friends: List[int] = []

external_data = {'id': '123', 'signup_ts': '2017-06-01 12:22', 'friends': [1, '2', b'3']}
user = User(**external_data)
print(user)
#> User id=123 name='John Doe' signup_ts=datetime.datetime(2017, 6, 1, 12, 22) friends=[1, 2, 3]
print(user.id)
#> 123
```

## Contributing

For guidance on setting up a development environment and how to make a
contribution to Pydantic, see
[Contributing to Pydantic](https://docs.pydantic.dev/contributing/).

## Reporting a Security Vulnerability

See our [security policy](https://github.com/pydantic/pydantic/security/policy).

## Changelog

## v2.9.2 (2024-09-17)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.9.2)

### What's Changed

#### Fixes
* Do not error when trying to evaluate annotations of private attributes by [@Viicos](https://github.com/Viicos) in [#10358](https://github.com/pydantic/pydantic/pull/10358)
* Adding notes on designing sound `Callable` discriminators by [@sydney-runkle](https://github.com/sydney-runkle) in [#10400](https://github.com/pydantic/pydantic/pull/10400)
* Fix serialization schema generation when using `PlainValidator` by [@Viicos](https://github.com/Viicos) in [#10427](https://github.com/pydantic/pydantic/pull/10427)
* Fix `Union` serialization warnings by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1449](https://github.com/pydantic/pydantic-core/pull/1449)
* Fix variance issue in `_IncEx` type alias, only allow `True` by [@Viicos](https://github.com/Viicos) in [#10414](https://github.com/pydantic/pydantic/pull/10414)
* Fix `ZoneInfo` validation with various invalid types by [@sydney-runkle](https://github.com/sydney-runkle) in [#10408](https://github.com/pydantic/pydantic/pull/10408)

## v2.9.1 (2024-09-09)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.9.1)

### What's Changed

#### Fixes
* Fix Predicate issue in v2.9.0 by [@sydney-runkle](https://github.com/sydney-runkle) in [#10321](https://github.com/pydantic/pydantic/pull/10321)
* Fixing `annotated-types` bound to `>=0.6.0` by [@sydney-runkle](https://github.com/sydney-runkle) in [#10327](https://github.com/pydantic/pydantic/pull/10327)
* Turn `tzdata` install requirement into optional `timezone` dependency by [@jakob-keller](https://github.com/jakob-keller) in [#10331](https://github.com/pydantic/pydantic/pull/10331)
* Fix `IncExc` type alias definition by [@Viicos](https://github.com/Viicos) in [#10339](https://github.com/pydantic/pydantic/pull/10339)
* Use correct types namespace when building namedtuple core schemas by [@Viicos](https://github.com/Viicos) in [#10337](https://github.com/pydantic/pydantic/pull/10337)
* Fix evaluation of stringified annotations during namespace inspection by [@Viicos](https://github.com/Viicos) in [#10347](https://github.com/pydantic/pydantic/pull/10347)
* Fix tagged union serialization with alias generators by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1442](https://github.com/pydantic/pydantic-core/pull/1442)

## v2.9.0 (2024-09-05)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.9.0)

The code released in v2.9.0 is practically identical to that of v2.9.0b2.

### What's Changed

#### Packaging

* Bump `ruff` to `v0.5.0` and `pyright` to `v1.1.369` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9801](https://github.com/pydantic/pydantic/pull/9801)
* Bump `pydantic-extra-types` to `v2.9.0` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9832](https://github.com/pydantic/pydantic/pull/9832)
* Support compatibility with `pdm v2.18.1` by [@Viicos](https://github.com/Viicos) in [#10138](https://github.com/pydantic/pydantic/pull/10138)
* Bump `v1` version stub to `v1.10.18` by [@sydney-runkle](https://github.com/sydney-runkle) in [#10214](https://github.com/pydantic/pydantic/pull/10214)
* Bump `pydantic-core` to `v2.23.2` by [@sydney-runkle](https://github.com/sydney-runkle) in [#10311](https://github.com/pydantic/pydantic/pull/10311)

#### New Features

* Add support for `ZoneInfo` by [@Youssefares](https://github.com/Youssefares) in [#9896](https://github.com/pydantic/pydantic/pull/9896)
* Add `Config.val_json_bytes` by [@josh-newman](https://github.com/josh-newman) in [#9770](https://github.com/pydantic/pydantic/pull/9770)
* Add DSN for Snowflake by [@aditkumar72](https://github.com/aditkumar72) in [#10128](https://github.com/pydantic/pydantic/pull/10128)
* Support `complex` number by [@changhc](https://github.com/changhc) in [#9654](https://github.com/pydantic/pydantic/pull/9654)
* Add support for `annotated_types.Not` by [@aditkumar72](https://github.com/aditkumar72) in [#10210](https://github.com/pydantic/pydantic/pull/10210)
* Allow `WithJsonSchema` to inject `$ref`s w/ `http` or `https` links by [@dAIsySHEng1](https://github.com/dAIsySHEng1) in [#9863](https://github.com/pydantic/pydantic/pull/9863)
* Allow validators to customize validation JSON schema by [@Viicos](https://github.com/Viicos) in [#10094](https://github.com/pydantic/pydantic/pull/10094)
* Support parametrized `PathLike` types by [@nix010](https://github.com/nix010) in [#9764](https://github.com/pydantic/pydantic/pull/9764)
* Add tagged union serializer that attempts to use `str` or `callable` discriminators to select the correct serializer by [@sydney-runkle](https://github.com/sydney-runkle) in in [pydantic/pydantic-core#1397](https://github.com/pydantic/pydantic-core/pull/1397)

#### Changes

* Breaking Change: Merge `dict` type `json_schema_extra` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9792](https://github.com/pydantic/pydantic/pull/9792)
  * For more info (how to replicate old behavior) on this change, see [here](https://docs.pydantic.dev/dev/concepts/json_schema/#merging-json_schema_extra)
* Refactor annotation injection for known (often generic) types by [@sydney-runkle](https://github.com/sydney-runkle) in [#9979](https://github.com/pydantic/pydantic/pull/9979)
* Move annotation compatibility errors to validation phase by [@sydney-runkle](https://github.com/sydney-runkle) in [#9999](https://github.com/pydantic/pydantic/pull/9999)
* Improve runtime errors for string constraints like `pattern` for incompatible types by [@sydney-runkle](https://github.com/sydney-runkle) in [#10158](https://github.com/pydantic/pydantic/pull/10158)
* Remove `'allOf'` JSON schema workarounds by [@dpeachey](https://github.com/dpeachey) in [#10029](https://github.com/pydantic/pydantic/pull/10029)
* Remove `typed_dict_cls` data from `CoreMetadata` by [@sydney-runkle](https://github.com/sydney-runkle) in [#10180](https://github.com/pydantic/pydantic/pull/10180)
* Deprecate passing a dict to the `Examples` class by [@Viicos](https://github.com/Viicos) in [#10181](https://github.com/pydantic/pydantic/pull/10181)
* Remove `initial_metadata` from internal metadata construct by [@sydney-runkle](https://github.com/sydney-runkle) in [#10194](https://github.com/pydantic/pydantic/pull/10194)
* Use `re.Pattern.search` instead of `re.Pattern.match` for consistency with `rust` behavior by [@tinez](https://github.com/tinez) in [pydantic/pydantic-core#1368](https://github.com/pydantic/pydantic-core/pull/1368)
* Show value of wrongly typed data in `pydantic-core` serialization warning by [@BoxyUwU](https://github.com/BoxyUwU) in [pydantic/pydantic-core#1377](https://github.com/pydantic/pydantic-core/pull/1377)
* Breaking Change: in `pydantic-core`, change `metadata` type hint in core schemas from `Any` -> `Dict[str, Any] | None` by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1411](https://github.com/pydantic/pydantic-core/pull/1411)
* Raise helpful warning when `self` isn't returned from model validator by [@sydney-runkle](https://github.com/sydney-runkle) in [#10255](https://github.com/pydantic/pydantic/pull/10255)

#### Performance

* Initial start at improving import times for modules, using caching primarily by [@sydney-runkle](https://github.com/sydney-runkle) in [#10009](https://github.com/pydantic/pydantic/pull/10009)
* Using cached internal import for `BaseModel` by [@sydney-runkle](https://github.com/sydney-runkle) in [#10013](https://github.com/pydantic/pydantic/pull/10013)
* Simplify internal generics logic - remove generator overhead by [@sydney-runkle](https://github.com/sydney-runkle) in [#10059](https://github.com/pydantic/pydantic/pull/10059)
* Remove default module globals from types namespace by [@sydney-runkle](https://github.com/sydney-runkle) in [#10123](https://github.com/pydantic/pydantic/pull/10123)
* Performance boost: skip caching parent namespaces in most cases by [@sydney-runkle](https://github.com/sydney-runkle) in [#10113](https://github.com/pydantic/pydantic/pull/10113)
* Update ns stack with already copied ns by [@sydney-runkle](https://github.com/sydney-runkle) in [#10267](https://github.com/pydantic/pydantic/pull/10267)

##### Minor Internal Improvements
* âš¡ï¸ Speed up `multiple_of_validator()` by 31% in `pydantic/_internal/_validators.py` by [@misrasaurabh1](https://github.com/misrasaurabh1) in [#9839](https://github.com/pydantic/pydantic/pull/9839)
* âš¡ï¸ Speed up `ModelPrivateAttr.__set_name__()` by 18% in `pydantic/fields.py` by [@misrasaurabh1](https://github.com/misrasaurabh1) in [#9841](https://github.com/pydantic/pydantic/pull/9841)
* âš¡ï¸ Speed up `dataclass()` by 7% in `pydantic/dataclasses.py` by [@misrasaurabh1](https://github.com/misrasaurabh1) in [#9843](https://github.com/pydantic/pydantic/pull/9843)
* âš¡ï¸ Speed up function `_field_name_for_signature` by 37% in `pydantic/_internal/_signature.py` by [@misrasaurabh1](https://github.com/misrasaurabh1) in [#9951](https://github.com/pydantic/pydantic/pull/9951)
* âš¡ï¸ Speed up method `GenerateSchema._unpack_refs_defs` by 26% in `pydantic/_internal/_generate_schema.py` by [@misrasaurabh1](https://github.com/misrasaurabh1) in [#9949](https://github.com/pydantic/pydantic/pull/9949)
* âš¡ï¸ Speed up function `apply_each_item_validators` by 100% in `pydantic/_internal/_generate_schema.py` by [@misrasaurabh1](https://github.com/misrasaurabh1) in [#9950](https://github.com/pydantic/pydantic/pull/9950)
* âš¡ï¸ Speed up method `ConfigWrapper.core_config` by 28% in `pydantic/_internal/_config.py` by [@misrasaurabh1](https://github.com/misrasaurabh1) in [#9953](https://github.com/pydantic/pydantic/pull/9953)

#### Fixes

* Respect `use_enum_values` on `Literal` types by [@kwint](https://github.com/kwint) in [#9787](https://github.com/pydantic/pydantic/pull/9787)
* Prevent type error for exotic `BaseModel/RootModel` inheritance by [@dmontagu](https://github.com/dmontagu) in [#9913](https://github.com/pydantic/pydantic/pull/9913)
* Fix typing issue with field_validator-decorated methods by [@dmontagu](https://github.com/dmontagu) in [#9914](https://github.com/pydantic/pydantic/pull/9914)
* Replace `str` type annotation with `Any` in validator factories in documentation on validators by [@maximilianfellhuber](https://github.com/maximilianfellhuber) in [#9885](https://github.com/pydantic/pydantic/pull/9885)
* Fix `ComputedFieldInfo.wrapped_property` pointer when a property setter is assigned by [@tlambert03](https://github.com/tlambert03) in [#9892](https://github.com/pydantic/pydantic/pull/9892)
* Fix recursive typing of `main.IncEnx` by [@tlambert03](https://github.com/tlambert03) in [#9924](https://github.com/pydantic/pydantic/pull/9924)
* Allow usage of `type[Annotated[...]]` by [@Viicos](https://github.com/Viicos) in [#9932](https://github.com/pydantic/pydantic/pull/9932)
* `mypy` plugin: handle frozen fields on a per-field basis by [@dmontagu](https://github.com/dmontagu) in [#9935](https://github.com/pydantic/pydantic/pull/9935)
* Fix typo in `invalid-annotated-type` error code by [@sydney-runkle](https://github.com/sydney-runkle) in [#9948](https://github.com/pydantic/pydantic/pull/9948)
* Simplify schema generation for `uuid`, `url`, and `ip` types by [@sydney-runkle](https://github.com/sydney-runkle) in [#9975](https://github.com/pydantic/pydantic/pull/9975)
* Move `date` schemas to `_generate_schema.py` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9976](https://github.com/pydantic/pydantic/pull/9976)
* Move `decimal.Decimal` validation to `_generate_schema.py` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9977](https://github.com/pydantic/pydantic/pull/9977)
* Simplify IP address schema in `_std_types_schema.py` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9959](https://github.com/pydantic/pydantic/pull/9959)
* Fix type annotations for some potentially generic `GenerateSchema.match_type` options by [@sydney-runkle](https://github.com/sydney-runkle) in [#9961](https://github.com/pydantic/pydantic/pull/9961)
* Add class name to "has conflict" warnings by [@msabramo](https://github.com/msabramo) in [#9964](https://github.com/pydantic/pydantic/pull/9964)
* Fix `dataclass` ignoring `default_factory` passed in Annotated by [@kc0506](https://github.com/kc0506) in [#9971](https://github.com/pydantic/pydantic/pull/9971)
* Fix `Sequence` ignoring `discriminator` by [@kc0506](https://github.com/kc0506) in [#9980](https://github.com/pydantic/pydantic/pull/9980)
* Fix typing for `IPvAnyAddress` and `IPvAnyInterface` by [@haoyun](https://github.com/haoyun) in [#9990](https://github.com/pydantic/pydantic/pull/9990)
* Fix false positives on v1 models in `mypy` plugin for `from_orm` check requiring from_attributes=True config by [@radekwlsk](https://github.com/radekwlsk) in [#9938](https://github.com/pydantic/pydantic/pull/9938)
* Apply `strict=True` to `__init__` in `mypy` plugin by [@kc0506](https://github.com/kc0506) in [#9998](https://github.com/pydantic/pydantic/pull/9998)
* Refactor application of `deque` annotations by [@sydney-runkle](https://github.com/sydney-runkle) in [#10018](https://github.com/pydantic/pydantic/pull/10018)
* Raise a better user error when failing to evaluate a forward reference by [@Viicos](https://github.com/Viicos) in [#10030](https://github.com/pydantic/pydantic/pull/10030)
* Fix evaluation of `__pydantic_extra__` annotation in specific circumstances by [@Viicos](https://github.com/Viicos) in [#10070](https://github.com/pydantic/pydantic/pull/10070)
* Fix `frozen` enforcement for `dataclasses` by [@sydney-runkle](https://github.com/sydney-runkle) in [#10066](https://github.com/pydantic/pydantic/pull/10066)
* Remove logic to handle unused `__get_pydantic_core_schema__` signature by [@Viicos](https://github.com/Viicos) in [#10075](https://github.com/pydantic/pydantic/pull/10075)
* Use `is_annotated` consistently by [@Viicos](https://github.com/Viicos) in [#10095](https://github.com/pydantic/pydantic/pull/10095)
* Fix `PydanticDeprecatedSince26` typo by [@kc0506](https://github.com/kc0506) in [#10101](https://github.com/pydantic/pydantic/pull/10101)
* Improve `pyright` tests, refactor model decorators signatures by [@Viicos](https://github.com/Viicos) in [#10092](https://github.com/pydantic/pydantic/pull/10092)
* Fix `ip` serialization logic by [@sydney-runkle](https://github.com/sydney-runkle) in [#10112](https://github.com/pydantic/pydantic/pull/10112)
* Warn when frozen defined twice for `dataclasses` by [@mochi22](https://github.com/mochi22) in [#10082](https://github.com/pydantic/pydantic/pull/10082)
* Do not compute JSON Schema default when plain serializers are used with `when_used` set to `'json-unless-none'` and the default value is `None` by [@Viicos](https://github.com/Viicos) in [#10121](https://github.com/pydantic/pydantic/pull/10121)
* Fix `ImportString` special cases by [@sydney-runkle](https://github.com/sydney-runkle) in [#10137](https://github.com/pydantic/pydantic/pull/10137)
* Blacklist default globals to support exotic user code with `__` prefixed annotations by [@sydney-runkle](https://github.com/sydney-runkle) in [#10136](https://github.com/pydantic/pydantic/pull/10136)
* Handle `nullable` schemas with `serialization` schema available during JSON Schema generation by [@Viicos](https://github.com/Viicos) in [#10132](https://github.com/pydantic/pydantic/pull/10132)
* Reorganize `BaseModel` annotations by [@kc0506](https://github.com/kc0506) in [#10110](https://github.com/pydantic/pydantic/pull/10110)
* Fix core schema simplification when serialization schemas are involved in specific scenarios by [@Viicos](https://github.com/Viicos) in [#10155](https://github.com/pydantic/pydantic/pull/10155)
* Add support for stringified annotations when using `PrivateAttr` with `Annotated` by [@Viicos](https://github.com/Viicos) in [#10157](https://github.com/pydantic/pydantic/pull/10157)
* Fix JSON Schema `number` type for literal and enum schemas by [@Viicos](https://github.com/Viicos) in [#10172](https://github.com/pydantic/pydantic/pull/10172)
* Fix JSON Schema generation of fields with plain validators in serialization mode by [@Viicos](https://github.com/Viicos) in [#10167](https://github.com/pydantic/pydantic/pull/10167)
* Fix invalid JSON Schemas being generated for functions in certain scenarios by [@Viicos](https://github.com/Viicos) in [#10188](https://github.com/pydantic/pydantic/pull/10188)
* Make sure generated JSON Schemas are valid in tests by [@Viicos](https://github.com/Viicos) in [#10182](https://github.com/pydantic/pydantic/pull/10182)
* Fix key error with custom serializer by [@sydney-runkle](https://github.com/sydney-runkle) in [#10200](https://github.com/pydantic/pydantic/pull/10200)
* Add 'wss' for allowed schemes in NatsDsn by [@swelborn](https://github.com/swelborn) in [#10224](https://github.com/pydantic/pydantic/pull/10224)
* Fix `Mapping` and `MutableMapping` annotations to use mapping schema instead of dict schema by [@sydney-runkle](https://github.com/sydney-runkle) in [#10020](https://github.com/pydantic/pydantic/pull/10020)
* Fix JSON Schema generation for constrained dates by [@Viicos](https://github.com/Viicos) in [#10185](https://github.com/pydantic/pydantic/pull/10185)
* Fix discriminated union bug regression when using enums by [@kfreezen](https://github.com/kfreezen) in [pydantic/pydantic-core#1286](https://github.com/pydantic/pydantic-core/pull/1286)
* Fix `field_serializer` with computed field when using `*` by [@nix010](https://github.com/nix010) in [pydantic/pydantic-core#1349](https://github.com/pydantic/pydantic-core/pull/1349)
* Try each option in `Union` serializer before inference by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1398](https://github.com/pydantic/pydantic-core/pull/1398)
* Fix `float` serialization behavior in `strict` mode by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1400](https://github.com/pydantic/pydantic-core/pull/1400)
* Introduce `exactness` into Decimal validation logic to improve union validation behavior by [@sydney-runkle](https://github.com/sydney-runkle) in in [pydantic/pydantic-core#1405](https://github.com/pydantic/pydantic-core/pull/1405)
* Fix new warnings assertions to use `pytest.warns()` by [@mgorny](https://github.com/mgorny) in [#10241](https://github.com/pydantic/pydantic/pull/10241)
* Fix a crash when cleaning the namespace in `ModelMetaclass` by [@Viicos](https://github.com/Viicos) in [#10242](https://github.com/pydantic/pydantic/pull/10242)
* Fix parent namespace issue with model rebuilds by [@sydney-runkle](https://github.com/sydney-runkle) in [#10257](https://github.com/pydantic/pydantic/pull/10257)
* Remove defaults filter for namespace by [@sydney-runkle](https://github.com/sydney-runkle) in [#10261](https://github.com/pydantic/pydantic/pull/10261)
* Use identity instead of equality after validating model in `__init__` by [@Viicos](https://github.com/Viicos) in [#10264](https://github.com/pydantic/pydantic/pull/10264)
* Support `BigInt` serialization for `int` subclasses by [@kxx317](https://github.com/kxx317) in [pydantic/pydantic-core#1417](https://github.com/pydantic/pydantic-core/pull/1417)
* Support signature for wrap validators without `info` by [@sydney-runkle](https://github.com/sydney-runkle) in [#10277](https://github.com/pydantic/pydantic/pull/10277)
* Ensure `__pydantic_complete__` is set when rebuilding `dataclasses` by [@Viicos](https://github.com/Viicos) in [#10291](https://github.com/pydantic/pydantic/pull/10291)
* Respect `schema_generator` config value in `TypeAdapter` by [@sydney-runkle](https://github.com/sydney-runkle) in [#10300](https://github.com/pydantic/pydantic/pull/10300)

### New Contributors

#### `pydantic`

* [@kwint](https://github.com/kwint) made their first contribution in [#9787](https://github.com/pydantic/pydantic/pull/9787)
* [@seekinginfiniteloop](https://github.com/seekinginfiniteloop) made their first contribution in [#9822](https://github.com/pydantic/pydantic/pull/9822)
* [@a-alexander](https://github.com/a-alexander) made their first contribution in [#9848](https://github.com/pydantic/pydantic/pull/9848)
* [@maximilianfellhuber](https://github.com/maximilianfellhuber) made their first contribution in [#9885](https://github.com/pydantic/pydantic/pull/9885)
* [@karmaBonfire](https://github.com/karmaBonfire) made their first contribution in [#9945](https://github.com/pydantic/pydantic/pull/9945)
* [@s-rigaud](https://github.com/s-rigaud) made their first contribution in [#9958](https://github.com/pydantic/pydantic/pull/9958)
* [@msabramo](https://github.com/msabramo) made their first contribution in [#9964](https://github.com/pydantic/pydantic/pull/9964)
* [@DimaCybr](https://github.com/DimaCybr) made their first contribution in [#9972](https://github.com/pydantic/pydantic/pull/9972)
* [@kc0506](https://github.com/kc0506) made their first contribution in [#9971](https://github.com/pydantic/pydantic/pull/9971)
* [@haoyun](https://github.com/haoyun) made their first contribution in [#9990](https://github.com/pydantic/pydantic/pull/9990)
* [@radekwlsk](https://github.com/radekwlsk) made their first contribution in [#9938](https://github.com/pydantic/pydantic/pull/9938)
* [@dpeachey](https://github.com/dpeachey) made their first contribution in [#10029](https://github.com/pydantic/pydantic/pull/10029)
* [@BoxyUwU](https://github.com/BoxyUwU) made their first contribution in [#10085](https://github.com/pydantic/pydantic/pull/10085)
* [@mochi22](https://github.com/mochi22) made their first contribution in [#10082](https://github.com/pydantic/pydantic/pull/10082)
* [@aditkumar72](https://github.com/aditkumar72) made their first contribution in [#10128](https://github.com/pydantic/pydantic/pull/10128)
* [@changhc](https://github.com/changhc) made their first contribution in [#9654](https://github.com/pydantic/pydantic/pull/9654)
* [@insumanth](https://github.com/insumanth) made their first contribution in [#10229](https://github.com/pydantic/pydantic/pull/10229)
* [@AdolfoVillalobos](https://github.com/AdolfoVillalobos) made their first contribution in [#10240](https://github.com/pydantic/pydantic/pull/10240)
* [@bllchmbrs](https://github.com/bllchmbrs) made their first contribution in [#10270](https://github.com/pydantic/pydantic/pull/10270)

#### `pydantic-core`

* [@kfreezen](https://github.com/kfreezen) made their first contribution in [pydantic/pydantic-core#1286](https://github.com/pydantic/pydantic-core/pull/1286)
* [@tinez](https://github.com/tinez) made their first contribution in [pydantic/pydantic-core#1368](https://github.com/pydantic/pydantic-core/pull/1368)
* [@fft001](https://github.com/fft001) made their first contribution in [pydantic/pydantic-core#1362](https://github.com/pydantic/pydantic-core/pull/1362)
* [@nix010](https://github.com/nix010) made their first contribution in [pydantic/pydantic-core#1349](https://github.com/pydantic/pydantic-core/pull/1349)
* [@BoxyUwU](https://github.com/BoxyUwU) made their first contribution in [pydantic/pydantic-core#1379](https://github.com/pydantic/pydantic-core/pull/1379)
* [@candleindark](https://github.com/candleindark) made their first contribution in [pydantic/pydantic-core#1404](https://github.com/pydantic/pydantic-core/pull/1404)
* [@changhc](https://github.com/changhc) made their first contribution in [pydantic/pydantic-core#1331](https://github.com/pydantic/pydantic-core/pull/1331)

## v2.9.0b2 (2024-08-30)

Pre-release, see [the GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.9.0b2) for details.

## v2.9.0b1 (2024-08-26)

Pre-release, see [the GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.9.0b1) for details.

## v2.8.2 (2024-07-03)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.8.2)

### What's Changed

#### Fixes

* Fix issue with assertion caused by pluggable schema validator by [@dmontagu](https://github.com/dmontagu) in [#9838](https://github.com/pydantic/pydantic/pull/9838)

## v2.8.1 (2024-07-03)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.8.1)

### What's Changed

#### Packaging
* Bump `ruff` to `v0.5.0` and `pyright` to `v1.1.369` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9801](https://github.com/pydantic/pydantic/pull/9801)
* Bump `pydantic-core` to `v2.20.1`, `pydantic-extra-types` to `v2.9.0` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9832](https://github.com/pydantic/pydantic/pull/9832)

#### Fixes
* Fix breaking change in `to_snake` from v2.7 -> v2.8 by [@sydney-runkle](https://github.com/sydney-runkle) in [#9812](https://github.com/pydantic/pydantic/pull/9812)
* Fix list constraint json schema application by [@sydney-runkle](https://github.com/sydney-runkle) in [#9818](https://github.com/pydantic/pydantic/pull/9818)
* Support time duration more than 23 by [@nix010](https://github.com/nix010) in [pydantic/speedate#64](https://github.com/pydantic/speedate/pull/64)
* Fix millisecond fraction being handled with the wrong scale by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/speedate#65](https://github.com/pydantic/speedate/pull/65)
* Handle negative fractional durations correctly by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/speedate#71](https://github.com/pydantic/speedate/pull/71)

## v2.8.0 (2024-07-01)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.8.0)

The code released in v2.8.0 is functionally identical to that of v2.8.0b1.

### What's Changed

#### Packaging

* Update citation version automatically with new releases by [@sydney-runkle](https://github.com/sydney-runkle) in [#9673](https://github.com/pydantic/pydantic/pull/9673)
* Bump pyright to `v1.1.367` and add type checking tests for pipeline API by [@adriangb](https://github.com/adriangb) in [#9674](https://github.com/pydantic/pydantic/pull/9674)
* Update `pydantic.v1` stub to `v1.10.17` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9707](https://github.com/pydantic/pydantic/pull/9707)
* General package updates to prep for `v2.8.0b1` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9741](https://github.com/pydantic/pydantic/pull/9741)
* Bump `pydantic-core` to `v2.20.0` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9745](https://github.com/pydantic/pydantic/pull/9745)
* Add support for Python 3.13 by [@sydney-runkle](https://github.com/sydney-runkle) in [#9743](https://github.com/pydantic/pydantic/pull/9743)
* Update `pdm` version used for `pdm.lock` to v2.16.1 by [@sydney-runkle](https://github.com/sydney-runkle) in [#9761](https://github.com/pydantic/pydantic/pull/9761)
* Update to `ruff` `v0.4.8` by [@Viicos](https://github.com/Viicos) in [#9585](https://github.com/pydantic/pydantic/pull/9585)

#### New Features

* Experimental: support `defer_build` for `TypeAdapter` by [@MarkusSintonen](https://github.com/MarkusSintonen) in [#8939](https://github.com/pydantic/pydantic/pull/8939)
* Implement `deprecated` field in json schema by [@NeevCohen](https://github.com/NeevCohen) in [#9298](https://github.com/pydantic/pydantic/pull/9298)
* Experimental: Add pipeline API by [@adriangb](https://github.com/adriangb) in [#9459](https://github.com/pydantic/pydantic/pull/9459)
* Add support for programmatic title generation by [@NeevCohen](https://github.com/NeevCohen) in [#9183](https://github.com/pydantic/pydantic/pull/9183)
* Implement `fail_fast` feature by [@uriyyo](https://github.com/uriyyo) in [#9708](https://github.com/pydantic/pydantic/pull/9708)
* Add `ser_json_inf_nan='strings'` mode to produce valid JSON by [@josh-newman](https://github.com/josh-newman) in [pydantic/pydantic-core#1307](https://github.com/pydantic/pydantic-core/pull/1307)

#### Changes

* Add warning when "alias" is set in ignored `Annotated` field by [@nix010](https://github.com/nix010) in [#9170](https://github.com/pydantic/pydantic/pull/9170)
* Support serialization of some serializable defaults in JSON schema by [@sydney-runkle](https://github.com/sydney-runkle) in [#9624](https://github.com/pydantic/pydantic/pull/9624)
* Relax type specification for `__validators__` values in `create_model` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9697](https://github.com/pydantic/pydantic/pull/9697)
* **Breaking Change:** Improve `smart` union matching logic by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1322](https://github.com/pydantic/pydantic-core/pull/1322)
You can read more about our `smart` union matching logic [here](https://docs.pydantic.dev/dev/concepts/unions/#smart-mode). In some cases, if the old behavior
is desired, you can switch to `left-to-right` mode and change the order of your `Union` members.

#### Performance

##### Internal Improvements

* âš¡ï¸ Speed up `_display_error_loc()` by 25% in `pydantic/v1/error_wrappers.py` by [@misrasaurabh1](https://github.com/misrasaurabh1) in [#9653](https://github.com/pydantic/pydantic/pull/9653)
* âš¡ï¸ Speed up `_get_all_json_refs()` by 34% in `pydantic/json_schema.py` by [@misrasaurabh1](https://github.com/misrasaurabh1) in [#9650](https://github.com/pydantic/pydantic/pull/9650)
* âš¡ï¸ Speed up `is_pydantic_dataclass()` by 41% in `pydantic/dataclasses.py` by [@misrasaurabh1](https://github.com/misrasaurabh1) in [#9652](https://github.com/pydantic/pydantic/pull/9652)
* âš¡ï¸ Speed up `to_snake()` by 27% in `pydantic/alias_generators.py` by [@misrasaurabh1](https://github.com/misrasaurabh1) in [#9747](https://github.com/pydantic/pydantic/pull/9747)
* âš¡ï¸ Speed up `unwrap_wrapped_function()` by 93% in `pydantic/_internal/_decorators.py` by [@misrasaurabh1](https://github.com/misrasaurabh1) in [#9727](https://github.com/pydantic/pydantic/pull/9727)

#### Fixes

* Replace `__spec__.parent` with `__package__` by [@hramezani](https://github.com/hramezani) in [#9331](https://github.com/pydantic/pydantic/pull/9331)
* Fix Outputted Model JSON Schema for `Sequence` type by [@anesmemisevic](https://github.com/anesmemisevic) in [#9303](https://github.com/pydantic/pydantic/pull/9303)
* Fix typing of `_frame_depth` by [@Viicos](https://github.com/Viicos) in [#9353](https://github.com/pydantic/pydantic/pull/9353)
* Make `ImportString` json schema compatible by [@amitschang](https://github.com/amitschang) in [#9344](https://github.com/pydantic/pydantic/pull/9344)
* Hide private attributes (`PrivateAttr`) from `__init__` signature in type checkers by [@idan22moral](https://github.com/idan22moral) in [#9293](https://github.com/pydantic/pydantic/pull/9293)
* Make detection of `TypeVar` defaults robust to the CPython `PEP-696` implementation by [@AlexWaygood](https://github.com/AlexWaygood) in [#9426](https://github.com/pydantic/pydantic/pull/9426)
* Fix usage of `PlainSerializer` with builtin types by [@Viicos](https://github.com/Viicos) in [#9450](https://github.com/pydantic/pydantic/pull/9450)
* Add more robust custom validation examples by [@ChrisPappalardo](https://github.com/ChrisPappalardo) in [#9468](https://github.com/pydantic/pydantic/pull/9468)
* Fix ignored `strict` specification for `StringConstraint(strict=False)` by [@vbmendes](https://github.com/vbmendes) in [#9476](https://github.com/pydantic/pydantic/pull/9476)
* **Breaking Change:** Use PEP 570 syntax by [@Viicos](https://github.com/Viicos) in [#9479](https://github.com/pydantic/pydantic/pull/9479)
* Use `Self` where possible by [@Viicos](https://github.com/Viicos) in [#9479](https://github.com/pydantic/pydantic/pull/9479)
* Do not alter `RootModel.model_construct` signature in the `mypy` plugin by [@Viicos](https://github.com/Viicos) in [#9480](https://github.com/pydantic/pydantic/pull/9480)
* Fixed type hint of `validation_context` by [@OhioDschungel6](https://github.com/OhioDschungel6) in [#9508](https://github.com/pydantic/pydantic/pull/9508)
* Support context being passed to TypeAdapter's `dump_json`/`dump_python` by [@alexcouper](https://github.com/alexcouper) in [#9495](https://github.com/pydantic/pydantic/pull/9495)
* Updates type signature for `Field()` constructor by [@bjmc](https://github.com/bjmc) in [#9484](https://github.com/pydantic/pydantic/pull/9484)
* Improve builtin alias generators by [@sydney-runkle](https://github.com/sydney-runkle) in [#9561](https://github.com/pydantic/pydantic/pull/9561)
* Fix typing of `TypeAdapter` by [@Viicos](https://github.com/Viicos) in [#9570](https://github.com/pydantic/pydantic/pull/9570)
* Add fallback default value for private fields in `__setstate__` of BaseModel by [@anhpham1509](https://github.com/anhpham1509) in [#9584](https://github.com/pydantic/pydantic/pull/9584)
* Support `PEP 746` by [@adriangb](https://github.com/adriangb) in [#9587](https://github.com/pydantic/pydantic/pull/9587)
* Allow validator and serializer functions to have default values by [@Viicos](https://github.com/Viicos) in [#9478](https://github.com/pydantic/pydantic/pull/9478)
* Fix bug with mypy plugin's handling of covariant `TypeVar` fields by [@dmontagu](https://github.com/dmontagu) in [#9606](https://github.com/pydantic/pydantic/pull/9606)
* Fix multiple annotation / constraint application logic by [@sydney-runkle](https://github.com/sydney-runkle) in [#9623](https://github.com/pydantic/pydantic/pull/9623)
* Respect `regex` flags in validation and json schema by [@sydney-runkle](https://github.com/sydney-runkle) in [#9591](https://github.com/pydantic/pydantic/pull/9591)
* Fix type hint on `IpvAnyAddress` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9640](https://github.com/pydantic/pydantic/pull/9640)
* Allow a field specifier on `__pydantic_extra__` by [@dmontagu](https://github.com/dmontagu) in [#9659](https://github.com/pydantic/pydantic/pull/9659)
* Use normalized case for file path comparison by [@sydney-runkle](https://github.com/sydney-runkle) in [#9737](https://github.com/pydantic/pydantic/pull/9737)
* Modify constraint application logic to allow field constraints on `Optional[Decimal]` by [@lazyhope](https://github.com/lazyhope) in [#9754](https://github.com/pydantic/pydantic/pull/9754)
* `validate_call` type params fix by [@sydney-runkle](https://github.com/sydney-runkle) in [#9760](https://github.com/pydantic/pydantic/pull/9760)
* Check all warnings returned by pytest.warns() by [@s-t-e-v-e-n-k](https://github.com/s-t-e-v-e-n-k) in [#9702](https://github.com/pydantic/pydantic/pull/9702)
* Reuse `re.Pattern` object in regex patterns to allow for regex flags by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1318](https://github.com/pydantic/pydantic-core/pull/1318)

### New Contributors

* [@idan22moral](https://github.com/idan22moral) made their first contribution in [#9294](https://github.com/pydantic/pydantic/pull/9294)
* [@anesmemisevic](https://github.com/anesmemisevic) made their first contribution in [#9303](https://github.com/pydantic/pydantic/pull/9303)
* [@max-muoto](https://github.com/max-muoto) made their first contribution in [#9338](https://github.com/pydantic/pydantic/pull/9338)
* [@amitschang](https://github.com/amitschang) made their first contribution in [#9344](https://github.com/pydantic/pydantic/pull/9344)
* [@paulmartin91](https://github.com/paulmartin91) made their first contribution in [#9410](https://github.com/pydantic/pydantic/pull/9410)
* [@OhioDschungel6](https://github.com/OhioDschungel6) made their first contribution in [#9405](https://github.com/pydantic/pydantic/pull/9405)
* [@AlexWaygood](https://github.com/AlexWaygood) made their first contribution in [#9426](https://github.com/pydantic/pydantic/pull/9426)
* [@kinuax](https://github.com/kinuax) made their first contribution in [#9433](https://github.com/pydantic/pydantic/pull/9433)
* [@antoni-jamiolkowski](https://github.com/antoni-jamiolkowski) made their first contribution in [#9431](https://github.com/pydantic/pydantic/pull/9431)
* [@candleindark](https://github.com/candleindark) made their first contribution in [#9448](https://github.com/pydantic/pydantic/pull/9448)
* [@nix010](https://github.com/nix010) made their first contribution in [#9170](https://github.com/pydantic/pydantic/pull/9170)
* [@tomy0000000](https://github.com/tomy0000000) made their first contribution in [#9457](https://github.com/pydantic/pydantic/pull/9457)
* [@vbmendes](https://github.com/vbmendes) made their first contribution in [#9470](https://github.com/pydantic/pydantic/pull/9470)
* [@micheleAlberto](https://github.com/micheleAlberto) made their first contribution in [#9471](https://github.com/pydantic/pydantic/pull/9471)
* [@ChrisPappalardo](https://github.com/ChrisPappalardo) made their first contribution in [#9468](https://github.com/pydantic/pydantic/pull/9468)
* [@blueTurtz](https://github.com/blueTurtz) made their first contribution in [#9475](https://github.com/pydantic/pydantic/pull/9475)
* [@WinterBlue16](https://github.com/WinterBlue16) made their first contribution in [#9477](https://github.com/pydantic/pydantic/pull/9477)
* [@bittner](https://github.com/bittner) made their first contribution in [#9500](https://github.com/pydantic/pydantic/pull/9500)
* [@alexcouper](https://github.com/alexcouper) made their first contribution in [#9495](https://github.com/pydantic/pydantic/pull/9495)
* [@bjmc](https://github.com/bjmc) made their first contribution in [#9484](https://github.com/pydantic/pydantic/pull/9484)
* [@pjvv](https://github.com/pjvv) made their first contribution in [#9529](https://github.com/pydantic/pydantic/pull/9529)
* [@nedbat](https://github.com/nedbat) made their first contribution in [#9530](https://github.com/pydantic/pydantic/pull/9530)
* [@gunnellEvan](https://github.com/gunnellEvan) made their first contribution in [#9469](https://github.com/pydantic/pydantic/pull/9469)
* [@jaymbans](https://github.com/jaymbans) made their first contribution in [#9531](https://github.com/pydantic/pydantic/pull/9531)
* [@MarcBresson](https://github.com/MarcBresson) made their first contribution in [#9534](https://github.com/pydantic/pydantic/pull/9534)
* [@anhpham1509](https://github.com/anhpham1509) made their first contribution in [#9584](https://github.com/pydantic/pydantic/pull/9584)
* [@K-dash](https://github.com/K-dash) made their first contribution in [#9595](https://github.com/pydantic/pydantic/pull/9595)
* [@s-t-e-v-e-n-k](https://github.com/s-t-e-v-e-n-k) made their first contribution in [#9527](https://github.com/pydantic/pydantic/pull/9527)
* [@airwoodix](https://github.com/airwoodix) made their first contribution in [#9506](https://github.com/pydantic/pydantic/pull/9506)
* [@misrasaurabh1](https://github.com/misrasaurabh1) made their first contribution in [#9653](https://github.com/pydantic/pydantic/pull/9653)
* [@AlessandroMiola](https://github.com/AlessandroMiola) made their first contribution in [#9740](https://github.com/pydantic/pydantic/pull/9740)
* [@mylapallilavanyaa](https://github.com/mylapallilavanyaa) made their first contribution in [#9746](https://github.com/pydantic/pydantic/pull/9746)
* [@lazyhope](https://github.com/lazyhope) made their first contribution in [#9754](https://github.com/pydantic/pydantic/pull/9754)
* [@YassinNouh21](https://github.com/YassinNouh21) made their first contribution in [#9759](https://github.com/pydantic/pydantic/pull/9759)

## v2.8.0b1 (2024-06-27)

Pre-release, see [the GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.8.0b1) for details.

## v2.7.4 (2024-06-12)

[Github release](https://github.com/pydantic/pydantic/releases/tag/v2.7.4)

### What's Changed

#### Packaging

* Bump `pydantic.v1` to `v1.10.16` reference by [@sydney-runkle](https://github.com/sydney-runkle) in [#9639](https://github.com/pydantic/pydantic/pull/9639)

#### Fixes

* Specify `recursive_guard` as kwarg in `FutureRef._evaluate` by [@vfazio](https://github.com/vfazio) in [#9612](https://github.com/pydantic/pydantic/pull/9612)

## v2.7.3 (2024-06-03)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.7.3)

### What's Changed

#### Packaging

* Bump `pydantic-core` to `v2.18.4` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9550](https://github.com/pydantic/pydantic/pull/9550)

#### Fixes

* Fix u style unicode strings in python [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/jiter#110](https://github.com/pydantic/jiter/pull/110)

## v2.7.2 (2024-05-28)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.7.2)

### What's Changed

#### Packaging

* Bump `pydantic-core` to `v2.18.3` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9515](https://github.com/pydantic/pydantic/pull/9515)

#### Fixes

* Replace `__spec__.parent` with `__package__` by [@hramezani](https://github.com/hramezani) in [#9331](https://github.com/pydantic/pydantic/pull/9331)
* Fix validation of `int`s with leading unary minus by [@RajatRajdeep](https://github.com/RajatRajdeep) in [pydantic/pydantic-core#1291](https://github.com/pydantic/pydantic-core/pull/1291)
* Fix `str` subclass validation for enums by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1273](https://github.com/pydantic/pydantic-core/pull/1273)
* Support `BigInt`s in `Literal`s and `Enum`s by [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/pydantic-core#1297](https://github.com/pydantic/pydantic-core/pull/1297)
* Fix: uuid - allow `str` subclass as input by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#1296](https://github.com/pydantic/pydantic-core/pull/1296)

## v2.7.1 (2024-04-23)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.7.1)

### What's Changed

#### Packaging

* Bump `pydantic-core` to `v2.18.2` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9307](https://github.com/pydantic/pydantic/pull/9307)

#### New Features

* Ftp and Websocket connection strings support by [@CherrySuryp](https://github.com/CherrySuryp) in [#9205](https://github.com/pydantic/pydantic/pull/9205)

#### Changes

* Use field description for RootModel schema description when there is `â€¦` by [@LouisGobert](https://github.com/LouisGobert) in [#9214](https://github.com/pydantic/pydantic/pull/9214)

#### Fixes

* Fix `validation_alias` behavior with `model_construct` for `AliasChoices` and `AliasPath` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9223](https://github.com/pydantic/pydantic/pull/9223)
* Revert `typing.Literal` and import it outside the TYPE_CHECKING block by [@frost-nzcr4](https://github.com/frost-nzcr4) in [#9232](https://github.com/pydantic/pydantic/pull/9232)
* Fix `Secret` serialization schema, applicable for unions by [@sydney-runkle](https://github.com/sydney-runkle) in [#9240](https://github.com/pydantic/pydantic/pull/9240)
* Fix `strict` application to `function-after` with `use_enum_values` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9279](https://github.com/pydantic/pydantic/pull/9279)
* Address case where `model_construct` on a class which defines `model_post_init` fails with `AttributeError` by [@babygrimes](https://github.com/babygrimes) in [#9168](https://github.com/pydantic/pydantic/pull/9168)
* Fix `model_json_schema` with config types by [@NeevCohen](https://github.com/NeevCohen) in [#9287](https://github.com/pydantic/pydantic/pull/9287)
* Support multiple zeros as an `int` by [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/pydantic-core#1269](https://github.com/pydantic/pydantic-core/pull/1269)
* Fix validation of `int`s with leading unary plus by [@cknv](https://github.com/cknv) in [pydantic/pydantic-core#1272](https://github.com/pydantic/pydantic-core/pull/1272)
* Fix interaction between `extra != 'ignore'` and `from_attributes=True` by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#1276](https://github.com/pydantic/pydantic-core/pull/1276)
* Handle error from `Enum`'s `missing` function as `ValidationError` by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1274](https://github.com/pydantic/pydantic-core/pull/1754)
* Fix memory leak with `Iterable` validation by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#1271](https://github.com/pydantic/pydantic-core/pull/1751)

### New Contributors

* [@zzstoatzz](https://github.com/zzstoatzz) made their first contribution in [#9219](https://github.com/pydantic/pydantic/pull/9219)
* [@frost-nzcr4](https://github.com/frost-nzcr4) made their first contribution in [#9232](https://github.com/pydantic/pydantic/pull/9232)
* [@CherrySuryp](https://github.com/CherrySuryp) made their first contribution in [#9205](https://github.com/pydantic/pydantic/pull/9205)
* [@vagenas](https://github.com/vagenas) made their first contribution in [#9268](https://github.com/pydantic/pydantic/pull/9268)
* [@ollz272](https://github.com/ollz272) made their first contribution in [#9262](https://github.com/pydantic/pydantic/pull/9262)
* [@babygrimes](https://github.com/babygrimes) made their first contribution in [#9168](https://github.com/pydantic/pydantic/pull/9168)
* [@swelborn](https://github.com/swelborn) made their first contribution in [#9296](https://github.com/pydantic/pydantic/pull/9296)
* [@kf-novi](https://github.com/kf-novi) made their first contribution in [#9236](https://github.com/pydantic/pydantic/pull/9236)
* [@lgeiger](https://github.com/lgeiger) made their first contribution in [#9288](https://github.com/pydantic/pydantic/pull/9288)

## v2.7.0 (2024-04-11)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.7.0)

The code released in v2.7.0 is practically identical to that of v2.7.0b1.

### What's Changed

#### Packaging

* Reorganize `pyproject.toml` sections by [@Viicos](https://github.com/Viicos) in [#8899](https://github.com/pydantic/pydantic/pull/8899)
* Bump `pydantic-core` to `v2.18.1` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9211](https://github.com/pydantic/pydantic/pull/9211)
* Adopt `jiter` `v0.2.0` by [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/pydantic-core#1250](https://github.com/pydantic/pydantic-core/pull/1250)

#### New Features

* Extract attribute docstrings from `FieldInfo.description` by [@Viicos](https://github.com/Viicos) in [#6563](https://github.com/pydantic/pydantic/pull/6563)
* Add a `with_config` decorator to comply with typing spec by [@Viicos](https://github.com/Viicos) in [#8611](https://github.com/pydantic/pydantic/pull/8611)
* Allow an optional separator splitting the value and unit of the result of `ByteSize.human_readable` by [@jks15satoshi](https://github.com/jks15satoshi) in [#8706](https://github.com/pydantic/pydantic/pull/8706)
* Add generic `Secret` base type by [@conradogarciaberrotaran](https://github.com/conradogarciaberrotaran) in [#8519](https://github.com/pydantic/pydantic/pull/8519)
* Make use of `Sphinx` inventories for cross references in docs by [@Viicos](https://github.com/Viicos) in [#8682](https://github.com/pydantic/pydantic/pull/8682)
* Add environment variable to disable plugins by [@geospackle](https://github.com/geospackle) in [#8767](https://github.com/pydantic/pydantic/pull/8767)
* Add support for `deprecated` fields by [@Viicos](https://github.com/Viicos) in [#8237](https://github.com/pydantic/pydantic/pull/8237)
* Allow `field_serializer('*')` by [@ornariece](https://github.com/ornariece) in [#9001](https://github.com/pydantic/pydantic/pull/9001)
* Handle a case when `model_config` is defined as a model property by [@alexeyt101](https://github.com/alexeyt101) in [#9004](https://github.com/pydantic/pydantic/pull/9004)
* Update `create_model()` to support `typing.Annotated` as input by [@wannieman98](https://github.com/wannieman98) in [#8947](https://github.com/pydantic/pydantic/pull/8947)
* Add `ClickhouseDsn` support by [@solidguy7](https://github.com/solidguy7) in [#9062](https://github.com/pydantic/pydantic/pull/9062)
* Add support for `re.Pattern[str]` to `pattern` field by [@jag-k](https://github.com/jag-k) in [#9053](https://github.com/pydantic/pydantic/pull/9053)
* Support for `serialize_as_any` runtime setting by [@sydney-runkle](https://github.com/sydney-runkle) in [#8830](https://github.com/pydantic/pydantic/pull/8830)
* Add support for `typing.Self` by [@Youssefares](https://github.com/Youssefares) in [#9023](https://github.com/pydantic/pydantic/pull/9023)
* Ability to pass `context` to serialization by [@ornariece](https://github.com/ornariece) in [#8965](https://github.com/pydantic/pydantic/pull/8965)
* Add feedback widget to docs with flarelytics integration by [@sydney-runkle](https://github.com/sydney-runkle) in [#9129](https://github.com/pydantic/pydantic/pull/9129)
* Support for parsing partial JSON strings in Python by [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/jiter#66](https://github.com/pydantic/jiter/pull/66)

**Finalized in v2.7.0, rather than v2.7.0b1:**
* Add support for field level number to str coercion option by [@NeevCohen](https://github.com/NeevCohen) in [#9137](https://github.com/pydantic/pydantic/pull/9137)
* Update `warnings` parameter for serialization utilities to allow raising a warning by [@Lance-Drane](https://github.com/Lance-Drane) in [#9166](https://github.com/pydantic/pydantic/pull/9166)

#### Changes

* Correct docs, logic for `model_construct` behavior with `extra` by [@sydney-runkle](https://github.com/sydney-runkle) in [#8807](https://github.com/pydantic/pydantic/pull/8807)
* Improve error message for improper `RootModel` subclasses by [@sydney-runkle](https://github.com/sydney-runkle) in [#8857](https://github.com/pydantic/pydantic/pull/8857)
* **Breaking Change:** Use `PEP570` syntax by [@Viicos](https://github.com/Viicos) in [#8940](https://github.com/pydantic/pydantic/pull/8940)
* Add `enum` and `type` to the JSON schema for single item literals by [@dmontagu](https://github.com/dmontagu) in [#8944](https://github.com/pydantic/pydantic/pull/8944)
* Deprecate `update_json_schema` internal function by [@sydney-runkle](https://github.com/sydney-runkle) in [#9125](https://github.com/pydantic/pydantic/pull/9125)
* Serialize duration to hour minute second, instead of just seconds by [@kakilangit](https://github.com/kakilangit) in [pydantic/speedate#50](https://github.com/pydantic/speedate/pull/50)
* Trimming str before parsing to int and float by [@hungtsetse](https://github.com/hungtsetse) in [pydantic/pydantic-core#1203](https://github.com/pydantic/pydantic-core/pull/1203)

#### Performance

* `enum` validator improvements by [@samuelcolvin](https://github.com/samuelcolvin) in [#9045](https://github.com/pydantic/pydantic/pull/9045)
* Move `enum` validation and serialization to Rust by [@samuelcolvin](https://github.com/samuelcolvin) in [#9064](https://github.com/pydantic/pydantic/pull/9064)
* Improve schema generation for nested dataclasses by [@sydney-runkle](https://github.com/sydney-runkle) in [#9114](https://github.com/pydantic/pydantic/pull/9114)
* Fast path for ASCII python string creation in JSON by [@samuelcolvin](https://github.com/samuelcolvin) in in [pydantic/jiter#72](https://github.com/pydantic/jiter/pull/72)
* SIMD integer and string JSON parsing on `aarch64`(**Note:** SIMD on x86 will be implemented in a future release) by [@samuelcolvin](https://github.com/samuelcolvin) in in [pydantic/jiter#65](https://github.com/pydantic/jiter/pull/65)
* Support JSON `Cow<str>` from `jiter` by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#1231](https://github.com/pydantic/pydantic-core/pull/1231)
* MAJOR performance improvement: update to PyO3 0.21 final by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#1248](https://github.com/pydantic/pydantic-core/pull/1248)
* cache Python strings by [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/pydantic-core#1240](https://github.com/pydantic/pydantic-core/pull/1240)

#### Fixes

* Fix strict parsing for some `Sequence`s by [@sydney-runkle](https://github.com/sydney-runkle) in [#8614](https://github.com/pydantic/pydantic/pull/8614)
* Add a check on the existence of `__qualname__` by [@anci3ntr0ck](https://github.com/anci3ntr0ck) in [#8642](https://github.com/pydantic/pydantic/pull/8642)
* Handle `__pydantic_extra__` annotation being a string or inherited by [@alexmojaki](https://github.com/alexmojaki) in [#8659](https://github.com/pydantic/pydantic/pull/8659)
* Fix json validation for `NameEmail` by [@Holi0317](https://github.com/Holi0317) in [#8650](https://github.com/pydantic/pydantic/pull/8650)
* Fix type-safety of attribute access in `BaseModel` by [@bluenote10](https://github.com/bluenote10) in [#8651](https://github.com/pydantic/pydantic/pull/8651)
* Fix bug with `mypy` plugin and `no_strict_optional = True` by [@dmontagu](https://github.com/dmontagu) in [#8666](https://github.com/pydantic/pydantic/pull/8666)
* Fix `ByteSize` error `type` change by [@sydney-runkle](https://github.com/sydney-runkle) in [#8681](https://github.com/pydantic/pydantic/pull/8681)
* Fix inheriting annotations in dataclasses by [@sydney-runkle](https://github.com/sydney-runkle) in [#8679](https://github.com/pydantic/pydantic/pull/8679)
* Fix regression in core schema generation for indirect definition references by [@dmontagu](https://github.com/dmontagu) in [#8702](https://github.com/pydantic/pydantic/pull/8702)
* Fix unsupported types bug with plain validator by [@sydney-runkle](https://github.com/sydney-runkle) in [#8710](https://github.com/pydantic/pydantic/pull/8710)
* Reverting problematic fix from 2.6 release, fixing schema building bug by [@sydney-runkle](https://github.com/sydney-runkle) in [#8718](https://github.com/pydantic/pydantic/pull/8718)
* fixes `__pydantic_config__` ignored for TypeDict by [@13sin](https://github.com/13sin) in [#8734](https://github.com/pydantic/pydantic/pull/8734)
* Fix test failures with `pytest v8.0.0` due to `pytest.warns()` starting to work inside `pytest.raises()` by [@mgorny](https://github.com/mgorny) in [#8678](https://github.com/pydantic/pydantic/pull/8678)
* Use `is_valid_field` from 1.x for `mypy` plugin by [@DanielNoord](https://github.com/DanielNoord) in [#8738](https://github.com/pydantic/pydantic/pull/8738)
* Better-support `mypy` strict equality flag by [@dmontagu](https://github.com/dmontagu) in [#8799](https://github.com/pydantic/pydantic/pull/8799)
* model_json_schema export with Annotated types misses 'required' parameters by [@LouisGobert](https://github.com/LouisGobert) in [#8793](https://github.com/pydantic/pydantic/pull/8793)
* Fix default inclusion in `FieldInfo.__repr_args__` by [@sydney-runkle](https://github.com/sydney-runkle) in [#8801](https://github.com/pydantic/pydantic/pull/8801)
* Fix resolution of forward refs in dataclass base classes that are not present in the subclass module namespace by [@matsjoyce-refeyn](https://github.com/matsjoyce-refeyn) in [#8751](https://github.com/pydantic/pydantic/pull/8751)
* Fix `BaseModel` type annotations to be resolvable by `typing.get_type_hints` by [@devmonkey22](https://github.com/devmonkey22) in [#7680](https://github.com/pydantic/pydantic/pull/7680)
* Fix: allow empty string aliases with `AliasGenerator` by [@sydney-runkle](https://github.com/sydney-runkle) in [#8810](https://github.com/pydantic/pydantic/pull/8810)
* Fix test along with `date` -> `datetime` timezone assumption fix by [@sydney-runkle](https://github.com/sydney-runkle) in [#8823](https://github.com/pydantic/pydantic/pull/8823)
* Fix deprecation warning with usage of `ast.Str` by [@Viicos](https://github.com/Viicos) in [#8837](https://github.com/pydantic/pydantic/pull/8837)
* Add missing `deprecated` decorators by [@Viicos](https://github.com/Viicos) in [#8877](https://github.com/pydantic/pydantic/pull/8877)
* Fix serialization of `NameEmail` if name includes an email address by [@NeevCohen](https://github.com/NeevCohen) in [#8860](https://github.com/pydantic/pydantic/pull/8860)
* Add information about class in error message of schema generation by [@Czaki](https://github.com/Czaki) in [#8917](https://github.com/pydantic/pydantic/pull/8917)
* Make `TypeAdapter`'s typing compatible with special forms by [@adriangb](https://github.com/adriangb) in [#8923](https://github.com/pydantic/pydantic/pull/8923)
* Fix issue with config behavior being baked into the ref schema for `enum`s by [@dmontagu](https://github.com/dmontagu) in [#8920](https://github.com/pydantic/pydantic/pull/8920)
* More helpful error re wrong `model_json_schema` usage by [@sydney-runkle](https://github.com/sydney-runkle) in [#8928](https://github.com/pydantic/pydantic/pull/8928)
* Fix nested discriminated union schema gen, pt 2 by [@sydney-runkle](https://github.com/sydney-runkle) in [#8932](https://github.com/pydantic/pydantic/pull/8932)
* Fix schema build for nested dataclasses / TypedDicts with discriminators by [@sydney-runkle](https://github.com/sydney-runkle) in [#8950](https://github.com/pydantic/pydantic/pull/8950)
* Remove unnecessary logic for definitions schema gen with discriminated unions by [@sydney-runkle](https://github.com/sydney-runkle) in [#8951](https://github.com/pydantic/pydantic/pull/8951)
* Fix handling of optionals in `mypy` plugin by [@dmontagu](https://github.com/dmontagu) in [#9008](https://github.com/pydantic/pydantic/pull/9008)
* Fix `PlainSerializer` usage with std type constructor by [@sydney-runkle](https://github.com/sydney-runkle) in [#9031](https://github.com/pydantic/pydantic/pull/9031)
* Remove unnecessary warning for config in plugin by [@dmontagu](https://github.com/dmontagu) in [#9039](https://github.com/pydantic/pydantic/pull/9039)
* Fix default value serializing by [@NeevCohen](https://github.com/NeevCohen) in [#9066](https://github.com/pydantic/pydantic/pull/9066)
* Fix extra fields check in `Model.__getattr__()` by [@NeevCohen](https://github.com/NeevCohen) in [#9082](https://github.com/pydantic/pydantic/pull/9082)
* Fix `ClassVar` forward ref inherited from parent class by [@alexmojaki](https://github.com/alexmojaki) in [#9097](https://github.com/pydantic/pydantic/pull/9097)
* fix sequence like validator with strict `True` by [@andresliszt](https://github.com/andresliszt) in [#8977](https://github.com/pydantic/pydantic/pull/8977)
* Improve warning message when a field name shadows a field in a parent model by [@chan-vince](https://github.com/chan-vince) in [#9105](https://github.com/pydantic/pydantic/pull/9105)
* Do not warn about shadowed fields if they are not redefined in a child class by [@chan-vince](https://github.com/chan-vince) in [#9111](https://github.com/pydantic/pydantic/pull/9111)
* Fix discriminated union bug with unsubstituted type var by [@sydney-runkle](https://github.com/sydney-runkle) in [#9124](https://github.com/pydantic/pydantic/pull/9124)
* Support serialization of `deque` when passed to `Sequence[blah blah blah]` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9128](https://github.com/pydantic/pydantic/pull/9128)
* Init private attributes from super-types in `model_post_init` by [@Viicos](https://github.com/Viicos) in [#9134](https://github.com/pydantic/pydantic/pull/9134)
* fix `model_construct` with `validation_alias` by [@ornariece](https://github.com/ornariece) in [#9144](https://github.com/pydantic/pydantic/pull/9144)
* Ensure json-schema generator handles `Literal` `null` types by [@bruno-f-cruz](https://github.com/bruno-f-cruz) in [#9135](https://github.com/pydantic/pydantic/pull/9135)
* **Fixed in v2.7.0**: Fix allow extra generic by [@dmontagu](https://github.com/dmontagu) in [#9193](https://github.com/pydantic/pydantic/pull/9193)

### New Contributors

* [@hungtsetse](https://github.com/hungtsetse) made their first contribution in [#8546](https://github.com/pydantic/pydantic/pull/8546)
* [@StrawHatDrag0n](https://github.com/StrawHatDrag0n) made their first contribution in [#8583](https://github.com/pydantic/pydantic/pull/8583)
* [@anci3ntr0ck](https://github.com/anci3ntr0ck) made their first contribution in [#8642](https://github.com/pydantic/pydantic/pull/8642)
* [@Holi0317](https://github.com/Holi0317) made their first contribution in [#8650](https://github.com/pydantic/pydantic/pull/8650)
* [@bluenote10](https://github.com/bluenote10) made their first contribution in [#8651](https://github.com/pydantic/pydantic/pull/8651)
* [@ADSteele916](https://github.com/ADSteele916) made their first contribution in [#8703](https://github.com/pydantic/pydantic/pull/8703)
* [@musicinmybrain](https://github.com/musicinmybrain) made their first contribution in [#8731](https://github.com/pydantic/pydantic/pull/8731)
* [@jks15satoshi](https://github.com/jks15satoshi) made their first contribution in [#8706](https://github.com/pydantic/pydantic/pull/8706)
* [@13sin](https://github.com/13sin) made their first contribution in [#8734](https://github.com/pydantic/pydantic/pull/8734)
* [@DanielNoord](https://github.com/DanielNoord) made their first contribution in [#8738](https://github.com/pydantic/pydantic/pull/8738)
* [@conradogarciaberrotaran](https://github.com/conradogarciaberrotaran) made their first contribution in [#8519](https://github.com/pydantic/pydantic/pull/8519)
* [@chris-griffin](https://github.com/chris-griffin) made their first contribution in [#8775](https://github.com/pydantic/pydantic/pull/8775)
* [@LouisGobert](https://github.com/LouisGobert) made their first contribution in [#8793](https://github.com/pydantic/pydantic/pull/8793)
* [@matsjoyce-refeyn](https://github.com/matsjoyce-refeyn) made their first contribution in [#8751](https://github.com/pydantic/pydantic/pull/8751)
* [@devmonkey22](https://github.com/devmonkey22) made their first contribution in [#7680](https://github.com/pydantic/pydantic/pull/7680)
* [@adamency](https://github.com/adamency) made their first contribution in [#8847](https://github.com/pydantic/pydantic/pull/8847)
* [@MamfTheKramf](https://github.com/MamfTheKramf) made their first contribution in [#8851](https://github.com/pydantic/pydantic/pull/8851)
* [@ornariece](https://github.com/ornariece) made their first contribution in [#9001](https://github.com/pydantic/pydantic/pull/9001)
* [@alexeyt101](https://github.com/alexeyt101) made their first contribution in [#9004](https://github.com/pydantic/pydantic/pull/9004)
* [@wannieman98](https://github.com/wannieman98) made their first contribution in [#8947](https://github.com/pydantic/pydantic/pull/8947)
* [@solidguy7](https://github.com/solidguy7) made their first contribution in [#9062](https://github.com/pydantic/pydantic/pull/9062)
* [@kloczek](https://github.com/kloczek) made their first contribution in [#9047](https://github.com/pydantic/pydantic/pull/9047)
* [@jag-k](https://github.com/jag-k) made their first contribution in [#9053](https://github.com/pydantic/pydantic/pull/9053)
* [@priya-gitTest](https://github.com/priya-gitTest) made their first contribution in [#9088](https://github.com/pydantic/pydantic/pull/9088)
* [@Youssefares](https://github.com/Youssefares) made their first contribution in [#9023](https://github.com/pydantic/pydantic/pull/9023)
* [@chan-vince](https://github.com/chan-vince) made their first contribution in [#9105](https://github.com/pydantic/pydantic/pull/9105)
* [@bruno-f-cruz](https://github.com/bruno-f-cruz) made their first contribution in [#9135](https://github.com/pydantic/pydantic/pull/9135)
* [@Lance-Drane](https://github.com/Lance-Drane) made their first contribution in [#9166](https://github.com/pydantic/pydantic/pull/9166)

## v2.7.0b1 (2024-04-03)

Pre-release, see [the GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.7.0b1) for details.

## v2.6.4 (2024-03-12)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.6.4)

### What's Changed

#### Fixes

* Fix usage of `AliasGenerator` with `computed_field` decorator by [@sydney-runkle](https://github.com/sydney-runkle) in [#8806](https://github.com/pydantic/pydantic/pull/8806)
* Fix nested discriminated union schema gen, pt 2 by [@sydney-runkle](https://github.com/sydney-runkle) in [#8932](https://github.com/pydantic/pydantic/pull/8932)
* Fix bug with no_strict_optional=True caused by API deferral by [@dmontagu](https://github.com/dmontagu) in [#8826](https://github.com/pydantic/pydantic/pull/8826)


## v2.6.3 (2024-02-27)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.6.3)

### What's Changed

#### Packaging

* Update `pydantic-settings` version in the docs by [@hramezani](https://github.com/hramezani) in [#8906](https://github.com/pydantic/pydantic/pull/8906)

#### Fixes

* Fix discriminated union schema gen bug by [@sydney-runkle](https://github.com/sydney-runkle) in [#8904](https://github.com/pydantic/pydantic/pull/8904)


## v2.6.2 (2024-02-23)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.6.2)

### What's Changed

#### Packaging

* Upgrade to `pydantic-core` 2.16.3 by [@sydney-runkle](https://github.com/sydney-runkle) in [#8879](https://github.com/pydantic/pydantic/pull/8879)

#### Fixes

* 'YYYY-MM-DD' date string coerced to datetime shouldn't infer timezone by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1193](https://github.com/pydantic/pydantic-core/pull/1193)


## v2.6.1 (2024-02-05)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.6.1)

### What's Changed

#### Packaging

* Upgrade to `pydantic-core` 2.16.2 by [@sydney-runkle](https://github.com/sydney-runkle) in [#8717](https://github.com/pydantic/pydantic/pull/8717)

#### Fixes

* Fix bug with `mypy` plugin and `no_strict_optional = True` by [@dmontagu](https://github.com/dmontagu) in [#8666](https://github.com/pydantic/pydantic/pull/8666)
* Fix `ByteSize` error `type` change by [@sydney-runkle](https://github.com/sydney-runkle) in [#8681](https://github.com/pydantic/pydantic/pull/8681)
* Fix inheriting `Field` annotations in dataclasses by [@sydney-runkle](https://github.com/sydney-runkle) in [#8679](https://github.com/pydantic/pydantic/pull/8679)
* Fix regression in core schema generation for indirect definition references by [@dmontagu](https://github.com/dmontagu) in [#8702](https://github.com/pydantic/pydantic/pull/8702)
* Fix unsupported types bug with `PlainValidator` by [@sydney-runkle](https://github.com/sydney-runkle) in [#8710](https://github.com/pydantic/pydantic/pull/8710)
* Reverting problematic fix from 2.6 release, fixing schema building bug by [@sydney-runkle](https://github.com/sydney-runkle) in [#8718](https://github.com/pydantic/pydantic/pull/8718)
* Fix warning for tuple of wrong size in `Union` by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#1174](https://github.com/pydantic/pydantic-core/pull/1174)
* Fix `computed_field` JSON serializer `exclude_none` behavior by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1187](https://github.com/pydantic/pydantic-core/pull/1187)


## v2.6.0 (2024-01-23)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.6.0)

The code released in v2.6.0 is practically identical to that of v2.6.0b1.

### What's Changed

#### Packaging

* Check for `email-validator` version >= 2.0 by [@commonism](https://github.com/commonism) in [#6033](https://github.com/pydantic/pydantic/pull/6033)
* Upgrade `ruff`` target version to Python 3.8 by [@Elkiwa](https://github.com/Elkiwa) in [#8341](https://github.com/pydantic/pydantic/pull/8341)
* Update to `pydantic-extra-types==2.4.1` by [@yezz123](https://github.com/yezz123) in [#8478](https://github.com/pydantic/pydantic/pull/8478)
* Update to `pyright==1.1.345` by [@Viicos](https://github.com/Viicos) in [#8453](https://github.com/pydantic/pydantic/pull/8453)
* Update pydantic-core from 2.14.6 to 2.16.1, significant changes from these updates are described below, full changelog [here](https://github.com/pydantic/pydantic-core/compare/v2.14.6...v2.16.1)

#### New Features

* Add `NatsDsn` by [@ekeew](https://github.com/ekeew) in [#6874](https://github.com/pydantic/pydantic/pull/6874)
* Add `ConfigDict.ser_json_inf_nan` by [@davidhewitt](https://github.com/davidhewitt) in [#8159](https://github.com/pydantic/pydantic/pull/8159)
* Add `types.OnErrorOmit` by [@adriangb](https://github.com/adriangb) in [#8222](https://github.com/pydantic/pydantic/pull/8222)
* Support `AliasGenerator` usage by [@sydney-runkle](https://github.com/sydney-runkle) in [#8282](https://github.com/pydantic/pydantic/pull/8282)
* Add Pydantic People Page to docs by [@sydney-runkle](https://github.com/sydney-runkle) in [#8345](https://github.com/pydantic/pydantic/pull/8345)
* Support `yyyy-MM-DD` datetime parsing by [@sydney-runkle](https://github.com/sydney-runkle) in [#8404](https://github.com/pydantic/pydantic/pull/8404)
* Added bits conversions to the `ByteSize` class [#8415](https://github.com/pydantic/pydantic/issues/8415) by [@luca-matei](https://github.com/luca-matei) in [#8507](https://github.com/pydantic/pydantic/pull/8507)
* Enable json schema creation with type `ByteSize` by [@geospackle](https://github.com/geospackle) in [#8537](https://github.com/pydantic/pydantic/pull/8537)
* Add `eval_type_backport` to handle union operator and builtin generic subscripting in older Pythons by [@alexmojaki](https://github.com/alexmojaki) in [#8209](https://github.com/pydantic/pydantic/pull/8209)
* Add support for `dataclass` fields `init` by [@dmontagu](https://github.com/dmontagu) in [#8552](https://github.com/pydantic/pydantic/pull/8552)
* Implement pickling for `ValidationError` by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#1119](https://github.com/pydantic/pydantic-core/pull/1119)
* Add unified tuple validator that can handle "variadic" tuples via PEP-646 by [@dmontagu](https://github.com/dmontagu) in [pydantic/pydantic-core#865](https://github.com/pydantic/pydantic-core/pull/865)

#### Changes

* Drop Python3.7 support by [@hramezani](https://github.com/hramezani) in [#7188](https://github.com/pydantic/pydantic/pull/7188)
* Drop Python 3.7, and PyPy 3.7 and 3.8 by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#1129](https://github.com/pydantic/pydantic-core/pull/1129)
* Use positional-only `self` in `BaseModel` constructor, so no field name can ever conflict with it by [@ariebovenberg](https://github.com/ariebovenberg) in [#8072](https://github.com/pydantic/pydantic/pull/8072)
* Make `@validate_call` return a function instead of a custom descriptor - fixes binding issue with inheritance and adds `self/cls` argument to validation errors by [@alexmojaki](https://github.com/alexmojaki) in [#8268](https://github.com/pydantic/pydantic/pull/8268)
* Exclude `BaseModel` docstring from JSON schema description by [@sydney-runkle](https://github.com/sydney-runkle) in [#8352](https://github.com/pydantic/pydantic/pull/8352)
* Introducing `classproperty` decorator for `model_computed_fields` by [@Jocelyn-Gas](https://github.com/Jocelyn-Gas) in [#8437](https://github.com/pydantic/pydantic/pull/8437)
* Explicitly raise an error if field names clashes with types by [@Viicos](https://github.com/Viicos) in [#8243](https://github.com/pydantic/pydantic/pull/8243)
* Use stricter serializer for unions of simple types by [@alexdrydew](https://github.com/alexdrydew) [pydantic/pydantic-core#1132](https://github.com/pydantic/pydantic-core/pull/1132)

#### Performance

* Add Codspeed profiling Actions workflow  by [@lambertsbennett](https://github.com/lambertsbennett) in [#8054](https://github.com/pydantic/pydantic/pull/8054)
* Improve `int` extraction by [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/pydantic-core#1155](https://github.com/pydantic/pydantic-core/pull/1155)
* Improve performance of recursion guard by [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/pydantic-core#1156](https://github.com/pydantic/pydantic-core/pull/1156)
* `dataclass` serialization speedups by [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/pydantic-core#1162](https://github.com/pydantic/pydantic-core/pull/1162)
* Avoid `HashMap` creation when looking up small JSON objects in `LazyIndexMaps` by [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/jiter#55](https://github.com/pydantic/jiter/pull/55)
* use hashbrown to speedup python string caching by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/jiter#51](https://github.com/pydantic/jiter/pull/51)
* Replace `Peak` with more efficient `Peek` by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/jiter#48](https://github.com/pydantic/jiter/pull/48)

#### Fixes

* Move `getattr` warning in deprecated `BaseConfig` by [@tlambert03](https://github.com/tlambert03) in [#7183](https://github.com/pydantic/pydantic/pull/7183)
* Only hash `model_fields`, not whole `__dict__` by [@alexmojaki](https://github.com/alexmojaki) in [#7786](https://github.com/pydantic/pydantic/pull/7786)
* Fix mishandling of unions while freezing types in the `mypy` plugin by [@dmontagu](https://github.com/dmontagu) in [#7411](https://github.com/pydantic/pydantic/pull/7411)
* Fix `mypy` error on untyped `ClassVar` by [@vincent-hachin-wmx](https://github.com/vincent-hachin-wmx) in [#8138](https://github.com/pydantic/pydantic/pull/8138)
* Only compare pydantic fields in `BaseModel.__eq__` instead of whole `__dict__` by [@QuentinSoubeyranAqemia](https://github.com/QuentinSoubeyranAqemia) in [#7825](https://github.com/pydantic/pydantic/pull/7825)
* Update `strict` docstring in `model_validate` method. by [@LukeTonin](https://github.com/LukeTonin) in [#8223](https://github.com/pydantic/pydantic/pull/8223)
* Fix overload position of `computed_field` by [@Viicos](https://github.com/Viicos) in [#8227](https://github.com/pydantic/pydantic/pull/8227)
* Fix custom type type casting used in multiple attributes by [@ianhfc](https://github.com/ianhfc) in [#8066](https://github.com/pydantic/pydantic/pull/8066)
* Fix issue not allowing `validate_call` decorator to be dynamically assigned to a class method by [@jusexton](https://github.com/jusexton) in [#8249](https://github.com/pydantic/pydantic/pull/8249)
* Fix issue `unittest.mock` deprecation warnings  by [@ibleedicare](https://github.com/ibleedicare) in [#8262](https://github.com/pydantic/pydantic/pull/8262)
* Added tests for the case `JsonValue` contains subclassed primitive values by [@jusexton](https://github.com/jusexton) in [#8286](https://github.com/pydantic/pydantic/pull/8286)
* Fix `mypy` error on free before validator (classmethod) by [@sydney-runkle](https://github.com/sydney-runkle) in [#8285](https://github.com/pydantic/pydantic/pull/8285)
* Fix `to_snake` conversion by [@jevins09](https://github.com/jevins09) in [#8316](https://github.com/pydantic/pydantic/pull/8316)
* Fix type annotation of `ModelMetaclass.__prepare__` by [@slanzmich](https://github.com/slanzmich) in [#8305](https://github.com/pydantic/pydantic/pull/8305)
* Disallow `config` specification when initializing a `TypeAdapter` when the annotated type has config already by [@sydney-runkle](https://github.com/sydney-runkle) in [#8365](https://github.com/pydantic/pydantic/pull/8365)
* Fix a naming issue with JSON schema for generics parametrized by recursive type aliases by [@dmontagu](https://github.com/dmontagu) in [#8389](https://github.com/pydantic/pydantic/pull/8389)
* Fix type annotation in pydantic people script by [@shenxiangzhuang](https://github.com/shenxiangzhuang) in [#8402](https://github.com/pydantic/pydantic/pull/8402)
* Add support for field `alias` in `dataclass` signature by [@NeevCohen](https://github.com/NeevCohen) in [#8387](https://github.com/pydantic/pydantic/pull/8387)
* Fix bug with schema generation with `Field(...)` in a forward ref by [@dmontagu](https://github.com/dmontagu) in [#8494](https://github.com/pydantic/pydantic/pull/8494)
* Fix ordering of keys in `__dict__` with `model_construct` call by [@sydney-runkle](https://github.com/sydney-runkle) in [#8500](https://github.com/pydantic/pydantic/pull/8500)
* Fix module `path_type` creation when globals does not contain `__name__` by [@hramezani](https://github.com/hramezani) in [#8470](https://github.com/pydantic/pydantic/pull/8470)
* Fix for namespace issue with dataclasses with `from __future__ import annotations` by [@sydney-runkle](https://github.com/sydney-runkle) in [#8513](https://github.com/pydantic/pydantic/pull/8513)
* Fix: make function validator types positional-only by [@pmmmwh](https://github.com/pmmmwh) in [#8479](https://github.com/pydantic/pydantic/pull/8479)
* Fix usage of `@deprecated` by [@Viicos](https://github.com/Viicos) in [#8294](https://github.com/pydantic/pydantic/pull/8294)
* Add more support for private attributes in `model_construct` call by [@sydney-runkle](https://github.com/sydney-runkle) in [#8525](https://github.com/pydantic/pydantic/pull/8525)
* Use a stack for the types namespace by [@dmontagu](https://github.com/dmontagu) in [#8378](https://github.com/pydantic/pydantic/pull/8378)
* Fix schema-building bug with `TypeAliasType` for types with refs by [@dmontagu](https://github.com/dmontagu) in [#8526](https://github.com/pydantic/pydantic/pull/8526)
* Support `pydantic.Field(repr=False)` in dataclasses by [@tigeryy2](https://github.com/tigeryy2) in [#8511](https://github.com/pydantic/pydantic/pull/8511)
* Override `dataclass_transform` behavior for `RootModel` by [@Viicos](https://github.com/Viicos) in [#8163](https://github.com/pydantic/pydantic/pull/8163)
* Refactor signature generation for simplicity by [@sydney-runkle](https://github.com/sydney-runkle) in [#8572](https://github.com/pydantic/pydantic/pull/8572)
* Fix ordering bug of PlainValidator annotation by [@Anvil](https://github.com/Anvil) in [#8567](https://github.com/pydantic/pydantic/pull/8567)
* Fix `exclude_none` for json serialization of `computed_field`s by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1098](https://github.com/pydantic/pydantic-core/pull/1098)
* Support yyyy-MM-DD string for datetimes by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1124](https://github.com/pydantic/pydantic-core/pull/1124)
* Tweak ordering of definitions in generated schemas by [@StrawHatDrag0n](https://github.com/StrawHatDrag0n) in [#8583](https://github.com/pydantic/pydantic/pull/8583)


### New Contributors

#### `pydantic`
* [@ekeew](https://github.com/ekeew) made their first contribution in [#6874](https://github.com/pydantic/pydantic/pull/6874)
* [@lambertsbennett](https://github.com/lambertsbennett) made their first contribution in [#8054](https://github.com/pydantic/pydantic/pull/8054)
* [@vincent-hachin-wmx](https://github.com/vincent-hachin-wmx) made their first contribution in [#8138](https://github.com/pydantic/pydantic/pull/8138)
* [@QuentinSoubeyranAqemia](https://github.com/QuentinSoubeyranAqemia) made their first contribution in [#7825](https://github.com/pydantic/pydantic/pull/7825)
* [@ariebovenberg](https://github.com/ariebovenberg) made their first contribution in [#8072](https://github.com/pydantic/pydantic/pull/8072)
* [@LukeTonin](https://github.com/LukeTonin) made their first contribution in [#8223](https://github.com/pydantic/pydantic/pull/8223)
* [@denisart](https://github.com/denisart) made their first contribution in [#8231](https://github.com/pydantic/pydantic/pull/8231)
* [@ianhfc](https://github.com/ianhfc) made their first contribution in [#8066](https://github.com/pydantic/pydantic/pull/8066)
* [@eonu](https://github.com/eonu) made their first contribution in [#8255](https://github.com/pydantic/pydantic/pull/8255)
* [@amandahla](https://github.com/amandahla) made their first contribution in [#8263](https://github.com/pydantic/pydantic/pull/8263)
* [@ibleedicare](https://github.com/ibleedicare) made their first contribution in [#8262](https://github.com/pydantic/pydantic/pull/8262)
* [@jevins09](https://github.com/jevins09) made their first contribution in [#8316](https://github.com/pydantic/pydantic/pull/8316)
* [@cuu508](https://github.com/cuu508) made their first contribution in [#8322](https://github.com/pydantic/pydantic/pull/8322)
* [@slanzmich](https://github.com/slanzmich) made their first contribution in [#8305](https://github.com/pydantic/pydantic/pull/8305)
* [@jensenbox](https://github.com/jensenbox) made their first contribution in [#8331](https://github.com/pydantic/pydantic/pull/8331)
* [@szepeviktor](https://github.com/szepeviktor) made their first contribution in [#8356](https://github.com/pydantic/pydantic/pull/8356)
* [@Elkiwa](https://github.com/Elkiwa) made their first contribution in [#8341](https://github.com/pydantic/pydantic/pull/8341)
* [@parhamfh](https://github.com/parhamfh) made their first contribution in [#8395](https://github.com/pydantic/pydantic/pull/8395)
* [@shenxiangzhuang](https://github.com/shenxiangzhuang) made their first contribution in [#8402](https://github.com/pydantic/pydantic/pull/8402)
* [@NeevCohen](https://github.com/NeevCohen) made their first contribution in [#8387](https://github.com/pydantic/pydantic/pull/8387)
* [@zby](https://github.com/zby) made their first contribution in [#8497](https://github.com/pydantic/pydantic/pull/8497)
* [@patelnets](https://github.com/patelnets) made their first contribution in [#8491](https://github.com/pydantic/pydantic/pull/8491)
* [@edwardwli](https://github.com/edwardwli) made their first contribution in [#8503](https://github.com/pydantic/pydantic/pull/8503)
* [@luca-matei](https://github.com/luca-matei) made their first contribution in [#8507](https://github.com/pydantic/pydantic/pull/8507)
* [@Jocelyn-Gas](https://github.com/Jocelyn-Gas) made their first contribution in [#8437](https://github.com/pydantic/pydantic/pull/8437)
* [@bL34cHig0](https://github.com/bL34cHig0) made their first contribution in [#8501](https://github.com/pydantic/pydantic/pull/8501)
* [@tigeryy2](https://github.com/tigeryy2) made their first contribution in [#8511](https://github.com/pydantic/pydantic/pull/8511)
* [@geospackle](https://github.com/geospackle) made their first contribution in [#8537](https://github.com/pydantic/pydantic/pull/8537)
* [@Anvil](https://github.com/Anvil) made their first contribution in [#8567](https://github.com/pydantic/pydantic/pull/8567)
* [@hungtsetse](https://github.com/hungtsetse) made their first contribution in [#8546](https://github.com/pydantic/pydantic/pull/8546)
* [@StrawHatDrag0n](https://github.com/StrawHatDrag0n) made their first contribution in [#8583](https://github.com/pydantic/pydantic/pull/8583)

#### `pydantic-core`
* [@mariuswinger](https://github.com/mariuswinger) made their first contribution in [pydantic/pydantic-core#1087](https://github.com/pydantic/pydantic-core/pull/1087)
* [@adamchainz](https://github.com/adamchainz) made their first contribution in [pydantic/pydantic-core#1090](https://github.com/pydantic/pydantic-core/pull/1090)
* [@akx](https://github.com/akx) made their first contribution in [pydantic/pydantic-core#1123](https://github.com/pydantic/pydantic-core/pull/1123)

## v2.6.0b1 (2024-01-19)

Pre-release, see [the GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.6.0b1) for details.

## v2.5.3 (2023-12-22)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.5.3)

### What's Changed

#### Packaging

* uprev `pydantic-core` to 2.14.6

#### Fixes

* Fix memory leak with recursive definitions creating reference cycles by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#1125](https://github.com/pydantic/pydantic-core/pull/1125)

## v2.5.2 (2023-11-22)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.5.2)

### What's Changed

#### Packaging

* uprev `pydantic-core` to 2.14.5

#### New Features

* Add `ConfigDict.ser_json_inf_nan` by [@davidhewitt](https://github.com/davidhewitt) in [#8159](https://github.com/pydantic/pydantic/pull/8159)

#### Fixes

* Fix validation of `Literal` from JSON keys when used as `dict` key by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1075](https://github.com/pydantic/pydantic-core/pull/1075)
* Fix bug re `custom_init` on members of `Union` by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1076](https://github.com/pydantic/pydantic-core/pull/1076)
* Fix `JsonValue` `bool` serialization by [@sydney-runkle](https://github.com/sydney-runkle) in [#8190](https://github.com/pydantic/pydantic/pull/8159)
* Fix handling of unhashable inputs with `Literal` in `Union`s by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1089](https://github.com/pydantic/pydantic-core/pull/1089)

## v2.5.1 (2023-11-15)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.5.1)

### What's Changed

#### Packaging

* uprev pydantic-core to 2.14.3 by [@samuelcolvin](https://github.com/samuelcolvin) in [#8120](https://github.com/pydantic/pydantic/pull/8120)

#### Fixes

* Fix package description limit by [@dmontagu](https://github.com/dmontagu) in [#8097](https://github.com/pydantic/pydantic/pull/8097)
* Fix `ValidateCallWrapper` error when creating a model which has a [@validate_call](https://github.com/validate_call) wrapped field annotation by [@sydney-runkle](https://github.com/sydney-runkle) in [#8110](https://github.com/pydantic/pydantic/pull/8110)

## v2.5.0 (2023-11-13)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.5.0)

The code released in v2.5.0 is functionally identical to that of v2.5.0b1.

### What's Changed

#### Packaging

* Update pydantic-core from 2.10.1 to 2.14.1, significant changes from these updates are described below, full changelog [here](https://github.com/pydantic/pydantic-core/compare/v2.10.1...v2.14.1)
* Update to `pyright==1.1.335` by [@Viicos](https://github.com/Viicos) in [#8075](https://github.com/pydantic/pydantic/pull/8075)

#### New Features

* Allow plugins to catch non `ValidationError` errors by [@adriangb](https://github.com/adriangb) in [#7806](https://github.com/pydantic/pydantic/pull/7806)
* Support `__doc__` argument in `create_model()` by [@chris-spann](https://github.com/chris-spann) in [#7863](https://github.com/pydantic/pydantic/pull/7863)
* Expose `regex_engine` flag - meaning you can use with the Rust or Python regex libraries in constraints by [@utkini](https://github.com/utkini) in [#7768](https://github.com/pydantic/pydantic/pull/7768)
* Save return type generated from type annotation in `ComputedFieldInfo` by [@alexmojaki](https://github.com/alexmojaki) in [#7889](https://github.com/pydantic/pydantic/pull/7889)
* Adopting `ruff` formatter by [@Luca-Blight](https://github.com/Luca-Blight) in [#7930](https://github.com/pydantic/pydantic/pull/7930)
* Added `validation_error_cause` to config by [@zakstucke](https://github.com/zakstucke) in [#7626](https://github.com/pydantic/pydantic/pull/7626)
* Make path of the item to validate available in plugin by [@hramezani](https://github.com/hramezani) in [#7861](https://github.com/pydantic/pydantic/pull/7861)
* Add `CallableDiscriminator` and `Tag` by [@dmontagu](https://github.com/dmontagu) in [#7983](https://github.com/pydantic/pydantic/pull/7983)
  * `CallableDiscriminator` renamed to `Discriminator` by [@dmontagu](https://github.com/dmontagu) in [#8047](https://github.com/pydantic/pydantic/pull/8047)
* Make union case tags affect union error messages by [@dmontagu](https://github.com/dmontagu) in [#8001](https://github.com/pydantic/pydantic/pull/8001)
* Add `examples` and `json_schema_extra` to `@computed_field` by [@alexmojaki](https://github.com/alexmojaki) in [#8013](https://github.com/pydantic/pydantic/pull/8013)
* Add `JsonValue` type by [@dmontagu](https://github.com/dmontagu) in [#7998](https://github.com/pydantic/pydantic/pull/7998)
* Allow `str` as argument to `Discriminator` by [@dmontagu](https://github.com/dmontagu) in [#8047](https://github.com/pydantic/pydantic/pull/8047)
* Add `SchemaSerializer.__reduce__` method to enable pickle serialization by [@edoakes](https://github.com/edoakes) in [pydantic/pydantic-core#1006](https://github.com/pydantic/pydantic-core/pull/1006)

#### Changes

* **Significant Change:** replace `ultra_strict` with new smart union implementation, the way unions are validated has changed significantly to improve performance and correctness, we have worked hard to absolutely minimise the number of cases where behaviour has changed, see the PR for details - by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#867](https://github.com/pydantic/pydantic-core/pull/867)
* Add support for instance method reassignment when `extra='allow'` by [@sydney-runkle](https://github.com/sydney-runkle) in [#7683](https://github.com/pydantic/pydantic/pull/7683)
* Support JSON schema generation for `Enum` types with no cases by [@sydney-runkle](https://github.com/sydney-runkle) in [#7927](https://github.com/pydantic/pydantic/pull/7927)
* Warn if a class inherits from `Generic` before `BaseModel` by [@alexmojaki](https://github.com/alexmojaki) in [#7891](https://github.com/pydantic/pydantic/pull/7891)

#### Performance

* New custom JSON parser, `jiter` by [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/pydantic-core#974](https://github.com/pydantic/pydantic-core/pull/974)
* PGO build for MacOS M1 by [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/pydantic-core#1063](https://github.com/pydantic/pydantic-core/pull/1063)
* Use `__getattr__` for all package imports, improve import time by [@samuelcolvin](https://github.com/samuelcolvin) in [#7947](https://github.com/pydantic/pydantic/pull/7947)

#### Fixes

* Fix `mypy` issue with subclasses of `RootModel` by [@sydney-runkle](https://github.com/sydney-runkle) in [#7677](https://github.com/pydantic/pydantic/pull/7677)
* Properly rebuild the `FieldInfo` when a forward ref gets evaluated by [@dmontagu](https://github.com/dmontagu) in [#7698](https://github.com/pydantic/pydantic/pull/7698)
* Fix failure to load `SecretStr` from JSON (regression in v2.4) by [@sydney-runkle](https://github.com/sydney-runkle) in [#7729](https://github.com/pydantic/pydantic/pull/7729)
* Fix `defer_build` behavior with `TypeAdapter` by [@sydney-runkle](https://github.com/sydney-runkle) in [#7736](https://github.com/pydantic/pydantic/pull/7736)
* Improve compatibility with legacy `mypy` versions by [@dmontagu](https://github.com/dmontagu) in [#7742](https://github.com/pydantic/pydantic/pull/7742)
* Fix: update `TypeVar` handling when default is not set by [@pmmmwh](https://github.com/pmmmwh) in [#7719](https://github.com/pydantic/pydantic/pull/7719)
* Support specification of `strict` on `Enum` type fields by [@sydney-runkle](https://github.com/sydney-runkle) in [#7761](https://github.com/pydantic/pydantic/pull/7761)
* Wrap `weakref.ref` instead of subclassing to fix `cloudpickle` serialization by [@edoakes](https://github.com/edoakes) in [#7780](https://github.com/pydantic/pydantic/pull/7780)
* Keep values of private attributes set within `model_post_init` in subclasses by [@alexmojaki](https://github.com/alexmojaki) in [#7775](https://github.com/pydantic/pydantic/pull/7775)
* Add more specific type for non-callable `json_schema_extra` by [@alexmojaki](https://github.com/alexmojaki) in [#7803](https://github.com/pydantic/pydantic/pull/7803)
* Raise an error when deleting frozen (model) fields by [@alexmojaki](https://github.com/alexmojaki) in [#7800](https://github.com/pydantic/pydantic/pull/7800)
* Fix schema sorting bug with default values by [@sydney-runkle](https://github.com/sydney-runkle) in [#7817](https://github.com/pydantic/pydantic/pull/7817)
* Use generated alias for aliases that are not specified otherwise by [@alexmojaki](https://github.com/alexmojaki) in [#7802](https://github.com/pydantic/pydantic/pull/7802)
* Support `strict` specification for `UUID` types by [@sydney-runkle](https://github.com/sydney-runkle) in [#7865](https://github.com/pydantic/pydantic/pull/7865)
* JSON schema: fix extra parameter handling by [@me-and](https://github.com/me-and) in [#7810](https://github.com/pydantic/pydantic/pull/7810)
* Fix: support `pydantic.Field(kw_only=True)` with inherited dataclasses by [@PrettyWood](https://github.com/PrettyWood) in [#7827](https://github.com/pydantic/pydantic/pull/7827)
* Support `validate_call` decorator for methods in classes with `__slots__` by [@sydney-runkle](https://github.com/sydney-runkle) in [#7883](https://github.com/pydantic/pydantic/pull/7883)
* Fix pydantic dataclass problem with `dataclasses.field` default by [@hramezani](https://github.com/hramezani) in [#7898](https://github.com/pydantic/pydantic/pull/7898)
* Fix schema generation for generics with union type bounds by [@sydney-runkle](https://github.com/sydney-runkle) in [#7899](https://github.com/pydantic/pydantic/pull/7899)
* Fix version for `importlib_metadata` on python 3.7 by [@sydney-runkle](https://github.com/sydney-runkle) in [#7904](https://github.com/pydantic/pydantic/pull/7904)
* Support `|` operator (Union) in PydanticRecursiveRef by [@alexmojaki](https://github.com/alexmojaki) in [#7892](https://github.com/pydantic/pydantic/pull/7892)
* Fix `display_as_type` for `TypeAliasType` in python 3.12 by [@dmontagu](https://github.com/dmontagu) in [#7929](https://github.com/pydantic/pydantic/pull/7929)
* Add support for `NotRequired` generics in `TypedDict` by [@sydney-runkle](https://github.com/sydney-runkle) in [#7932](https://github.com/pydantic/pydantic/pull/7932)
* Make generic `TypeAliasType` specifications produce different schema definitions by [@alexdrydew](https://github.com/alexdrydew) in [#7893](https://github.com/pydantic/pydantic/pull/7893)
* Added fix for signature of inherited dataclass by [@howsunjow](https://github.com/howsunjow) in [#7925](https://github.com/pydantic/pydantic/pull/7925)
* Make the model name generation more robust in JSON schema by [@joakimnordling](https://github.com/joakimnordling) in [#7881](https://github.com/pydantic/pydantic/pull/7881)
* Fix plurals in validation error messages (in tests) by [@Iipin](https://github.com/Iipin) in [#7972](https://github.com/pydantic/pydantic/pull/7972)
* `PrivateAttr` is passed from `Annotated` default position by [@tabassco](https://github.com/tabassco) in [#8004](https://github.com/pydantic/pydantic/pull/8004)
* Don't decode bytes (which may not be UTF8) when displaying SecretBytes by [@alexmojaki](https://github.com/alexmojaki) in [#8012](https://github.com/pydantic/pydantic/pull/8012)
* Use `classmethod` instead of `classmethod[Any, Any, Any]` by [@Mr-Pepe](https://github.com/Mr-Pepe) in [#7979](https://github.com/pydantic/pydantic/pull/7979)
* Clearer error on invalid Plugin by [@samuelcolvin](https://github.com/samuelcolvin) in [#8023](https://github.com/pydantic/pydantic/pull/8023)
* Correct pydantic dataclasses import by [@samuelcolvin](https://github.com/samuelcolvin) in [#8027](https://github.com/pydantic/pydantic/pull/8027)
* Fix misbehavior for models referencing redefined type aliases by [@dmontagu](https://github.com/dmontagu) in [#8050](https://github.com/pydantic/pydantic/pull/8050)
* Fix `Optional` field with `validate_default` only performing one field validation by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1002](https://github.com/pydantic/pydantic-core/pull/1002)
* Fix `definition-ref` bug with `Dict` keys by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1014](https://github.com/pydantic/pydantic-core/pull/1014)
* Fix bug allowing validation of `bool` types with `coerce_numbers_to_str=True` by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1017](https://github.com/pydantic/pydantic-core/pull/1017)
* Don't accept `NaN` in float and decimal constraints by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#1037](https://github.com/pydantic/pydantic-core/pull/1037)
* Add `lax_str` and `lax_int` support for enum values not inherited from str/int by [@michaelhly](https://github.com/michaelhly) in [pydantic/pydantic-core#1015](https://github.com/pydantic/pydantic-core/pull/1015)
* Support subclasses in lists in `Union` of `List` types by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1039](https://github.com/pydantic/pydantic-core/pull/1039)
* Allow validation against `max_digits` and `decimals` to pass if normalized or non-normalized input is valid by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1049](https://github.com/pydantic/pydantic-core/pull/1049)
* Fix: proper pluralization in `ValidationError` messages by [@Iipin](https://github.com/Iipin) in [pydantic/pydantic-core#1050](https://github.com/pydantic/pydantic-core/pull/1050)
* Disallow the string `'-'` as `datetime` input by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/speedate#52](https://github.com/pydantic/speedate/pull/52) & [pydantic/pydantic-core#1060](https://github.com/pydantic/pydantic-core/pull/1060)
* Fix: NaN and Inf float serialization by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#1062](https://github.com/pydantic/pydantic-core/pull/1062)
* Restore manylinux-compatible PGO builds by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#1068](https://github.com/pydantic/pydantic-core/pull/1068)

### New Contributors

#### `pydantic`
* [@schneebuzz](https://github.com/schneebuzz) made their first contribution in [#7699](https://github.com/pydantic/pydantic/pull/7699)
* [@edoakes](https://github.com/edoakes) made their first contribution in [#7780](https://github.com/pydantic/pydantic/pull/7780)
* [@alexmojaki](https://github.com/alexmojaki) made their first contribution in [#7775](https://github.com/pydantic/pydantic/pull/7775)
* [@NickG123](https://github.com/NickG123) made their first contribution in [#7751](https://github.com/pydantic/pydantic/pull/7751)
* [@gowthamgts](https://github.com/gowthamgts) made their first contribution in [#7830](https://github.com/pydantic/pydantic/pull/7830)
* [@jamesbraza](https://github.com/jamesbraza) made their first contribution in [#7848](https://github.com/pydantic/pydantic/pull/7848)
* [@laundmo](https://github.com/laundmo) made their first contribution in [#7850](https://github.com/pydantic/pydantic/pull/7850)
* [@rahmatnazali](https://github.com/rahmatnazali) made their first contribution in [#7870](https://github.com/pydantic/pydantic/pull/7870)
* [@waterfountain1996](https://github.com/waterfountain1996) made their first contribution in [#7878](https://github.com/pydantic/pydantic/pull/7878)
* [@chris-spann](https://github.com/chris-spann) made their first contribution in [#7863](https://github.com/pydantic/pydantic/pull/7863)
* [@me-and](https://github.com/me-and) made their first contribution in [#7810](https://github.com/pydantic/pydantic/pull/7810)
* [@utkini](https://github.com/utkini) made their first contribution in [#7768](https://github.com/pydantic/pydantic/pull/7768)
* [@bn-l](https://github.com/bn-l) made their first contribution in [#7744](https://github.com/pydantic/pydantic/pull/7744)
* [@alexdrydew](https://github.com/alexdrydew) made their first contribution in [#7893](https://github.com/pydantic/pydantic/pull/7893)
* [@Luca-Blight](https://github.com/Luca-Blight) made their first contribution in [#7930](https://github.com/pydantic/pydantic/pull/7930)
* [@howsunjow](https://github.com/howsunjow) made their first contribution in [#7925](https://github.com/pydantic/pydantic/pull/7925)
* [@joakimnordling](https://github.com/joakimnordling) made their first contribution in [#7881](https://github.com/pydantic/pydantic/pull/7881)
* [@icfly2](https://github.com/icfly2) made their first contribution in [#7976](https://github.com/pydantic/pydantic/pull/7976)
* [@Yummy-Yums](https://github.com/Yummy-Yums) made their first contribution in [#8003](https://github.com/pydantic/pydantic/pull/8003)
* [@Iipin](https://github.com/Iipin) made their first contribution in [#7972](https://github.com/pydantic/pydantic/pull/7972)
* [@tabassco](https://github.com/tabassco) made their first contribution in [#8004](https://github.com/pydantic/pydantic/pull/8004)
* [@Mr-Pepe](https://github.com/Mr-Pepe) made their first contribution in [#7979](https://github.com/pydantic/pydantic/pull/7979)
* [@0x00cl](https://github.com/0x00cl) made their first contribution in [#8010](https://github.com/pydantic/pydantic/pull/8010)
* [@barraponto](https://github.com/barraponto) made their first contribution in [#8032](https://github.com/pydantic/pydantic/pull/8032)

#### `pydantic-core`
* [@sisp](https://github.com/sisp) made their first contribution in [pydantic/pydantic-core#995](https://github.com/pydantic/pydantic-core/pull/995)
* [@michaelhly](https://github.com/michaelhly) made their first contribution in [pydantic/pydantic-core#1015](https://github.com/pydantic/pydantic-core/pull/1015)

## v2.5.0b1 (2023-11-09)

Pre-release, see [the GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.5.0b1) for details.

## v2.4.2 (2023-09-27)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.4.2)

### What's Changed

#### Fixes

* Fix bug with JSON schema for sequence of discriminated union by [@dmontagu](https://github.com/dmontagu) in [#7647](https://github.com/pydantic/pydantic/pull/7647)
* Fix schema references in discriminated unions by [@adriangb](https://github.com/adriangb) in [#7646](https://github.com/pydantic/pydantic/pull/7646)
* Fix json schema generation for recursive models by [@adriangb](https://github.com/adriangb) in [#7653](https://github.com/pydantic/pydantic/pull/7653)
* Fix `models_json_schema` for generic models by [@adriangb](https://github.com/adriangb) in [#7654](https://github.com/pydantic/pydantic/pull/7654)
* Fix xfailed test for generic model signatures by [@adriangb](https://github.com/adriangb) in [#7658](https://github.com/pydantic/pydantic/pull/7658)

### New Contributors

* [@austinorr](https://github.com/austinorr) made their first contribution in [#7657](https://github.com/pydantic/pydantic/pull/7657)
* [@peterHoburg](https://github.com/peterHoburg) made their first contribution in [#7670](https://github.com/pydantic/pydantic/pull/7670)

## v2.4.1 (2023-09-26)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.4.1)

### What's Changed

#### Packaging

* Update pydantic-core to 2.10.1 by [@davidhewitt](https://github.com/davidhewitt) in [#7633](https://github.com/pydantic/pydantic/pull/7633)

#### Fixes

* Serialize unsubstituted type vars as `Any` by [@adriangb](https://github.com/adriangb) in [#7606](https://github.com/pydantic/pydantic/pull/7606)
* Remove schema building caches by [@adriangb](https://github.com/adriangb) in [#7624](https://github.com/pydantic/pydantic/pull/7624)
* Fix an issue where JSON schema extras weren't JSON encoded by [@dmontagu](https://github.com/dmontagu) in [#7625](https://github.com/pydantic/pydantic/pull/7625)

## v2.4.0 (2023-09-22)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.4.0)

### What's Changed

#### Packaging

* Update pydantic-core to 2.10.0 by [@samuelcolvin](https://github.com/samuelcolvin) in [#7542](https://github.com/pydantic/pydantic/pull/7542)

#### New Features

* Add `Base64Url` types by [@dmontagu](https://github.com/dmontagu) in [#7286](https://github.com/pydantic/pydantic/pull/7286)
* Implement optional `number` to `str` coercion by [@lig](https://github.com/lig) in [#7508](https://github.com/pydantic/pydantic/pull/7508)
* Allow access to `field_name` and `data` in all validators if there is data and a field name by [@samuelcolvin](https://github.com/samuelcolvin) in [#7542](https://github.com/pydantic/pydantic/pull/7542)
* Add `BaseModel.model_validate_strings` and `TypeAdapter.validate_strings` by [@hramezani](https://github.com/hramezani) in [#7552](https://github.com/pydantic/pydantic/pull/7552)
* Add Pydantic `plugins` experimental implementation by [@lig](https://github.com/lig) [@samuelcolvin](https://github.com/samuelcolvin) and [@Kludex](https://github.com/Kludex) in [#6820](https://github.com/pydantic/pydantic/pull/6820)

#### Changes

* Do not override `model_post_init` in subclass with private attrs by [@Viicos](https://github.com/Viicos) in [#7302](https://github.com/pydantic/pydantic/pull/7302)
* Make fields with defaults not required in the serialization schema by default by [@dmontagu](https://github.com/dmontagu) in [#7275](https://github.com/pydantic/pydantic/pull/7275)
* Mark `Extra` as deprecated by [@disrupted](https://github.com/disrupted) in [#7299](https://github.com/pydantic/pydantic/pull/7299)
* Make `EncodedStr` a dataclass by [@Kludex](https://github.com/Kludex) in [#7396](https://github.com/pydantic/pydantic/pull/7396)
* Move `annotated_handlers` to be public by [@samuelcolvin](https://github.com/samuelcolvin) in [#7569](https://github.com/pydantic/pydantic/pull/7569)

#### Performance

* Simplify flattening and inlining of `CoreSchema` by [@adriangb](https://github.com/adriangb) in [#7523](https://github.com/pydantic/pydantic/pull/7523)
* Remove unused copies in `CoreSchema` walking by [@adriangb](https://github.com/adriangb) in [#7528](https://github.com/pydantic/pydantic/pull/7528)
* Add caches for collecting definitions and invalid schemas from a CoreSchema by [@adriangb](https://github.com/adriangb) in [#7527](https://github.com/pydantic/pydantic/pull/7527)
* Eagerly resolve discriminated unions and cache cases where we can't by [@adriangb](https://github.com/adriangb) in [#7529](https://github.com/pydantic/pydantic/pull/7529)
* Replace `dict.get` and `dict.setdefault` with more verbose versions in `CoreSchema` building hot paths by [@adriangb](https://github.com/adriangb) in [#7536](https://github.com/pydantic/pydantic/pull/7536)
* Cache invalid `CoreSchema` discovery by [@adriangb](https://github.com/adriangb) in [#7535](https://github.com/pydantic/pydantic/pull/7535)
* Allow disabling `CoreSchema` validation for faster startup times by [@adriangb](https://github.com/adriangb) in [#7565](https://github.com/pydantic/pydantic/pull/7565)

#### Fixes

* Fix config detection for `TypedDict` from grandparent classes by [@dmontagu](https://github.com/dmontagu) in [#7272](https://github.com/pydantic/pydantic/pull/7272)
* Fix hash function generation for frozen models with unusual MRO by [@dmontagu](https://github.com/dmontagu) in [#7274](https://github.com/pydantic/pydantic/pull/7274)
* Make `strict` config overridable in field for Path by [@hramezani](https://github.com/hramezani) in [#7281](https://github.com/pydantic/pydantic/pull/7281)
* Use `ser_json_<timedelta|bytes>` on default in `GenerateJsonSchema` by [@Kludex](https://github.com/Kludex) in [#7269](https://github.com/pydantic/pydantic/pull/7269)
* Adding a check that alias is validated as an identifier for Python by [@andree0](https://github.com/andree0) in [#7319](https://github.com/pydantic/pydantic/pull/7319)
* Raise an error when computed field overrides field by [@sydney-runkle](https://github.com/sydney-runkle) in [#7346](https://github.com/pydantic/pydantic/pull/7346)
* Fix applying `SkipValidation` to referenced schemas by [@adriangb](https://github.com/adriangb) in [#7381](https://github.com/pydantic/pydantic/pull/7381)
* Enforce behavior of private attributes having double leading underscore by [@lig](https://github.com/lig) in [#7265](https://github.com/pydantic/pydantic/pull/7265)
* Standardize `__get_pydantic_core_schema__` signature by [@hramezani](https://github.com/hramezani) in [#7415](https://github.com/pydantic/pydantic/pull/7415)
* Fix generic dataclass fields mutation bug (when using `TypeAdapter`) by [@sydney-runkle](https://github.com/sydney-runkle) in [#7435](https://github.com/pydantic/pydantic/pull/7435)
* Fix `TypeError` on `model_validator` in `wrap` mode by [@pmmmwh](https://github.com/pmmmwh) in [#7496](https://github.com/pydantic/pydantic/pull/7496)
* Improve enum error message by [@hramezani](https://github.com/hramezani) in [#7506](https://github.com/pydantic/pydantic/pull/7506)
* Make `repr` work for instances that failed initialization when handling `ValidationError`s by [@dmontagu](https://github.com/dmontagu) in [#7439](https://github.com/pydantic/pydantic/pull/7439)
* Fixed a regular expression denial of service issue by limiting whitespaces by [@prodigysml](https://github.com/prodigysml) in [#7360](https://github.com/pydantic/pydantic/pull/7360)
* Fix handling of `UUID` values having `UUID.version=None` by [@lig](https://github.com/lig) in [#7566](https://github.com/pydantic/pydantic/pull/7566)
* Fix `__iter__` returning private `cached_property` info by [@sydney-runkle](https://github.com/sydney-runkle) in [#7570](https://github.com/pydantic/pydantic/pull/7570)
* Improvements to version info message by [@samuelcolvin](https://github.com/samuelcolvin) in [#7594](https://github.com/pydantic/pydantic/pull/7594)

### New Contributors
* [@15498th](https://github.com/15498th) made their first contribution in [#7238](https://github.com/pydantic/pydantic/pull/7238)
* [@GabrielCappelli](https://github.com/GabrielCappelli) made their first contribution in [#7213](https://github.com/pydantic/pydantic/pull/7213)
* [@tobni](https://github.com/tobni) made their first contribution in [#7184](https://github.com/pydantic/pydantic/pull/7184)
* [@redruin1](https://github.com/redruin1) made their first contribution in [#7282](https://github.com/pydantic/pydantic/pull/7282)
* [@FacerAin](https://github.com/FacerAin) made their first contribution in [#7288](https://github.com/pydantic/pydantic/pull/7288)
* [@acdha](https://github.com/acdha) made their first contribution in [#7297](https://github.com/pydantic/pydantic/pull/7297)
* [@andree0](https://github.com/andree0) made their first contribution in [#7319](https://github.com/pydantic/pydantic/pull/7319)
* [@gordonhart](https://github.com/gordonhart) made their first contribution in [#7375](https://github.com/pydantic/pydantic/pull/7375)
* [@pmmmwh](https://github.com/pmmmwh) made their first contribution in [#7496](https://github.com/pydantic/pydantic/pull/7496)
* [@disrupted](https://github.com/disrupted) made their first contribution in [#7299](https://github.com/pydantic/pydantic/pull/7299)
* [@prodigysml](https://github.com/prodigysml) made their first contribution in [#7360](https://github.com/pydantic/pydantic/pull/7360)

## v2.3.0 (2023-08-23)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.3.0)

* ðŸ”¥ Remove orphaned changes file from repo by [@lig](https://github.com/lig) in [#7168](https://github.com/pydantic/pydantic/pull/7168)
* Add copy button on documentation by [@Kludex](https://github.com/Kludex) in [#7190](https://github.com/pydantic/pydantic/pull/7190)
* Fix docs on JSON type by [@Kludex](https://github.com/Kludex) in [#7189](https://github.com/pydantic/pydantic/pull/7189)
* Update mypy 1.5.0 to 1.5.1 in CI by [@hramezani](https://github.com/hramezani) in [#7191](https://github.com/pydantic/pydantic/pull/7191)
* fix download links badge by [@samuelcolvin](https://github.com/samuelcolvin) in [#7200](https://github.com/pydantic/pydantic/pull/7200)
* add 2.2.1 to changelog by [@samuelcolvin](https://github.com/samuelcolvin) in [#7212](https://github.com/pydantic/pydantic/pull/7212)
* Make ModelWrapValidator protocols generic by [@dmontagu](https://github.com/dmontagu) in [#7154](https://github.com/pydantic/pydantic/pull/7154)
* Correct `Field(..., exclude: bool)` docs by [@samuelcolvin](https://github.com/samuelcolvin) in [#7214](https://github.com/pydantic/pydantic/pull/7214)
* Make shadowing attributes a warning instead of an error by [@adriangb](https://github.com/adriangb) in [#7193](https://github.com/pydantic/pydantic/pull/7193)
* Document `Base64Str` and `Base64Bytes` by [@Kludex](https://github.com/Kludex) in [#7192](https://github.com/pydantic/pydantic/pull/7192)
* Fix `config.defer_build` for serialization first cases by [@samuelcolvin](https://github.com/samuelcolvin) in [#7024](https://github.com/pydantic/pydantic/pull/7024)
* clean Model docstrings in JSON Schema by [@samuelcolvin](https://github.com/samuelcolvin) in [#7210](https://github.com/pydantic/pydantic/pull/7210)
* fix [#7228](https://github.com/pydantic/pydantic/pull/7228) (typo): docs in `validators.md` to correct `validate_default` kwarg by [@lmmx](https://github.com/lmmx) in [#7229](https://github.com/pydantic/pydantic/pull/7229)
* âœ… Implement `tzinfo.fromutc` method for `TzInfo` in `pydantic-core` by [@lig](https://github.com/lig) in [#7019](https://github.com/pydantic/pydantic/pull/7019)
* Support `__get_validators__` by [@hramezani](https://github.com/hramezani) in [#7197](https://github.com/pydantic/pydantic/pull/7197)

## v2.2.1 (2023-08-18)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.2.1)

* Make `xfail`ing test for root model extra stop `xfail`ing by [@dmontagu](https://github.com/dmontagu) in [#6937](https://github.com/pydantic/pydantic/pull/6937)
* Optimize recursion detection by stopping on the second visit for the same object by [@mciucu](https://github.com/mciucu) in [#7160](https://github.com/pydantic/pydantic/pull/7160)
* fix link in docs by [@tlambert03](https://github.com/tlambert03) in [#7166](https://github.com/pydantic/pydantic/pull/7166)
* Replace MiMalloc w/ default allocator by [@adriangb](https://github.com/adriangb) in [pydantic/pydantic-core#900](https://github.com/pydantic/pydantic-core/pull/900)
* Bump pydantic-core to 2.6.1 and prepare 2.2.1 release by [@adriangb](https://github.com/adriangb) in [#7176](https://github.com/pydantic/pydantic/pull/7176)

## v2.2.0 (2023-08-17)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.2.0)

* Split "pipx install" setup command into two commands on the documentation site by [@nomadmtb](https://github.com/nomadmtb) in [#6869](https://github.com/pydantic/pydantic/pull/6869)
* Deprecate `Field.include` by [@hramezani](https://github.com/hramezani) in [#6852](https://github.com/pydantic/pydantic/pull/6852)
* Fix typo in default factory error msg by [@hramezani](https://github.com/hramezani) in [#6880](https://github.com/pydantic/pydantic/pull/6880)
* Simplify handling of typing.Annotated in GenerateSchema by [@dmontagu](https://github.com/dmontagu) in [#6887](https://github.com/pydantic/pydantic/pull/6887)
* Re-enable fastapi tests in CI by [@dmontagu](https://github.com/dmontagu) in [#6883](https://github.com/pydantic/pydantic/pull/6883)
* Make it harder to hit collisions with json schema defrefs by [@dmontagu](https://github.com/dmontagu) in [#6566](https://github.com/pydantic/pydantic/pull/6566)
* Cleaner error for invalid input to `Path` fields by [@samuelcolvin](https://github.com/samuelcolvin) in [#6903](https://github.com/pydantic/pydantic/pull/6903)
* :memo: support Coordinate Type by [@yezz123](https://github.com/yezz123) in [#6906](https://github.com/pydantic/pydantic/pull/6906)
* Fix `ForwardRef` wrapper for py 3.10.0 (shim until bpo-45166) by [@randomir](https://github.com/randomir) in [#6919](https://github.com/pydantic/pydantic/pull/6919)
* Fix misbehavior related to copying of RootModel by [@dmontagu](https://github.com/dmontagu) in [#6918](https://github.com/pydantic/pydantic/pull/6918)
* Fix issue with recursion error caused by ParamSpec by [@dmontagu](https://github.com/dmontagu) in [#6923](https://github.com/pydantic/pydantic/pull/6923)
* Add section about Constrained classes to the Migration Guide by [@Kludex](https://github.com/Kludex) in [#6924](https://github.com/pydantic/pydantic/pull/6924)
* Use `main` branch for badge links by [@Viicos](https://github.com/Viicos) in [#6925](https://github.com/pydantic/pydantic/pull/6925)
* Add test for v1/v2 Annotated discrepancy by [@carlbordum](https://github.com/carlbordum) in [#6926](https://github.com/pydantic/pydantic/pull/6926)
* Make the v1 mypy plugin work with both v1 and v2 by [@dmontagu](https://github.com/dmontagu) in [#6921](https://github.com/pydantic/pydantic/pull/6921)
* Fix issue where generic models couldn't be parametrized with BaseModel by [@dmontagu](https://github.com/dmontagu) in [#6933](https://github.com/pydantic/pydantic/pull/6933)
* Remove xfail for discriminated union with alias by [@dmontagu](https://github.com/dmontagu) in [#6938](https://github.com/pydantic/pydantic/pull/6938)
* add field_serializer to computed_field by [@andresliszt](https://github.com/andresliszt) in [#6965](https://github.com/pydantic/pydantic/pull/6965)
* Use union_schema with Type[Union[...]] by [@JeanArhancet](https://github.com/JeanArhancet) in [#6952](https://github.com/pydantic/pydantic/pull/6952)
* Fix inherited typeddict attributes / config by [@adriangb](https://github.com/adriangb) in [#6981](https://github.com/pydantic/pydantic/pull/6981)
* fix dataclass annotated before validator called twice by [@davidhewitt](https://github.com/davidhewitt) in [#6998](https://github.com/pydantic/pydantic/pull/6998)
* Update test-fastapi deselected tests by [@hramezani](https://github.com/hramezani) in [#7014](https://github.com/pydantic/pydantic/pull/7014)
* Fix validator doc format by [@hramezani](https://github.com/hramezani) in [#7015](https://github.com/pydantic/pydantic/pull/7015)
* Fix typo in docstring of model_json_schema by [@AdamVinch-Federated](https://github.com/AdamVinch-Federated) in [#7032](https://github.com/pydantic/pydantic/pull/7032)
* remove unused "type ignores" with pyright by [@samuelcolvin](https://github.com/samuelcolvin) in [#7026](https://github.com/pydantic/pydantic/pull/7026)
* Add benchmark representing FastAPI startup time by [@adriangb](https://github.com/adriangb) in [#7030](https://github.com/pydantic/pydantic/pull/7030)
* Fix json_encoders for Enum subclasses by [@adriangb](https://github.com/adriangb) in [#7029](https://github.com/pydantic/pydantic/pull/7029)
* Update docstring of `ser_json_bytes` regarding base64 encoding by [@Viicos](https://github.com/Viicos) in [#7052](https://github.com/pydantic/pydantic/pull/7052)
* Allow `@validate_call` to work on async methods by [@adriangb](https://github.com/adriangb) in [#7046](https://github.com/pydantic/pydantic/pull/7046)
* Fix: mypy error with `Settings` and `SettingsConfigDict` by [@JeanArhancet](https://github.com/JeanArhancet) in [#7002](https://github.com/pydantic/pydantic/pull/7002)
* Fix some typos (repeated words and it's/its) by [@eumiro](https://github.com/eumiro) in [#7063](https://github.com/pydantic/pydantic/pull/7063)
* Fix the typo in docstring by [@harunyasar](https://github.com/harunyasar) in [#7062](https://github.com/pydantic/pydantic/pull/7062)
* Docs: Fix broken URL in the pydantic-settings package recommendation by [@swetjen](https://github.com/swetjen) in [#6995](https://github.com/pydantic/pydantic/pull/6995)
* Handle constraints being applied to schemas that don't accept it by [@adriangb](https://github.com/adriangb) in [#6951](https://github.com/pydantic/pydantic/pull/6951)
* Replace almost_equal_floats with math.isclose by [@eumiro](https://github.com/eumiro) in [#7082](https://github.com/pydantic/pydantic/pull/7082)
* bump pydantic-core to 2.5.0 by [@davidhewitt](https://github.com/davidhewitt) in [#7077](https://github.com/pydantic/pydantic/pull/7077)
* Add `short_version` and use it in links by [@hramezani](https://github.com/hramezani) in [#7115](https://github.com/pydantic/pydantic/pull/7115)
* ðŸ“ Add usage link to `RootModel` by [@Kludex](https://github.com/Kludex) in [#7113](https://github.com/pydantic/pydantic/pull/7113)
* Revert "Fix default port for mongosrv DSNs (#6827)" by [@Kludex](https://github.com/Kludex) in [#7116](https://github.com/pydantic/pydantic/pull/7116)
* Clarify validate_default and _Unset handling in usage docs and migration guide by [@benbenbang](https://github.com/benbenbang) in [#6950](https://github.com/pydantic/pydantic/pull/6950)
* Tweak documentation of `Field.exclude` by [@Viicos](https://github.com/Viicos) in [#7086](https://github.com/pydantic/pydantic/pull/7086)
* Do not require `validate_assignment` to use `Field.frozen` by [@Viicos](https://github.com/Viicos) in [#7103](https://github.com/pydantic/pydantic/pull/7103)
* tweaks to `_core_utils` by [@samuelcolvin](https://github.com/samuelcolvin) in [#7040](https://github.com/pydantic/pydantic/pull/7040)
* Make DefaultDict working with set by [@hramezani](https://github.com/hramezani) in [#7126](https://github.com/pydantic/pydantic/pull/7126)
* Don't always require typing.Generic as a base for partially parametrized models by [@dmontagu](https://github.com/dmontagu) in [#7119](https://github.com/pydantic/pydantic/pull/7119)
* Fix issue with JSON schema incorrectly using parent class core schema by [@dmontagu](https://github.com/dmontagu) in [#7020](https://github.com/pydantic/pydantic/pull/7020)
* Fix xfailed test related to TypedDict and alias_generator by [@dmontagu](https://github.com/dmontagu) in [#6940](https://github.com/pydantic/pydantic/pull/6940)
* Improve error message for NameEmail by [@dmontagu](https://github.com/dmontagu) in [#6939](https://github.com/pydantic/pydantic/pull/6939)
* Fix generic computed fields by [@dmontagu](https://github.com/dmontagu) in [#6988](https://github.com/pydantic/pydantic/pull/6988)
* Reflect namedtuple default values during validation by [@dmontagu](https://github.com/dmontagu) in [#7144](https://github.com/pydantic/pydantic/pull/7144)
* Update dependencies, fix pydantic-core usage, fix CI issues by [@dmontagu](https://github.com/dmontagu) in [#7150](https://github.com/pydantic/pydantic/pull/7150)
* Add mypy 1.5.0 by [@hramezani](https://github.com/hramezani) in [#7118](https://github.com/pydantic/pydantic/pull/7118)
* Handle non-json native enum values by [@adriangb](https://github.com/adriangb) in [#7056](https://github.com/pydantic/pydantic/pull/7056)
* document `round_trip` in Json type documentation  by [@jc-louis](https://github.com/jc-louis) in [#7137](https://github.com/pydantic/pydantic/pull/7137)
* Relax signature checks to better support builtins and C extension functions as validators by [@adriangb](https://github.com/adriangb) in [#7101](https://github.com/pydantic/pydantic/pull/7101)
* add union_mode='left_to_right' by [@davidhewitt](https://github.com/davidhewitt) in [#7151](https://github.com/pydantic/pydantic/pull/7151)
* Include an error message hint for inherited ordering by [@yvalencia91](https://github.com/yvalencia91) in [#7124](https://github.com/pydantic/pydantic/pull/7124)
* Fix one docs link and resolve some warnings for two others by [@dmontagu](https://github.com/dmontagu) in [#7153](https://github.com/pydantic/pydantic/pull/7153)
* Include Field extra keys name in warning by [@hramezani](https://github.com/hramezani) in [#7136](https://github.com/pydantic/pydantic/pull/7136)

## v2.1.1 (2023-07-25)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.1.1)

* Skip FieldInfo merging when unnecessary by [@dmontagu](https://github.com/dmontagu) in [#6862](https://github.com/pydantic/pydantic/pull/6862)

## v2.1.0 (2023-07-25)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.1.0)

* Add `StringConstraints` for use as Annotated metadata by [@adriangb](https://github.com/adriangb) in [#6605](https://github.com/pydantic/pydantic/pull/6605)
* Try to fix intermittently failing CI by [@adriangb](https://github.com/adriangb) in [#6683](https://github.com/pydantic/pydantic/pull/6683)
* Remove redundant example of optional vs default. by [@ehiggs-deliverect](https://github.com/ehiggs-deliverect) in [#6676](https://github.com/pydantic/pydantic/pull/6676)
* Docs update by [@samuelcolvin](https://github.com/samuelcolvin) in [#6692](https://github.com/pydantic/pydantic/pull/6692)
* Remove the Validate always section in validator docs by [@adriangb](https://github.com/adriangb) in [#6679](https://github.com/pydantic/pydantic/pull/6679)
* Fix recursion error in json schema generation by [@adriangb](https://github.com/adriangb) in [#6720](https://github.com/pydantic/pydantic/pull/6720)
* Fix incorrect subclass check for secretstr by [@AlexVndnblcke](https://github.com/AlexVndnblcke) in [#6730](https://github.com/pydantic/pydantic/pull/6730)
* update pdm / pdm lockfile to 2.8.0 by [@davidhewitt](https://github.com/davidhewitt) in [#6714](https://github.com/pydantic/pydantic/pull/6714)
* unpin pdm on more CI jobs by [@davidhewitt](https://github.com/davidhewitt) in [#6755](https://github.com/pydantic/pydantic/pull/6755)
* improve source locations for auxiliary packages in docs by [@davidhewitt](https://github.com/davidhewitt) in [#6749](https://github.com/pydantic/pydantic/pull/6749)
* Assume builtins don't accept an info argument by [@adriangb](https://github.com/adriangb) in [#6754](https://github.com/pydantic/pydantic/pull/6754)
* Fix bug where calling `help(BaseModelSubclass)` raises errors by [@hramezani](https://github.com/hramezani) in [#6758](https://github.com/pydantic/pydantic/pull/6758)
* Fix mypy plugin handling of `@model_validator(mode="after")` by [@ljodal](https://github.com/ljodal) in [#6753](https://github.com/pydantic/pydantic/pull/6753)
* update pydantic-core to 2.3.1 by [@davidhewitt](https://github.com/davidhewitt) in [#6756](https://github.com/pydantic/pydantic/pull/6756)
* Mypy plugin for settings by [@hramezani](https://github.com/hramezani) in [#6760](https://github.com/pydantic/pydantic/pull/6760)
* Use `contentSchema` keyword for JSON schema by [@dmontagu](https://github.com/dmontagu) in [#6715](https://github.com/pydantic/pydantic/pull/6715)
* fast-path checking finite decimals by [@davidhewitt](https://github.com/davidhewitt) in [#6769](https://github.com/pydantic/pydantic/pull/6769)
* Docs update by [@samuelcolvin](https://github.com/samuelcolvin) in [#6771](https://github.com/pydantic/pydantic/pull/6771)
* Improve json schema doc by [@hramezani](https://github.com/hramezani) in [#6772](https://github.com/pydantic/pydantic/pull/6772)
* Update validator docs by [@adriangb](https://github.com/adriangb) in [#6695](https://github.com/pydantic/pydantic/pull/6695)
* Fix typehint for wrap validator by [@dmontagu](https://github.com/dmontagu) in [#6788](https://github.com/pydantic/pydantic/pull/6788)
* ðŸ› Fix validation warning for unions of Literal and other type by [@lig](https://github.com/lig) in [#6628](https://github.com/pydantic/pydantic/pull/6628)
* Update documentation for generics support in V2 by [@tpdorsey](https://github.com/tpdorsey) in [#6685](https://github.com/pydantic/pydantic/pull/6685)
* add pydantic-core build info to `version_info()` by [@samuelcolvin](https://github.com/samuelcolvin) in [#6785](https://github.com/pydantic/pydantic/pull/6785)
* Fix pydantic dataclasses that use slots with default values by [@dmontagu](https://github.com/dmontagu) in [#6796](https://github.com/pydantic/pydantic/pull/6796)
* Fix inheritance of hash function for frozen models by [@dmontagu](https://github.com/dmontagu) in [#6789](https://github.com/pydantic/pydantic/pull/6789)
* âœ¨ Add `SkipJsonSchema` annotation by [@Kludex](https://github.com/Kludex) in [#6653](https://github.com/pydantic/pydantic/pull/6653)
* Error if an invalid field name is used with Field by [@dmontagu](https://github.com/dmontagu) in [#6797](https://github.com/pydantic/pydantic/pull/6797)
* Add `GenericModel` to `MOVED_IN_V2` by [@adriangb](https://github.com/adriangb) in [#6776](https://github.com/pydantic/pydantic/pull/6776)
* Remove unused code from `docs/usage/types/custom.md` by [@hramezani](https://github.com/hramezani) in [#6803](https://github.com/pydantic/pydantic/pull/6803)
* Fix `float` -> `Decimal` coercion precision loss by [@adriangb](https://github.com/adriangb) in [#6810](https://github.com/pydantic/pydantic/pull/6810)
* remove email validation from the north star benchmark by [@davidhewitt](https://github.com/davidhewitt) in [#6816](https://github.com/pydantic/pydantic/pull/6816)
* Fix link to mypy by [@progsmile](https://github.com/progsmile) in [#6824](https://github.com/pydantic/pydantic/pull/6824)
* Improve initialization hooks example by [@hramezani](https://github.com/hramezani) in [#6822](https://github.com/pydantic/pydantic/pull/6822)
* Fix default port for mongosrv DSNs by [@dmontagu](https://github.com/dmontagu) in [#6827](https://github.com/pydantic/pydantic/pull/6827)
* Improve API documentation, in particular more links between usage and API docs by [@samuelcolvin](https://github.com/samuelcolvin) in [#6780](https://github.com/pydantic/pydantic/pull/6780)
* update pydantic-core to 2.4.0 by [@davidhewitt](https://github.com/davidhewitt) in [#6831](https://github.com/pydantic/pydantic/pull/6831)
* Fix `annotated_types.MaxLen` validator for custom sequence types by [@ImogenBits](https://github.com/ImogenBits) in [#6809](https://github.com/pydantic/pydantic/pull/6809)
* Update V1 by [@hramezani](https://github.com/hramezani) in [#6833](https://github.com/pydantic/pydantic/pull/6833)
* Make it so callable JSON schema extra works by [@dmontagu](https://github.com/dmontagu) in [#6798](https://github.com/pydantic/pydantic/pull/6798)
* Fix serialization issue with `InstanceOf` by [@dmontagu](https://github.com/dmontagu) in [#6829](https://github.com/pydantic/pydantic/pull/6829)
* Add back support for `json_encoders` by [@adriangb](https://github.com/adriangb) in [#6811](https://github.com/pydantic/pydantic/pull/6811)
* Update field annotations when building the schema by [@dmontagu](https://github.com/dmontagu) in [#6838](https://github.com/pydantic/pydantic/pull/6838)
* Use `WeakValueDictionary` to fix generic memory leak by [@dmontagu](https://github.com/dmontagu) in [#6681](https://github.com/pydantic/pydantic/pull/6681)
* Add `config.defer_build` to optionally make model building lazy by [@samuelcolvin](https://github.com/samuelcolvin) in [#6823](https://github.com/pydantic/pydantic/pull/6823)
* delegate `UUID` serialization to pydantic-core by [@davidhewitt](https://github.com/davidhewitt) in [#6850](https://github.com/pydantic/pydantic/pull/6850)
* Update `json_encoders` docs by [@adriangb](https://github.com/adriangb) in [#6848](https://github.com/pydantic/pydantic/pull/6848)
* Fix error message for `staticmethod`/`classmethod` order with validate_call by [@dmontagu](https://github.com/dmontagu) in [#6686](https://github.com/pydantic/pydantic/pull/6686)
* Improve documentation for `Config` by [@samuelcolvin](https://github.com/samuelcolvin) in [#6847](https://github.com/pydantic/pydantic/pull/6847)
* Update serialization doc to mention `Field.exclude` takes priority over call-time `include/exclude` by [@hramezani](https://github.com/hramezani) in [#6851](https://github.com/pydantic/pydantic/pull/6851)
* Allow customizing core schema generation by making `GenerateSchema` public by [@adriangb](https://github.com/adriangb) in [#6737](https://github.com/pydantic/pydantic/pull/6737)

## v2.0.3 (2023-07-05)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.0.3)

* Mention PyObject (v1) moving to ImportString (v2) in migration doc by [@slafs](https://github.com/slafs) in [#6456](https://github.com/pydantic/pydantic/pull/6456)
* Fix release-tweet CI by [@Kludex](https://github.com/Kludex) in [#6461](https://github.com/pydantic/pydantic/pull/6461)
* Revise the section on required / optional / nullable fields. by [@ybressler](https://github.com/ybressler) in [#6468](https://github.com/pydantic/pydantic/pull/6468)
* Warn if a type hint is not in fact a type by [@adriangb](https://github.com/adriangb) in [#6479](https://github.com/pydantic/pydantic/pull/6479)
* Replace TransformSchema with GetPydanticSchema by [@dmontagu](https://github.com/dmontagu) in [#6484](https://github.com/pydantic/pydantic/pull/6484)
* Fix the un-hashability of various annotation types, for use in caching generic containers by [@dmontagu](https://github.com/dmontagu) in [#6480](https://github.com/pydantic/pydantic/pull/6480)
* PYD-164: Rework custom types docs by [@adriangb](https://github.com/adriangb) in [#6490](https://github.com/pydantic/pydantic/pull/6490)
* Fix ci by [@adriangb](https://github.com/adriangb) in [#6507](https://github.com/pydantic/pydantic/pull/6507)
* Fix forward ref in generic by [@adriangb](https://github.com/adriangb) in [#6511](https://github.com/pydantic/pydantic/pull/6511)
* Fix generation of serialization JSON schemas for core_schema.ChainSchema by [@dmontagu](https://github.com/dmontagu) in [#6515](https://github.com/pydantic/pydantic/pull/6515)
* Document the change in `Field.alias` behavior in Pydantic V2 by [@hramezani](https://github.com/hramezani) in [#6508](https://github.com/pydantic/pydantic/pull/6508)
* Give better error message attempting to compute the json schema of a model with undefined fields by [@dmontagu](https://github.com/dmontagu) in [#6519](https://github.com/pydantic/pydantic/pull/6519)
* Document `alias_priority` by [@tpdorsey](https://github.com/tpdorsey) in [#6520](https://github.com/pydantic/pydantic/pull/6520)
* Add redirect for types documentation by [@tpdorsey](https://github.com/tpdorsey) in [#6513](https://github.com/pydantic/pydantic/pull/6513)
* Allow updating docs without release by [@samuelcolvin](https://github.com/samuelcolvin) in [#6551](https://github.com/pydantic/pydantic/pull/6551)
* Ensure docs tests always run in the right folder by [@dmontagu](https://github.com/dmontagu) in [#6487](https://github.com/pydantic/pydantic/pull/6487)
* Defer evaluation of return type hints for serializer functions by [@dmontagu](https://github.com/dmontagu) in [#6516](https://github.com/pydantic/pydantic/pull/6516)
* Disable E501 from Ruff and rely on just Black by [@adriangb](https://github.com/adriangb) in [#6552](https://github.com/pydantic/pydantic/pull/6552)
* Update JSON Schema documentation for V2 by [@tpdorsey](https://github.com/tpdorsey) in [#6492](https://github.com/pydantic/pydantic/pull/6492)
* Add documentation of cyclic reference handling by [@dmontagu](https://github.com/dmontagu) in [#6493](https://github.com/pydantic/pydantic/pull/6493)
* Remove the need for change files by [@samuelcolvin](https://github.com/samuelcolvin) in [#6556](https://github.com/pydantic/pydantic/pull/6556)
* add "north star" benchmark by [@davidhewitt](https://github.com/davidhewitt) in [#6547](https://github.com/pydantic/pydantic/pull/6547)
* Update Dataclasses docs by [@tpdorsey](https://github.com/tpdorsey) in [#6470](https://github.com/pydantic/pydantic/pull/6470)
* â™»ï¸ Use different error message on v1 redirects by [@Kludex](https://github.com/Kludex) in [#6595](https://github.com/pydantic/pydantic/pull/6595)
* â¬† Upgrade `pydantic-core` to v2.2.0 by [@lig](https://github.com/lig) in [#6589](https://github.com/pydantic/pydantic/pull/6589)
* Fix serialization for IPvAny by [@dmontagu](https://github.com/dmontagu) in [#6572](https://github.com/pydantic/pydantic/pull/6572)
* Improve CI by using PDM instead of pip to install typing-extensions by [@adriangb](https://github.com/adriangb) in [#6602](https://github.com/pydantic/pydantic/pull/6602)
* Add `enum` error type docs  by [@lig](https://github.com/lig) in [#6603](https://github.com/pydantic/pydantic/pull/6603)
* ðŸ› Fix `max_length` for unicode strings by [@lig](https://github.com/lig) in [#6559](https://github.com/pydantic/pydantic/pull/6559)
* Add documentation for accessing features via `pydantic.v1` by [@tpdorsey](https://github.com/tpdorsey) in [#6604](https://github.com/pydantic/pydantic/pull/6604)
* Include extra when iterating over a model by [@adriangb](https://github.com/adriangb) in [#6562](https://github.com/pydantic/pydantic/pull/6562)
* Fix typing of model_validator by [@adriangb](https://github.com/adriangb) in [#6514](https://github.com/pydantic/pydantic/pull/6514)
* Touch up Decimal validator by [@adriangb](https://github.com/adriangb) in [#6327](https://github.com/pydantic/pydantic/pull/6327)
* Fix various docstrings using fixed pytest-examples by [@dmontagu](https://github.com/dmontagu) in [#6607](https://github.com/pydantic/pydantic/pull/6607)
* Handle function validators in a discriminated union by [@dmontagu](https://github.com/dmontagu) in [#6570](https://github.com/pydantic/pydantic/pull/6570)
* Review json_schema.md by [@tpdorsey](https://github.com/tpdorsey) in [#6608](https://github.com/pydantic/pydantic/pull/6608)
* Make validate_call work on basemodel methods by [@dmontagu](https://github.com/dmontagu) in [#6569](https://github.com/pydantic/pydantic/pull/6569)
* add test for big int json serde by [@davidhewitt](https://github.com/davidhewitt) in [#6614](https://github.com/pydantic/pydantic/pull/6614)
* Fix pydantic dataclass problem with dataclasses.field default_factory by [@hramezani](https://github.com/hramezani) in [#6616](https://github.com/pydantic/pydantic/pull/6616)
* Fixed mypy type inference for TypeAdapter by [@zakstucke](https://github.com/zakstucke) in [#6617](https://github.com/pydantic/pydantic/pull/6617)
* Make it work to use None as a generic parameter by [@dmontagu](https://github.com/dmontagu) in [#6609](https://github.com/pydantic/pydantic/pull/6609)
* Make it work to use `$ref` as an alias by [@dmontagu](https://github.com/dmontagu) in [#6568](https://github.com/pydantic/pydantic/pull/6568)
* add note to migration guide about changes to `AnyUrl` etc by [@davidhewitt](https://github.com/davidhewitt) in [#6618](https://github.com/pydantic/pydantic/pull/6618)
* ðŸ› Support defining `json_schema_extra` on `RootModel` using `Field` by [@lig](https://github.com/lig) in [#6622](https://github.com/pydantic/pydantic/pull/6622)
* Update pre-commit to prevent commits to main branch on accident by [@dmontagu](https://github.com/dmontagu) in [#6636](https://github.com/pydantic/pydantic/pull/6636)
* Fix PDM CI for python 3.7 on MacOS/windows by [@dmontagu](https://github.com/dmontagu) in [#6627](https://github.com/pydantic/pydantic/pull/6627)
* Produce more accurate signatures for pydantic dataclasses by [@dmontagu](https://github.com/dmontagu) in [#6633](https://github.com/pydantic/pydantic/pull/6633)
* Updates to Url types for Pydantic V2 by [@tpdorsey](https://github.com/tpdorsey) in [#6638](https://github.com/pydantic/pydantic/pull/6638)
* Fix list markdown in `transform` docstring by [@StefanBRas](https://github.com/StefanBRas) in [#6649](https://github.com/pydantic/pydantic/pull/6649)
* simplify slots_dataclass construction to appease mypy by [@davidhewitt](https://github.com/davidhewitt) in [#6639](https://github.com/pydantic/pydantic/pull/6639)
* Update TypedDict schema generation docstring by [@adriangb](https://github.com/adriangb) in [#6651](https://github.com/pydantic/pydantic/pull/6651)
* Detect and lint-error for prints by [@dmontagu](https://github.com/dmontagu) in [#6655](https://github.com/pydantic/pydantic/pull/6655)
* Add xfailing test for pydantic-core PR 766 by [@dmontagu](https://github.com/dmontagu) in [#6641](https://github.com/pydantic/pydantic/pull/6641)
* Ignore unrecognized fields from dataclasses metadata by [@dmontagu](https://github.com/dmontagu) in [#6634](https://github.com/pydantic/pydantic/pull/6634)
* Make non-existent class getattr a mypy error by [@dmontagu](https://github.com/dmontagu) in [#6658](https://github.com/pydantic/pydantic/pull/6658)
* Update pydantic-core to 2.3.0 by [@hramezani](https://github.com/hramezani) in [#6648](https://github.com/pydantic/pydantic/pull/6648)
* Use OrderedDict from typing_extensions by [@dmontagu](https://github.com/dmontagu) in [#6664](https://github.com/pydantic/pydantic/pull/6664)
* Fix typehint for JSON schema extra callable by [@dmontagu](https://github.com/dmontagu) in [#6659](https://github.com/pydantic/pydantic/pull/6659)

## v2.0.2 (2023-07-05)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.0.2)

* Fix bug where round-trip pickling/unpickling a `RootModel` would change the value of `__dict__`, [#6457](https://github.com/pydantic/pydantic/pull/6457) by [@dmontagu](https://github.com/dmontagu)
* Allow single-item discriminated unions, [#6405](https://github.com/pydantic/pydantic/pull/6405) by [@dmontagu](https://github.com/dmontagu)
* Fix issue with union parsing of enums, [#6440](https://github.com/pydantic/pydantic/pull/6440) by [@dmontagu](https://github.com/dmontagu)
* Docs: Fixed `constr` documentation, renamed old `regex` to new `pattern`, [#6452](https://github.com/pydantic/pydantic/pull/6452) by [@miili](https://github.com/miili)
* Change `GenerateJsonSchema.generate_definitions` signature, [#6436](https://github.com/pydantic/pydantic/pull/6436) by [@dmontagu](https://github.com/dmontagu)

See the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0.2)

## v2.0.1 (2023-07-04)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.0.1)

First patch release of Pydantic V2

* Extra fields added via `setattr` (i.e. `m.some_extra_field = 'extra_value'`)
  are added to `.model_extra` if `model_config` `extra='allowed'`. Fixed [#6333](https://github.com/pydantic/pydantic/pull/6333), [#6365](https://github.com/pydantic/pydantic/pull/6365) by [@aaraney](https://github.com/aaraney)
* Automatically unpack JSON schema '$ref' for custom types, [#6343](https://github.com/pydantic/pydantic/pull/6343) by [@adriangb](https://github.com/adriangb)
* Fix tagged unions multiple processing in submodels, [#6340](https://github.com/pydantic/pydantic/pull/6340) by [@suharnikov](https://github.com/suharnikov)

See the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0.1)

## v2.0 (2023-06-30)

[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.0)

Pydantic V2 is here! :tada:

See [this post](https://docs.pydantic.dev/2.0/blog/pydantic-v2-final/) for more details.

## v2.0b3 (2023-06-16)

Third beta pre-release of Pydantic V2

See the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0b3)

## v2.0b2 (2023-06-03)

Add `from_attributes` runtime flag to `TypeAdapter.validate_python` and `BaseModel.model_validate`.

See the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0b2)

## v2.0b1 (2023-06-01)

First beta pre-release of Pydantic V2

See the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0b1)

## v2.0a4 (2023-05-05)

Fourth pre-release of Pydantic V2

See the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0a4)

## v2.0a3 (2023-04-20)

Third pre-release of Pydantic V2

See the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0a3)

## v2.0a2 (2023-04-12)

Second pre-release of Pydantic V2

See the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0a2)

## v2.0a1 (2023-04-03)

First pre-release of Pydantic V2!

See [this post](https://docs.pydantic.dev/blog/pydantic-v2-alpha/) for more details.


... see [here](https://docs.pydantic.dev/changelog/#v0322-2019-08-17) for earlier changes.
"""^^xsd:string ;
    schema1:name "pydantic"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.9.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pydantic_core> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/typing-extensions> ;
    schema1:description """# pydantic-core

[![CI](https://github.com/pydantic/pydantic-core/workflows/ci/badge.svg?event=push)](https://github.com/pydantic/pydantic-core/actions?query=event%3Apush+branch%3Amain+workflow%3Aci)
[![Coverage](https://codecov.io/gh/pydantic/pydantic-core/branch/main/graph/badge.svg)](https://codecov.io/gh/pydantic/pydantic-core)
[![pypi](https://img.shields.io/pypi/v/pydantic-core.svg)](https://pypi.python.org/pypi/pydantic-core)
[![versions](https://img.shields.io/pypi/pyversions/pydantic-core.svg)](https://github.com/pydantic/pydantic-core)
[![license](https://img.shields.io/github/license/pydantic/pydantic-core.svg)](https://github.com/pydantic/pydantic-core/blob/main/LICENSE)

This package provides the core functionality for [pydantic](https://docs.pydantic.dev) validation and serialization.

Pydantic-core is currently around 17x faster than pydantic V1.
See [`tests/benchmarks/`](./tests/benchmarks/) for details.

## Example of direct usage

_NOTE: You should not need to use pydantic-core directly; instead, use pydantic, which in turn uses pydantic-core._

```py
from pydantic_core import SchemaValidator, ValidationError


v = SchemaValidator(
    {
        'type': 'typed-dict',
        'fields': {
            'name': {
                'type': 'typed-dict-field',
                'schema': {
                    'type': 'str',
                },
            },
            'age': {
                'type': 'typed-dict-field',
                'schema': {
                    'type': 'int',
                    'ge': 18,
                },
            },
            'is_developer': {
                'type': 'typed-dict-field',
                'schema': {
                    'type': 'default',
                    'schema': {'type': 'bool'},
                    'default': True,
                },
            },
        },
    }
)

r1 = v.validate_python({'name': 'Samuel', 'age': 35})
assert r1 == {'name': 'Samuel', 'age': 35, 'is_developer': True}

# pydantic-core can also validate JSON directly
r2 = v.validate_json('{"name": "Samuel", "age": 35}')
assert r1 == r2

try:
    v.validate_python({'name': 'Samuel', 'age': 11})
except ValidationError as e:
    print(e)
    \"\"\"
    1 validation error for model
    age
      Input should be greater than or equal to 18
      [type=greater_than_equal, context={ge: 18}, input_value=11, input_type=int]
    \"\"\"
```

## Getting Started

You'll need rust stable [installed](https://rustup.rs/), or rust nightly if you want to generate accurate coverage.

With rust and python 3.8+ installed, compiling pydantic-core should be possible with roughly the following:

```bash
# clone this repo or your fork
git clone git@github.com:pydantic/pydantic-core.git
cd pydantic-core
# create a new virtual env
python3 -m venv env
source env/bin/activate
# install dependencies and install pydantic-core
make install
```

That should be it, the example shown above should now run.

You might find it useful to look at [`python/pydantic_core/_pydantic_core.pyi`](./python/pydantic_core/_pydantic_core.pyi) and
[`python/pydantic_core/core_schema.py`](./python/pydantic_core/core_schema.py) for more information on the python API,
beyond that, [`tests/`](./tests) provide a large number of examples of usage.

If you want to contribute to pydantic-core, you'll want to use some other make commands:
* `make build-dev` to build the package during development
* `make build-prod` to perform an optimised build for benchmarking
* `make test` to run the tests
* `make testcov` to run the tests and generate a coverage report
* `make lint` to run the linter
* `make format` to format python and rust code
* `make` to run `format build-dev lint test`

## Profiling

It's possible to profile the code using the [`flamegraph` utility from `flamegraph-rs`](https://github.com/flamegraph-rs/flamegraph). (Tested on Linux.) You can install this with `cargo install flamegraph`.

Run `make build-profiling` to install a release build with debugging symbols included (needed for profiling).

Once that is built, you can profile pytest benchmarks with (e.g.):

```bash
flamegraph -- pytest tests/benchmarks/test_micro_benchmarks.py -k test_list_of_ints_core_py --benchmark-enable
```
The `flamegraph` command will produce an interactive SVG at `flamegraph.svg`.

## Releasing

1. Bump package version locally. Do not just edit `Cargo.toml` on Github, you need both `Cargo.toml` and `Cargo.lock` to be updated.
2. Make a PR for the version bump and merge it.
3. Go to https://github.com/pydantic/pydantic-core/releases and click "Draft a new release"
4. In the "Choose a tag" dropdown enter the new tag `v<the.new.version>` and select "Create new tag on publish" when the option appears.
5. Enter the release title in the form "v<the.new.version> <YYYY-MM-DD>"
6. Click Generate release notes button
7. Click Publish release
8. Go to https://github.com/pydantic/pydantic-core/actions and ensure that all build for release are done successfully.
9. Go to https://pypi.org/project/pydantic-core/ and ensure that the latest release is published.
10. Done ðŸŽ‰

"""^^xsd:string ;
    schema1:name "pydantic_core"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.23.4"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pylint> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/astroid>,
        <https://data.siemens-energy.com/cybersec/package/colorama>,
        <https://data.siemens-energy.com/cybersec/package/dill>,
        <https://data.siemens-energy.com/cybersec/package/gitpython>,
        <https://data.siemens-energy.com/cybersec/package/isort>,
        <https://data.siemens-energy.com/cybersec/package/mccabe>,
        <https://data.siemens-energy.com/cybersec/package/platformdirs>,
        <https://data.siemens-energy.com/cybersec/package/tomli>,
        <https://data.siemens-energy.com/cybersec/package/tomlkit>,
        <https://data.siemens-energy.com/cybersec/package/typing-extensions> ;
    schema1:description """`Pylint`_
=========

.. _`Pylint`: https://pylint.readthedocs.io/

.. This is used inside the doc to recover the start of the introduction

.. image:: https://github.com/pylint-dev/pylint/actions/workflows/tests.yaml/badge.svg?branch=main
    :target: https://github.com/pylint-dev/pylint/actions

.. image:: https://codecov.io/gh/pylint-dev/pylint/branch/main/graph/badge.svg?token=ZETEzayrfk
    :target: https://codecov.io/gh/pylint-dev/pylint

.. image:: https://img.shields.io/pypi/v/pylint.svg
    :alt: PyPI Package version
    :target: https://pypi.python.org/pypi/pylint

.. image:: https://readthedocs.org/projects/pylint/badge/?version=latest
    :target: https://pylint.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/ambv/black

.. image:: https://img.shields.io/badge/linting-pylint-yellowgreen
    :target: https://github.com/pylint-dev/pylint

.. image:: https://results.pre-commit.ci/badge/github/pylint-dev/pylint/main.svg
   :target: https://results.pre-commit.ci/latest/github/pylint-dev/pylint/main
   :alt: pre-commit.ci status

.. image:: https://bestpractices.coreinfrastructure.org/projects/6328/badge
   :target: https://bestpractices.coreinfrastructure.org/projects/6328
   :alt: CII Best Practices

.. image:: https://img.shields.io/ossf-scorecard/github.com/PyCQA/pylint?label=openssf%20scorecard&style=flat
   :target: https://api.securityscorecards.dev/projects/github.com/PyCQA/pylint
   :alt: OpenSSF Scorecard

.. image:: https://img.shields.io/discord/825463413634891776.svg
   :target: https://discord.gg/qYxpadCgkx
   :alt: Discord

What is Pylint?
---------------

Pylint is a `static code analyser`_ for Python 2 or 3. The latest version supports Python
3.9.0 and above.

.. _`static code analyser`: https://en.wikipedia.org/wiki/Static_code_analysis

Pylint analyses your code without actually running it. It checks for errors, enforces a
coding standard, looks for `code smells`_, and can make suggestions about how the code
could be refactored.

.. _`code smells`: https://martinfowler.com/bliki/CodeSmell.html

Install
-------

.. This is used inside the doc to recover the start of the short text for installation

For command line use, pylint is installed with::

    pip install pylint

Or if you want to also check spelling with ``enchant`` (you might need to
`install the enchant C library <https://pyenchant.github.io/pyenchant/install.html#installing-the-enchant-c-library>`_):

.. code-block:: sh

   pip install pylint[spelling]

It can also be integrated in most editors or IDEs. More information can be found
`in the documentation`_.

.. _in the documentation: https://pylint.readthedocs.io/en/latest/user_guide/installation/index.html

.. This is used inside the doc to recover the end of the short text for installation

What differentiates Pylint?
---------------------------

Pylint is not trusting your typing and is inferring the actual values of nodes (for a
start because there was no typing when pylint started off) using its internal code
representation (astroid). If your code is ``import logging as argparse``, Pylint
can check and know that ``argparse.error(...)`` is in fact a logging call and not an
argparse call. This makes pylint slower, but it also lets pylint find more issues if
your code is not fully typed.

    [inference] is the killer feature that keeps us using [pylint] in our project despite how painfully slow it is.
    - `Realist pylint user`_, 2022

.. _`Realist pylint user`: https://github.com/charliermarsh/ruff/issues/970#issuecomment-1381067064

pylint, not afraid of being a little slower than it already is, is also a lot more thorough than other linters.
There are more checks, including some opinionated ones that are deactivated by default
but can be enabled using configuration.

How to use pylint
-----------------

Pylint isn't smarter than you: it may warn you about things that you have
conscientiously done or check for some things that you don't care about.
During adoption, especially in a legacy project where pylint was never enforced,
it's best to start with the ``--errors-only`` flag, then disable
convention and refactor messages with ``--disable=C,R`` and progressively
re-evaluate and re-enable messages as your priorities evolve.

Pylint is highly configurable and permits to write plugins in order to add your
own checks (for example, for internal libraries or an internal rule). Pylint also has an
ecosystem of existing plugins for popular frameworks and third-party libraries.

.. note::

    Pylint supports the Python standard library out of the box. Third-party
    libraries are not always supported, so a plugin might be needed. A good place
    to start is ``PyPI`` which often returns a plugin by searching for
    ``pylint <library>``. `pylint-pydantic`_, `pylint-django`_ and
    `pylint-sonarjson`_ are examples of such plugins. More information about plugins
    and how to load them can be found at `plugins`_.

.. _`plugins`: https://pylint.readthedocs.io/en/latest/development_guide/how_tos/plugins.html#plugins
.. _`pylint-pydantic`: https://pypi.org/project/pylint-pydantic
.. _`pylint-django`: https://github.com/pylint-dev/pylint-django
.. _`pylint-sonarjson`: https://github.com/cnescatlab/pylint-sonarjson-catlab

Advised linters alongside pylint
--------------------------------

Projects that you might want to use alongside pylint include ruff_ (**really** fast,
with builtin auto-fix and a large number of checks taken from popular linters, but
implemented in ``rust``) or flake8_ (a framework to implement your own checks in python using ``ast`` directly),
mypy_, pyright_ / pylance or pyre_ (typing checks), bandit_ (security oriented checks), black_ and
isort_ (auto-formatting), autoflake_ (automated removal of unused imports or variables), pyupgrade_
(automated upgrade to newer python syntax) and pydocstringformatter_ (automated pep257).

.. _ruff: https://github.com/astral-sh/ruff
.. _flake8: https://github.com/PyCQA/flake8
.. _bandit: https://github.com/PyCQA/bandit
.. _mypy: https://github.com/python/mypy
.. _pyright: https://github.com/microsoft/pyright
.. _pyre: https://github.com/facebook/pyre-check
.. _black: https://github.com/psf/black
.. _autoflake: https://github.com/myint/autoflake
.. _pyupgrade: https://github.com/asottile/pyupgrade
.. _pydocstringformatter: https://github.com/DanielNoord/pydocstringformatter
.. _isort: https://pycqa.github.io/isort/

Additional tools included in pylint
-----------------------------------

Pylint ships with two additional tools:

- pyreverse_ (standalone tool that generates package and class diagrams.)
- symilar_  (duplicate code finder that is also integrated in pylint)

.. _pyreverse: https://pylint.readthedocs.io/en/latest/pyreverse.html
.. _symilar: https://pylint.readthedocs.io/en/latest/symilar.html


.. This is used inside the doc to recover the end of the introduction

Contributing
------------

.. This is used inside the doc to recover the start of the short text for contribution

We welcome all forms of contributions such as updates for documentation, new code, checking issues for duplicates or telling us
that we can close them, confirming that issues still exist, `creating issues because
you found a bug or want a feature`_, etc. Everything is much appreciated!

Please follow the `code of conduct`_ and check `the Contributor Guides`_ if you want to
make a code contribution.

.. _creating issues because you found a bug or want a feature: https://pylint.readthedocs.io/en/latest/contact.html#bug-reports-feedback
.. _code of conduct: https://github.com/pylint-dev/pylint/blob/main/CODE_OF_CONDUCT.md
.. _the Contributor Guides: https://pylint.readthedocs.io/en/latest/development_guide/contribute.html

.. This is used inside the doc to recover the end of the short text for contribution

Show your usage
-----------------

You can place this badge in your README to let others know your project uses pylint.

    .. image:: https://img.shields.io/badge/linting-pylint-yellowgreen
        :target: https://github.com/pylint-dev/pylint

Learn how to add a badge to your documentation in `the badge documentation`_.

.. _the badge documentation: https://pylint.readthedocs.io/en/latest/user_guide/installation/badge.html

License
-------

pylint is, with a few exceptions listed below, `GPLv2 <https://github.com/pylint-dev/pylint/blob/main/LICENSE>`_.

The icon files are licensed under the `CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0/>`_ license:

- `doc/logo.png <https://raw.githubusercontent.com/pylint-dev/pylint/main/doc/logo.png>`_
- `doc/logo.svg <https://raw.githubusercontent.com/pylint-dev/pylint/main/doc/logo.svg>`_

Support
-------

Please check `the contact information`_.

.. _`the contact information`: https://pylint.readthedocs.io/en/latest/contact.html

.. |tideliftlogo| image:: https://raw.githubusercontent.com/pylint-dev/pylint/main/doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White.png
   :width: 200
   :alt: Tidelift

.. list-table::
   :widths: 10 100

   * - |tideliftlogo|
     - Professional support for pylint is available as part of the `Tidelift
       Subscription`_.  Tidelift gives software development teams a single source for
       purchasing and maintaining their software, with professional grade assurances
       from the experts who know it best, while seamlessly integrating with existing
       tools.

.. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-pylint?utm_source=pypi-pylint&utm_medium=referral&utm_campaign=readme
"""^^xsd:string ;
    schema1:name "pylint"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.3.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pyproject_hooks> a software_vocab:Software_package ;
    schema1:description """``pyproject-hooks``
===================

This is a low-level library for calling build-backends in ``pyproject.toml``-based project. It provides the basic functionality to help write tooling that generates distribution files from Python projects.

If you want a tool that builds Python packages, you'll want to use https://github.com/pypa/build instead. This is an underlying piece for `pip`, `build` and other "build frontends" use to call "build backends" within them.

You can read more in the `documentation <https://pyproject-hooks.readthedocs.io/>`_.

  Note: The ``pep517`` project has been replaced by this project (low level) and the ``build`` project (high level).

"""^^xsd:string ;
    schema1:name "pyproject_hooks"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.1.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/rdflib> a software_vocab:Software_package ;
    schema1:description """![](docs/_static/RDFlib.png)    

RDFLib
======
[![Build Status](https://github.com/RDFLib/rdflib/actions/workflows/validate.yaml/badge.svg?branch=main)](https://github.com/RDFLib/rdflib/actions?query=branch%3Amain)
[![Documentation Status](https://readthedocs.org/projects/rdflib/badge/?version=latest)](https://rdflib.readthedocs.io/en/latest/?badge=latest)
[![Coveralls branch](https://img.shields.io/coveralls/RDFLib/rdflib/main.svg)](https://coveralls.io/r/RDFLib/rdflib?branch=main)

[![GitHub stars](https://img.shields.io/github/stars/RDFLib/rdflib.svg)](https://github.com/RDFLib/rdflib/stargazers)
[![Downloads](https://pepy.tech/badge/rdflib/week)](https://pepy.tech/project/rdflib)
[![PyPI](https://img.shields.io/pypi/v/rdflib.svg)](https://pypi.python.org/pypi/rdflib)
[![PyPI](https://img.shields.io/pypi/pyversions/rdflib.svg)](https://pypi.python.org/pypi/rdflib)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6845245.svg)](https://doi.org/10.5281/zenodo.6845245)

[![Contribute with Gitpod](https://img.shields.io/badge/Contribute%20with-Gitpod-908a85?logo=gitpod)](https://gitpod.io/#https://github.com/RDFLib/rdflib)
[![Gitter](https://badges.gitter.im/RDFLib/rdflib.svg)](https://gitter.im/RDFLib/rdflib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
[![Matrix](https://img.shields.io/matrix/rdflib:matrix.org?label=matrix.org%20chat)](https://matrix.to/#/#RDFLib_rdflib:gitter.im)

RDFLib is a pure Python package for working with [RDF](http://www.w3.org/RDF/). RDFLib contains most things you need to work with RDF, including:

* parsers and serializers for RDF/XML, N3, NTriples, N-Quads, Turtle, TriX, Trig and JSON-LD
* a Graph interface which can be backed by any one of a number of Store implementations
* store implementations for in-memory, persistent on disk (Berkeley DB) and remote SPARQL endpoints
* a SPARQL 1.1 implementation - supporting SPARQL 1.1 Queries and Update statements
* SPARQL function extension mechanisms

## RDFlib Family of packages
The RDFlib community maintains many RDF-related Python code repositories with different purposes. For example:

* [rdflib](https://github.com/RDFLib/rdflib) - the RDFLib core
* [sparqlwrapper](https://github.com/RDFLib/sparqlwrapper) - a simple Python wrapper around a SPARQL service to remotely execute your queries
* [pyLODE](https://github.com/RDFLib/pyLODE) - An OWL ontology documentation tool using Python and templating, based on LODE.
* [pyrdfa3](https://github.com/RDFLib/pyrdfa3) - RDFa 1.1 distiller/parser library: can extract RDFa 1.1/1.0 from (X)HTML, SVG, or XML in general.
* [pymicrodata](https://github.com/RDFLib/pymicrodata) - A module to extract RDF from an HTML5 page annotated with microdata. 
* [pySHACL](https://github.com/RDFLib/pySHACL) - A pure Python module which allows for the validation of RDF graphs against SHACL graphs.
* [OWL-RL](https://github.com/RDFLib/OWL-RL) - A simple implementation of the OWL2 RL Profile which expands the graph with all possible triples that OWL RL defines.

Please see the list for all packages/repositories here:

* <https://github.com/RDFLib>

Help with maintenance of all of the RDFLib family of packages is always welcome and appreciated.

## Versions & Releases

* `main` branch in this repository is the unstable release
* `7.1.1` current stable release, bugfixes to 7.1.0
* `7.0.0` previous stable release, supports Python 3.8.1+ only.
    * see [Releases](https://github.com/RDFLib/rdflib/releases)
* `6.x.y` supports Python 3.7+ only. Many improvements over 5.0.0
    * see [Releases](https://github.com/RDFLib/rdflib/releases)
* `5.x.y` supports Python 2.7 and 3.4+ and is [mostly backwards compatible with 4.2.2](https://rdflib.readthedocs.io/en/stable/upgrade4to5.html).

See <https://github.com/RDFLib/rdflib/releases/> for the release details.

## Documentation
See <https://rdflib.readthedocs.io> for our documentation built from the code. Note that there are `latest`, `stable` and versioned builds, such as `5.0.0`, matching releases.

## Installation
The stable release of RDFLib may be installed with Python's package management tool *pip*:

    $ pip install rdflib

Some features of RDFLib require optional dependencies which may be installed using *pip* extras:

    $ pip install rdflib[berkeleydb,networkx,html,lxml,orjson]

Alternatively manually download the package from the Python Package
Index (PyPI) at https://pypi.python.org/pypi/rdflib

The current version of RDFLib is 7.1.1, see the ``CHANGELOG.md`` file for what's new in this release.

### Installation of the current main branch (for developers)

With *pip* you can also install rdflib from the git repository with one of the following options:

    $ pip install git+https://github.com/rdflib/rdflib@main

or

    $ pip install -e git+https://github.com/rdflib/rdflib@main#egg=rdflib

or from your locally cloned repository you can install it with one of the following options:

    $ poetry install  # installs into a poetry-managed venv

or

    $ pip install -e .

## Getting Started
RDFLib aims to be a pythonic RDF API. RDFLib's main data object is a `Graph` which is a Python collection
of RDF *Subject, Predicate, Object* Triples:

To create graph and load it with RDF data from DBPedia then print the results:

```python
from rdflib import Graph
g = Graph()
g.parse('http://dbpedia.org/resource/Semantic_Web')

for s, p, o in g:
    print(s, p, o)
```
The components of the triples are URIs (resources) or Literals
(values).

URIs are grouped together by *namespace*, common namespaces are included in RDFLib:

```python
from rdflib.namespace import DC, DCTERMS, DOAP, FOAF, SKOS, OWL, RDF, RDFS, VOID, XMLNS, XSD
```

You can use them like this:

```python
from rdflib import Graph, URIRef, Literal
from rdflib.namespace import RDFS, XSD

g = Graph()
semweb = URIRef('http://dbpedia.org/resource/Semantic_Web')
type = g.value(semweb, RDFS.label)
```
Where `RDFS` is the RDFS namespace, `XSD` the XML Schema Datatypes namespace and `g.value` returns an object of the triple-pattern given (or an arbitrary one if multiple exist).

Or like this, adding a triple to a graph `g`:

```python
g.add((
    URIRef("http://example.com/person/nick"),
    FOAF.givenName,
    Literal("Nick", datatype=XSD.string)
))
```
The triple (in n-triples notation) `<http://example.com/person/nick> <http://xmlns.com/foaf/0.1/givenName> "Nick"^^<http://www.w3.org/2001/XMLSchema#string> .`
is created where the property `FOAF.givenName` is the URI `<http://xmlns.com/foaf/0.1/givenName>` and `XSD.string` is the
URI `<http://www.w3.org/2001/XMLSchema#string>`.

You can bind namespaces to prefixes to shorten the URIs for RDF/XML, Turtle, N3, TriG, TriX & JSON-LD serializations:

 ```python
g.bind("foaf", FOAF)
g.bind("xsd", XSD)
```
This will allow the n-triples triple above to be serialised like this:
 ```python
print(g.serialize(format="turtle"))
```

With these results:
```turtle
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

<http://example.com/person/nick> foaf:givenName "Nick"^^xsd:string .
```

New Namespaces can also be defined:

```python
dbpedia = Namespace('http://dbpedia.org/ontology/')

abstracts = list(x for x in g.objects(semweb, dbpedia['abstract']) if x.language=='en')
```

See also [./examples](./examples)


## Features
The library contains parsers and serializers for RDF/XML, N3,
NTriples, N-Quads, Turtle, TriX, JSON-LD, RDFa and Microdata.

The library presents a Graph interface which can be backed by
any one of a number of Store implementations.

This core RDFLib package includes store implementations for
in-memory storage and persistent storage on top of the Berkeley DB.

A SPARQL 1.1 implementation is included - supporting SPARQL 1.1 Queries and Update statements.

RDFLib is open source and is maintained on [GitHub](https://github.com/RDFLib/rdflib/). RDFLib releases, current and previous
are listed on [PyPI](https://pypi.python.org/pypi/rdflib/)

Multiple other projects are contained within the RDFlib "family", see <https://github.com/RDFLib/>.

## Running tests

### Running the tests on the host

Run the test suite with `pytest`.
```shell
poetry install
poetry run pytest
```

### Running test coverage on the host with coverage report

Run the test suite and generate a HTML coverage report with `pytest` and `pytest-cov`.
```shell
poetry run pytest --cov
```

### Viewing test coverage

Once tests have produced HTML output of the coverage report, view it by running:
```shell
poetry run pytest --cov --cov-report term --cov-report html
python -m http.server --directory=htmlcov
```

## Contributing

RDFLib survives and grows via user contributions!
Please read our [contributing guide](https://rdflib.readthedocs.io/en/latest/CONTRIBUTING.html) and [developers guide](https://rdflib.readthedocs.io/en/latest/developers.html) to get started.
Please consider lodging Pull Requests here:

* <https://github.com/RDFLib/rdflib/pulls>

To get a development environment consider using Gitpod or Google Cloud Shell.

[![Open in Gitpod](https://gitpod.io/button/open-in-gitpod.svg)](https://gitpod.io/#https://github.com/RDFLib/rdflib)
[![Open in Cloud Shell](https://gstatic.com/cloudssh/images/open-btn.svg)](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2FRDFLib%2Frdflib&cloudshell_git_branch=main&cloudshell_open_in_editor=README.md)

You can also raise issues here:

* <https://github.com/RDFLib/rdflib/issues>

## Support & Contacts
For general "how do I..." queries, please use https://stackoverflow.com and tag your question with `rdflib`.
Existing questions:

* <https://stackoverflow.com/questions/tagged/rdflib>

If you want to contact the rdflib maintainers, please do so via:

* the rdflib-dev mailing list: <https://groups.google.com/group/rdflib-dev>
* the chat, which is available at [gitter](https://gitter.im/RDFLib/rdflib) or via matrix [#RDFLib_rdflib:gitter.im](https://matrix.to/#/#RDFLib_rdflib:gitter.im)

"""^^xsd:string ;
    schema1:name "rdflib"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "7.1.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/requests> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/PySocks>,
        <https://data.siemens-energy.com/cybersec/package/certifi>,
        <https://data.siemens-energy.com/cybersec/package/chardet>,
        <https://data.siemens-energy.com/cybersec/package/charset-normalizer>,
        <https://data.siemens-energy.com/cybersec/package/idna>,
        <https://data.siemens-energy.com/cybersec/package/urllib3> ;
    schema1:description """# Requests

**Requests** is a simple, yet elegant, HTTP library.

```python
>>> import requests
>>> r = requests.get('https://httpbin.org/basic-auth/user/pass', auth=('user', 'pass'))
>>> r.status_code
200
>>> r.headers['content-type']
'application/json; charset=utf8'
>>> r.encoding
'utf-8'
>>> r.text
'{"authenticated": true, ...'
>>> r.json()
{'authenticated': True, ...}
```

Requests allows you to send HTTP/1.1 requests extremely easily. Thereâ€™s no need to manually add query strings to your URLs, or to form-encode your `PUT` & `POST` data â€” but nowadays, just use the `json` method!

Requests is one of the most downloaded Python packages today, pulling in around `30M downloads / week`â€” according to GitHub, Requests is currently [depended upon](https://github.com/psf/requests/network/dependents?package_id=UGFja2FnZS01NzA4OTExNg%3D%3D) by `1,000,000+` repositories. You may certainly put your trust in this code.

[![Downloads](https://static.pepy.tech/badge/requests/month)](https://pepy.tech/project/requests)
[![Supported Versions](https://img.shields.io/pypi/pyversions/requests.svg)](https://pypi.org/project/requests)
[![Contributors](https://img.shields.io/github/contributors/psf/requests.svg)](https://github.com/psf/requests/graphs/contributors)

## Installing Requests and Supported Versions

Requests is available on PyPI:

```console
$ python -m pip install requests
```

Requests officially supports Python 3.8+.

## Supported Features & Bestâ€“Practices

Requests is ready for the demands of building robust and reliable HTTPâ€“speaking applications, for the needs of today.

- Keep-Alive & Connection Pooling
- International Domains and URLs
- Sessions with Cookie Persistence
- Browser-style TLS/SSL Verification
- Basic & Digest Authentication
- Familiar `dict`â€“like Cookies
- Automatic Content Decompression and Decoding
- Multi-part File Uploads
- SOCKS Proxy Support
- Connection Timeouts
- Streaming Downloads
- Automatic honoring of `.netrc`
- Chunked HTTP Requests

## API Reference and User Guide available on [Read the Docs](https://requests.readthedocs.io)

[![Read the Docs](https://raw.githubusercontent.com/psf/requests/main/ext/ss.png)](https://requests.readthedocs.io)

## Cloning the repository

When cloning the Requests repository, you may need to add the `-c
fetch.fsck.badTimezone=ignore` flag to avoid an error about a bad commit (see
[this issue](https://github.com/psf/requests/issues/2690) for more background):

```shell
git clone -c fetch.fsck.badTimezone=ignore https://github.com/psf/requests.git
```

You can also apply this setting to your global Git config:

```shell
git config --global fetch.fsck.badTimezone ignore
```

---

[![Kenneth Reitz](https://raw.githubusercontent.com/psf/requests/main/ext/kr.png)](https://kennethreitz.org) [![Python Software Foundation](https://raw.githubusercontent.com/psf/requests/main/ext/psf.png)](https://www.python.org/psf)
"""^^xsd:string ;
    schema1:name "requests"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.32.3"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/requirements-parser> a software_vocab:Software_package ;
    schema1:description """Requirements Parser
===================

[![Python CI](https://github.com/madpah/requirements-parser/actions/workflows/poetry.yml/badge.svg)](https://github.com/madpah/requirements-parser/actions/workflows/poetry.yml)
[![Documentation Status](http://readthedocs.org/projects/requirements-parser/badge/?version=latest)](http://requirements-parser.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

This is a small Python module for parsing [Pip](http://www.pip-installer.org/) requirement files.

The goal is to parse everything in the 
[Pip requirement file format](https://pip.pypa.io/en/stable/reference/pip_install/#requirements-file-format) spec.

Installation
============

    pip install requirements-parser

or

    poetry add requirements-parser

Examples
========

Requirements parser can parse a file-like object or a text string.

``` {.python}
>>> import requirements
>>> with open('requirements.txt', 'r') as fd:
...     for req in requirements.parse(fd):
...         print(req.name, req.specs)
Django [('>=', '1.11'), ('<', '1.12')]
six [('==', '1.10.0')]
```

It can handle most if not all of the options in requirement files that
do not involve traversing the local filesystem. These include:

-   editables (`-e git+https://github.com/toastdriven/pyelasticsearch.git]{.title-ref}`)
-   version control URIs
-   egg hashes and subdirectories (`[\\#egg=django-haystack&subdirectory=setup]{.title-ref}`)
-   extras ([DocParser\\[PDF\\]]{.title-ref})
-   URLs

Documentation
=============

For more details and examples, the documentation is available at:
<http://requirements-parser.readthedocs.io>.


Change Log
==========

Change log is available on GitHub [here]()

"""^^xsd:string ;
    schema1:name "requirements-parser"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.5.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/rfc3339-validator> a software_vocab:Software_package ;
    schema1:description """# rfc3339-validator

A pure python RFC3339 validator


[![image](https://img.shields.io/pypi/v/rfc3339_validator.svg)](https://pypi.python.org/pypi/rfc3339_validator)
[![Build Status](https://travis-ci.org/naimetti/rfc3339-validator.svg?branch=master)](https://travis-ci.org/naimetti/rfc3339-validator)

# Install

```shell script
pip install rfc3339-validator
```

# Usage

```python
from rfc3339_validator import validate_rfc3339

validate_rfc3339('1424-45-93T15:32:12.9023368Z')
>>> False

validate_rfc3339('2001-10-23T15:32:12.9023368Z')
>>> True
```


  - Free software: MIT license


"""^^xsd:string ;
    schema1:name "rfc3339-validator"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.1.4"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/rfc3987> a software_vocab:Software_package ;
    schema1:description """This module provides regular expressions according to `RFC 3986 "Uniform
Resource Identifier (URI): Generic Syntax"
<http://tools.ietf.org/html/rfc3986>`_ and `RFC 3987 "Internationalized
Resource Identifiers (IRIs)" <http://tools.ietf.org/html/rfc3987>`_, and
utilities for composition and relative resolution of references.


API
---

**match** (string, rule='IRI_reference')
    Convenience function for checking if `string` matches a specific rule.

    Returns a match object or None::

        >>> assert match('%C7X', 'pct_encoded') is None
        >>> assert match('%C7', 'pct_encoded')
        >>> assert match('%c7', 'pct_encoded')



**parse** (string, rule='IRI_reference')
    Parses `string` according to `rule` into a dict of subcomponents.

    If `rule` is None, parse an IRI_reference `without validation
    <http://tools.ietf.org/html/rfc3986#appendix-B>`_.

    If regex_ is available, any rule is supported; with re_, `rule` must be
    'IRI_reference' or some special case thereof ('IRI', 'absolute_IRI',
    'irelative_ref', 'irelative_part', 'URI_reference', 'URI', 'absolute_URI',
    'relative_ref', 'relative_part'). ::

        >>> d = parse('http://tools.ietf.org/html/rfc3986#appendix-A',
        ...           rule='URI')
        >>> assert all([ d['scheme'] == 'http',
        ...              d['authority'] == 'tools.ietf.org',
        ...              d['path'] == '/html/rfc3986',
        ...              d['query'] == None,
        ...              d['fragment'] == 'appendix-A' ])



**compose** (\\*\\*parts)
    Returns an URI composed_ from named parts.

    .. _composed: http://tools.ietf.org/html/rfc3986#section-5.3


**resolve** (base, uriref, strict=True, return_parts=False)
    Resolves_ an `URI reference` relative to a `base` URI.

    `Test cases <http://tools.ietf.org/html/rfc3986#section-5.4>`_::

        >>> base = resolve.test_cases_base
        >>> for relative, resolved in resolve.test_cases.items():
        ...     assert resolve(base, relative) == resolved

    If `return_parts` is True, returns a dict of named parts instead of
    a string.

    Examples::

        >>> assert resolve('urn:rootless', '../../name') == 'urn:name'
        >>> assert resolve('urn:root/less', '../../name') == 'urn:/name'
        >>> assert resolve('http://a/b', 'http:g') == 'http:g'
        >>> assert resolve('http://a/b', 'http:g', strict=False) == 'http://a/g'

    .. _Resolves: http://tools.ietf.org/html/rfc3986#section-5.2



**patterns**
    A dict of regular expressions with useful group names.
    Compilable (with regex_ only) without need for any particular compilation
    flag.

**[bmp_][u]patterns[_no_names]**
    Alternative versions of `patterns`.
    [u]nicode strings without group names for the re_ module.
    BMP only for narrow builds.

**get_compiled_pattern** (rule, flags=0)
    Returns a compiled pattern object for a rule name or template string.

    Usage for validation::

        >>> uri = get_compiled_pattern('^%(URI)s$')
        >>> assert uri.match('http://tools.ietf.org/html/rfc3986#appendix-A')
        >>> assert not get_compiled_pattern('^%(relative_ref)s$').match('#f#g')
        >>> from unicodedata import lookup
        >>> smp = 'urn:' + lookup('OLD ITALIC LETTER A')  # U+00010300
        >>> assert not uri.match(smp)
        >>> m = get_compiled_pattern('^%(IRI)s$').match(smp)

    On narrow builds, non-BMP characters are (incorrectly) excluded::

        >>> assert NARROW_BUILD == (not m)

    For parsing, some subcomponents are captured in named groups (*only if*
    regex_ is available, otherwise see `parse`)::

        >>> match = uri.match('http://tools.ietf.org/html/rfc3986#appendix-A')
        >>> d = match.groupdict()
        >>> if REGEX:
        ...     assert all([ d['scheme'] == 'http',
        ...                  d['authority'] == 'tools.ietf.org',
        ...                  d['path'] == '/html/rfc3986',
        ...                  d['query'] == None,
        ...                  d['fragment'] == 'appendix-A' ])

        >>> for r in patterns.keys():
        ...     assert get_compiled_pattern(r)



**format_patterns** (\\*\\*names)
    Returns a dict of patterns (regular expressions) keyed by
    `rule names for URIs`_ and `rule names for IRIs`_.

    See also the module level dicts of patterns, and `get_compiled_pattern`.

    To wrap a rule in a named capture group, pass it as keyword argument:
    rule_name='group_name'. By default, the formatted patterns contain no
    named groups.

    Patterns are `str` instances (be it in python 2.x or 3.x) containing ASCII
    characters only.

    Caveats:

      - with re_, named capture groups cannot occur on multiple branches of an
        alternation

      - with re_ before python 3.3, ``\\u`` and ``\\U`` escapes must be
        preprocessed (see `issue3665 <http://bugs.python.org/issue3665>`_)

      - on narrow builds, character ranges beyond BMP are not supported

    .. _rule names for URIs: http://tools.ietf.org/html/rfc3986#appendix-A
    .. _rule names for IRIs: http://tools.ietf.org/html/rfc3987#section-2.2



Dependencies
------------

Some features require regex_.

This package's docstrings are tested on python 2.6, 2.7, and 3.2 to 3.6.
Note that in python<=3.2, characters beyond the Basic Multilingual Plane are
not supported on narrow builds (see `issue12729
<http://bugs.python.org/issue12729>`_).


Release notes
-------------

version 1.3.8:

- fixed deprecated escape sequence

version 1.3.6:

- fixed a bug in IPv6 pattern:

  >>> assert match('::0:0:0:0:0.0.0.0', 'IPv6address')

version 1.3.4:

- allowed for lower case percent encoding

version 1.3.3:

- fixed a bug in `resolve` which left "../" at the beginning of some paths

version 1.3.2:

- convenience function `match`
- patterns restricted to the BMP for narrow builds
- adapted doctests for python 3.3
- compatibility with python 2.6 (thanks to Thijs Janssen)

version 1.3.1:

- some re_ compatibility: get_compiled_pattern, parse
- dropped regex_ from setup.py requirements

version 1.3.0:

- python 3.x compatibility
- format_patterns

version 1.2.1:

- compose, resolve


.. _re: http://docs.python.org/library/re
.. _regex: http://pypi.python.org/pypi/regex


Support
-------
This is free software. You may show your appreciation with a `donation`_.

.. _donation: http://danielgerber.net/Â¤#Thanks-for-python-package-rfc3987



"""^^xsd:string ;
    schema1:name "rfc3987"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.3.8"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/s3transfer> a software_vocab:Software_package ;
    schema1:description """=====================================================
s3transfer - An Amazon S3 Transfer Manager for Python
=====================================================

S3transfer is a Python library for managing Amazon S3 transfers.
This project is maintained and published by Amazon Web Services.

.. note::

  This project is not currently GA. If you are planning to use this code in
  production, make sure to lock to a minor version as interfaces may break
  from minor version to minor version. For a basic, stable interface of
  s3transfer, try the interfaces exposed in `boto3 <https://boto3.readthedocs.io/en/latest/guide/s3.html#using-the-transfer-manager>`__


"""^^xsd:string ;
    schema1:name "s3transfer"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.10.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/sbommerge> a software_vocab:Software_package ;
    schema1:description """# SBOMMerge

SBOMMerge merges two Software Bill of Materials (SBOMs) documents together. It supports SBOMs created in both
[SPDX](https://www.spdx.org) and [CycloneDX](https://www.cyclonedx.org) formats.

## Installation

To install use the following command:

`pip install sbommerge`

Alternatively, just clone the repo and install dependencies using the following command:

`pip install -U -r requirements.txt`

The tool requires Python 3 (3.7+). It is recommended to use a virtual python environment especially
if you are using different versions of python. `virtualenv` is a tool for setting up virtual python environments which
allows you to have all the dependencies for the tool set up in a single environment, or have different environments set
up for testing using different versions of Python.

## Usage

```bash
usage: sbommerge [-h] [--sbom {auto,spdx,cyclonedx}] [-d] [--format {tag,json,yaml}] [-o OUTPUT_FILE] [-V] FILE1 FILE2

SBOMMerge merges two Software Bill of Materials (SBOMs) documents together.

positional arguments:
  FILE1                 first SBOM file
  FILE2                 second SBOM file

options:
  -h, --help            show this help message and exit
  -V, --version         show program's version number and exit

Input:
  --sbom {auto,spdx,cyclonedx}
                        specify type of sbom to merge (default: auto)

Output:
  -d, --debug           show debug information
  --format {tag,json,yaml}
                        specify format of generated sbom (default: tag)
  -o OUTPUT_FILE, --output-file OUTPUT_FILE
                        output filename (default: output to stdout)
```
						
## Operation

The `--sbom` option is used to specify the format of the SBOM files. The default is for the type and format of the SBOM to be
automatically detected based on the extension of the file name.

| SBOM Type | Version   | Extension      |Format         |
| --------- | --------- | ---------------|---------------|
| SPDX      | 2.3       | .spdx          | TagValue      |
| SPDX      | 2.3       | .spdx.json     | JSON          |
| SPDX      | 2.3       | .spdx.yml      | YAML          |
| SPDX      | 2.3       | .spdx.yaml     | YAML          |
| CycloneDX | 1.4       | .json          | JSON          |
| CycloneDX | 1.5       | .json          | JSON          |

Details of the formats for each of the supported SBOM formats are available for
[SPDX](https://spdx.dev/) and [CycloneDX](https://cyclonedx.org/).

For SPDX SBOM files, it is assumed that the name of a Package precedes the version information for the package.
Only modules with a package name and associated version information shall be processed.

The `--output-file` option is used to control the destination of the output generated by the tool. The
default is to report to the console but can be stored in a file (specified using `--output-file` option).

## Implementation Notes

The following design decisions have been made in processing the SBOM files:

1. Package data is merged if the package version matches. Otherwise separate packages will be created.

2. It is assumed that the SBOM is valid and contains syntactically valid data. Invalid files will be silently ignored.

3. A non-zero return value indicates that differences were detected.

## License

Licensed under the Apache 2.0 Licence.

## Limitations

This tool is meant to support software development and security audit functions. However the usefulness of the tool is dependent on the SBOM data
which is provided to the tool. Unfortunately, the tool is unable to determine the validity or completeness of such a SBOM file; users of the tool
are therefore reminded that they should assert the quality of any data which is provided to the tool.

## Feedback and Contributions

Bugs and feature requests can be made via GitHub Issues.
"""^^xsd:string ;
    schema1:name "sbommerge"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.2.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/semantic-version> a software_vocab:Software_package ;
    schema1:description """Introduction
============

This small python library provides a few tools to handle `SemVer`_ in Python.
It follows strictly the 2.0.0 version of the SemVer scheme.

.. image:: https://github.com/rbarrois/python-semanticversion/actions/workflows/test.yml/badge.svg
    :target: https://github.com/rbarrois/python-semanticversion/actions/workflows/test.yml

.. image:: https://img.shields.io/pypi/v/semantic_version.svg
    :target: https://python-semanticversion.readthedocs.io/en/latest/changelog.html
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/pyversions/semantic_version.svg
    :target: https://pypi.python.org/pypi/semantic_version/
    :alt: Supported Python versions

.. image:: https://img.shields.io/pypi/wheel/semantic_version.svg
    :target: https://pypi.python.org/pypi/semantic_version/
    :alt: Wheel status

.. image:: https://img.shields.io/pypi/l/semantic_version.svg
    :target: https://pypi.python.org/pypi/semantic_version/
    :alt: License

Links
-----

- Package on `PyPI`_: https://pypi.org/project/semantic-version/
- Doc on `ReadTheDocs <http://readthedocs.org/>`_: https://python-semanticversion.readthedocs.io/
- Source on `GitHub <http://github.com/>`_: http://github.com/rbarrois/python-semanticversion/
- Build on Github Actions: https://github.com/rbarrois/python-semanticversion/actions
- Semantic Version specification: `SemVer`_


Getting started
===============

Install the package from `PyPI`_, using pip:

.. code-block:: sh

    pip install semantic-version

Or from GitHub:

.. code-block:: sh

    $ git clone git://github.com/rbarrois/python-semanticversion.git


Import it in your code:


.. code-block:: python

    import semantic_version


This module provides classes to handle semantic versions:

- ``Version`` represents a version number (``0.1.1-alpha+build.2012-05-15``)
- ``BaseSpec``-derived classes represent requirement specifications (``>=0.1.1,<0.3.0``):

  - ``SimpleSpec`` describes a natural description syntax
  - ``NpmSpec`` is used for NPM-style range descriptions.

Versions
--------

Defining a ``Version`` is quite simple:


.. code-block:: pycon

    >>> import semantic_version
    >>> v = semantic_version.Version('0.1.1')
    >>> v.major
    0
    >>> v.minor
    1
    >>> v.patch
    1
    >>> v.prerelease
    []
    >>> v.build
    []
    >>> list(v)
    [0, 1, 1, [], []]

If the provided version string is invalid, a ``ValueError`` will be raised:

.. code-block:: pycon

    >>> semantic_version.Version('0.1')
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "/Users/rbarrois/dev/semantic_version/src/semantic_version/base.py", line 64, in __init__
        major, minor, patch, prerelease, build = self.parse(version_string, partial)
      File "/Users/rbarrois/dev/semantic_version/src/semantic_version/base.py", line 86, in parse
        raise ValueError('Invalid version string: %r' % version_string)
    ValueError: Invalid version string: '0.1'


One may also create a ``Version`` with named components:

.. code-block:: pycon

    >>> semantic_version.Version(major=0, minor=1, patch=2)
    Version('0.1.2')

In that case, ``major``, ``minor`` and ``patch`` are mandatory, and must be integers.
``prerelease`` and ``build``, if provided, must be tuples of strings:

.. code-block:: pycon

    >>> semantic_version.Version(major=0, minor=1, patch=2, prerelease=('alpha', '2'))
    Version('0.1.2-alpha.2')


Some user-supplied input might not match the semantic version scheme.
For such cases, the ``Version.coerce`` method will try to convert any
version-like string into a valid semver version:

.. code-block:: pycon

    >>> Version.coerce('0')
    Version('0.0.0')
    >>> Version.coerce('0.1.2.3.4')
    Version('0.1.2+3.4')
    >>> Version.coerce('0.1.2a3')
    Version('0.1.2-a3')

Working with versions
\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"

Obviously, versions can be compared:


.. code-block:: pycon

    >>> semantic_version.Version('0.1.1') < semantic_version.Version('0.1.2')
    True
    >>> semantic_version.Version('0.1.1') > semantic_version.Version('0.1.1-alpha')
    True
    >>> semantic_version.Version('0.1.1') <= semantic_version.Version('0.1.1-alpha')
    False

You can also get a new version that represents a bump in one of the version levels:

.. code-block:: pycon

    >>> v = semantic_version.Version('0.1.1+build')
    >>> new_v = v.next_major()
    >>> str(new_v)
    '1.0.0'
    >>> v = semantic_version.Version('1.1.1+build')
    >>> new_v = v.next_minor()
    >>> str(new_v)
    '1.2.0'
    >>> v = semantic_version.Version('1.1.1+build')
    >>> new_v = v.next_patch()
    >>> str(new_v)
    '1.1.2'



Requirement specification
-------------------------

python-semanticversion provides a couple of ways to describe a range of accepted
versions:

- The ``SimpleSpec`` class provides a simple, easily understood scheme --
  somewhat inspired from PyPI range notations;
- The ``NpmSpec`` class supports the whole NPM range specification scheme:

  .. code-block:: pycon

      >>> Version('0.1.2') in NpmSpec('0.1.0-alpha.2 .. 0.2.4')
      True
      >>> Version('0.1.2') in NpmSpec('>=0.1.1 <0.1.3 || 2.x')
      True
      >>> Version('2.3.4') in NpmSpec('>=0.1.1 <0.1.3 || 2.x')
      True

The ``SimpleSpec`` scheme
\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""

Basic usage is simply a comparator and a base version:

.. code-block:: pycon

    >>> s = SimpleSpec('>=0.1.1')  # At least 0.1.1
    >>> s.match(Version('0.1.1'))
    True
    >>> s.match(Version('0.1.1-alpha1'))  # pre-release doesn't satisfy version spec
    False
    >>> s.match(Version('0.1.0'))
    False

Combining specifications can be expressed as follows:

  .. code-block:: pycon

      >>> SimpleSpec('>=0.1.1,<0.3.0')

Simpler test syntax is also available using the ``in`` keyword:

.. code-block:: pycon

    >>> s = SimpleSpec('==0.1.1')
    >>> Version('0.1.1+git7ccc72') in s  # build variants are equivalent to full versions
    True
    >>> Version('0.1.1-alpha1') in s     # pre-release variants don't match the full version.
    False
    >>> Version('0.1.2') in s
    False


Refer to the full documentation at
https://python-semanticversion.readthedocs.io/en/latest/ for more details on the
``SimpleSpec`` scheme.



Using a specification
\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"

The ``SimpleSpec.filter`` method filters an iterable of ``Version``:

.. code-block:: pycon

    >>> s = SimpleSpec('>=0.1.0,<0.4.0')
    >>> versions = (Version('0.%d.0' % i) for i in range(6))
    >>> for v in s.filter(versions):
    ...     print v
    0.1.0
    0.2.0
    0.3.0

It is also possible to select the 'best' version from such iterables:


.. code-block:: pycon

    >>> s = SimpleSpec('>=0.1.0,<0.4.0')
    >>> versions = (Version('0.%d.0' % i) for i in range(6))
    >>> s.select(versions)
    Version('0.3.0')



Contributing
============

In order to contribute to the source code:

- Open an issue on `GitHub`_: https://github.com/rbarrois/python-semanticversion/issues
- Fork the `repository <https://github.com/rbarrois/python-semanticversion>`_
  and submit a pull request on `GitHub`_
- Or send me a patch (mailto:raphael.barrois+semver@polytechnique.org)

When submitting patches or pull requests, you should respect the following rules:

- Coding conventions are based on :pep:`8`
- The whole test suite must pass after adding the changes
- The test coverage for a new feature must be 100%
- New features and methods should be documented in the ``reference`` section
  and included in the ``changelog``
- Include your name in the ``contributors`` section

.. note:: All files should contain the following header::

          # -*- encoding: utf-8 -*-
          # Copyright (c) The python-semanticversion project

.. _SemVer: http://semver.org/
.. _PyPI: http://pypi.python.org/


"""^^xsd:string ;
    schema1:name "semantic-version"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.10.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/semver> a software_vocab:Software_package ;
    schema1:description """Quickstart
==========

.. teaser-begin

A Python module to simplify `semantic versioning`_.

|GHAction| |python-support| |downloads| |license| |docs| |black|
|openissues| |GHDiscussion|

.. teaser-end

The module follows the ``MAJOR.MINOR.PATCH`` style:

* ``MAJOR`` version when you make incompatible API changes,
* ``MINOR`` version when you add functionality in a backwards compatible manner, and
* ``PATCH`` version when you make backwards compatible bug fixes.

Additional labels for pre-release and build metadata are supported.

To import this library, use:

.. code-block:: python

    >>> import semver

Working with the library is quite straightforward. To turn a version string into the
different parts, use the ``semver.Version.parse`` function:

.. code-block:: python

    >>> ver = semver.Version.parse('1.2.3-pre.2+build.4')
    >>> ver.major
    1
    >>> ver.minor
    2
    >>> ver.patch
    3
    >>> ver.prerelease
    'pre.2'
    >>> ver.build
    'build.4'

To raise parts of a version, there are a couple of functions available for
you. The function ``semver.Version.bump_major`` leaves the original object untouched, but
returns a new ``semver.Version`` instance with the raised major part:

.. code-block:: python

    >>> ver = semver.Version.parse("3.4.5")
    >>> ver.bump_major()
    Version(major=4, minor=0, patch=0, prerelease=None, build=None)

It is allowed to concatenate different "bump functions":

.. code-block:: python

    >>> ver.bump_major().bump_minor()
    Version(major=4, minor=1, patch=0, prerelease=None, build=None)

To compare two versions, semver provides the ``semver.compare`` function.
The return value indicates the relationship between the first and second
version:

.. code-block:: python

    >>> semver.compare("1.0.0", "2.0.0")
    -1
    >>> semver.compare("2.0.0", "1.0.0")
    1
    >>> semver.compare("2.0.0", "2.0.0")
    0


There are other functions to discover. Read on!


.. |latest-version| image:: https://img.shields.io/pypi/v/semver.svg
   :alt: Latest version on PyPI
   :target: https://pypi.org/project/semver
.. |python-support| image:: https://img.shields.io/pypi/pyversions/semver.svg
   :target: https://pypi.org/project/semver
   :alt: Python versions
.. |downloads| image:: https://img.shields.io/pypi/dm/semver.svg
   :alt: Monthly downloads from PyPI
   :target: https://pypi.org/project/semver
.. |license| image:: https://img.shields.io/pypi/l/semver.svg
   :alt: Software license
   :target: https://github.com/python-semver/python-semver/blob/master/LICENSE.txt
.. |docs| image:: https://readthedocs.org/projects/python-semver/badge/?version=latest
   :target: http://python-semver.readthedocs.io/en/latest/?badge=latest
   :alt: Documentation Status
.. _semantic versioning: https://semver.org/
.. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black
    :alt: Black Formatter
.. |Gitter| image:: https://badges.gitter.im/python-semver/community.svg
    :target: https://gitter.im/python-semver/community
    :alt: Gitter
.. |openissues| image:: http://isitmaintained.com/badge/open/python-semver/python-semver.svg
    :target: http://isitmaintained.com/project/python-semver/python-semver
    :alt: Percentage of open issues
.. |GHAction| image:: https://github.com/python-semver/python-semver/workflows/Python/badge.svg
    :alt: Python
.. |GHDiscussion| image:: https://shields.io/badge/GitHub-%20Discussions-green?logo=github
    :target: https://github.com/python-semver/python-semver/discussions
    :alt: GitHub Discussion
"""^^xsd:string ;
    schema1:name "semver"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.0.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/sortedcontainers> a software_vocab:Software_package ;
    schema1:description """Python Sorted Containers
========================

`Sorted Containers`_ is an Apache2 licensed `sorted collections library`_,
written in pure-Python, and fast as C-extensions.

Python's standard library is great until you need a sorted collections
type. Many will attest that you can get really far without one, but the moment
you **really need** a sorted list, sorted dict, or sorted set, you're faced
with a dozen different implementations, most using C-extensions without great
documentation and benchmarking.

In Python, we can do better. And we can do it in pure-Python!

.. code-block:: python

    >>> from sortedcontainers import SortedList
    >>> sl = SortedList(['e', 'a', 'c', 'd', 'b'])
    >>> sl
    SortedList(['a', 'b', 'c', 'd', 'e'])
    >>> sl *= 10_000_000
    >>> sl.count('c')
    10000000
    >>> sl[-3:]
    ['e', 'e', 'e']
    >>> from sortedcontainers import SortedDict
    >>> sd = SortedDict({'c': 3, 'a': 1, 'b': 2})
    >>> sd
    SortedDict({'a': 1, 'b': 2, 'c': 3})
    >>> sd.popitem(index=-1)
    ('c', 3)
    >>> from sortedcontainers import SortedSet
    >>> ss = SortedSet('abracadabra')
    >>> ss
    SortedSet(['a', 'b', 'c', 'd', 'r'])
    >>> ss.bisect_left('c')
    2

All of the operations shown above run in faster than linear time. The above
demo also takes nearly a gigabyte of memory to run. When the sorted list is
multiplied by ten million, it stores ten million references to each of "a"
through "e". Each reference requires eight bytes in the sorted
container. That's pretty hard to beat as it's the cost of a pointer to each
object. It's also 66% less overhead than a typical binary tree implementation
(e.g. Red-Black Tree, AVL-Tree, AA-Tree, Splay-Tree, Treap, etc.) for which
every node must also store two pointers to children nodes.

`Sorted Containers`_ takes all of the work out of Python sorted collections -
making your deployment and use of Python easy. There's no need to install a C
compiler or pre-build and distribute custom extensions. Performance is a
feature and testing has 100% coverage with unit tests and hours of stress.

.. _`Sorted Containers`: http://www.grantjenks.com/docs/sortedcontainers/
.. _`sorted collections library`: http://www.grantjenks.com/docs/sortedcontainers/

Testimonials
------------

**Alex Martelli**, `Fellow of the Python Software Foundation`_

"Good stuff! ... I like the `simple, effective implementation`_ idea of
splitting the sorted containers into smaller "fragments" to avoid the O(N)
insertion costs."

**Jeff Knupp**, `author of Writing Idiomatic Python and Python Trainer`_

"That last part, "fast as C-extensions," was difficult to believe. I would need
some sort of `Performance Comparison`_ to be convinced this is true. The author
includes this in the docs. It is."

**Kevin Samuel**, `Python and Django Trainer`_

I'm quite amazed, not just by the code quality (it's incredibly readable and
has more comment than code, wow), but the actual amount of work you put at
stuff that is *not* code: documentation, benchmarking, implementation
explanations. Even the git log is clean and the unit tests run out of the box
on Python 2 and 3.

**Mark Summerfield**, a short plea for `Python Sorted Collections`_

Python's "batteries included" standard library seems to have a battery
missing. And the argument that "we never had it before" has worn thin. It is
time that Python offered a full range of collection classes out of the box,
including sorted ones.

`Sorted Containers`_ is used in popular open source projects such as:
`Zipline`_, an algorithmic trading library from Quantopian; `Angr`_, a binary
analysis platform from UC Santa Barbara; `Trio`_, an async I/O library; and
`Dask Distributed`_, a distributed computation library supported by Continuum
Analytics.

.. _`Fellow of the Python Software Foundation`: https://en.wikipedia.org/wiki/Alex_Martelli
.. _`simple, effective implementation`: http://www.grantjenks.com/docs/sortedcontainers/implementation.html
.. _`author of Writing Idiomatic Python and Python Trainer`: https://jeffknupp.com/
.. _`Python and Django Trainer`: https://www.elephorm.com/formateur/kevin-samuel
.. _`Python Sorted Collections`: http://www.qtrac.eu/pysorted.html
.. _`Zipline`: https://github.com/quantopian/zipline
.. _`Angr`: https://github.com/angr/angr
.. _`Trio`: https://github.com/python-trio/trio
.. _`Dask Distributed`: https://github.com/dask/distributed

Features
--------

- Pure-Python
- Fully documented
- Benchmark comparison (alternatives, runtimes, load-factors)
- 100% test coverage
- Hours of stress testing
- Performance matters (often faster than C implementations)
- Compatible API (nearly identical to older blist and bintrees modules)
- Feature-rich (e.g. get the five largest keys in a sorted dict: d.keys()[-5:])
- Pragmatic design (e.g. SortedSet is a Python set with a SortedList index)
- Developed on Python 3.7
- Tested on CPython 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7 and PyPy, PyPy3

.. image:: https://api.travis-ci.org/grantjenks/python-sortedcontainers.svg?branch=master
   :target: http://www.grantjenks.com/docs/sortedcontainers/

.. image:: https://ci.appveyor.com/api/projects/status/github/grantjenks/python-sortedcontainers?branch=master&svg=true
   :target: http://www.grantjenks.com/docs/sortedcontainers/

Quickstart
----------

Installing `Sorted Containers`_ is simple with `pip
<https://pypi.org/project/pip/>`_::

    $ pip install sortedcontainers

You can access documentation in the interpreter with Python's built-in `help`
function. The `help` works on modules, classes and methods in `Sorted
Containers`_.

.. code-block:: python

    >>> import sortedcontainers
    >>> help(sortedcontainers)
    >>> from sortedcontainers import SortedDict
    >>> help(SortedDict)
    >>> help(SortedDict.popitem)

Documentation
-------------

Complete documentation for `Sorted Containers`_ is available at
http://www.grantjenks.com/docs/sortedcontainers/

User Guide
..........

The user guide provides an introduction to `Sorted Containers`_ and extensive
performance comparisons and analysis.

- `Introduction`_
- `Performance Comparison`_
- `Load Factor Performance Comparison`_
- `Runtime Performance Comparison`_
- `Simulated Workload Performance Comparison`_
- `Performance at Scale`_

.. _`Introduction`: http://www.grantjenks.com/docs/sortedcontainers/introduction.html
.. _`Performance Comparison`: http://www.grantjenks.com/docs/sortedcontainers/performance.html
.. _`Load Factor Performance Comparison`: http://www.grantjenks.com/docs/sortedcontainers/performance-load.html
.. _`Runtime Performance Comparison`: http://www.grantjenks.com/docs/sortedcontainers/performance-runtime.html
.. _`Simulated Workload Performance Comparison`: http://www.grantjenks.com/docs/sortedcontainers/performance-workload.html
.. _`Performance at Scale`: http://www.grantjenks.com/docs/sortedcontainers/performance-scale.html

Community Guide
...............

The community guide provides information on the development of `Sorted
Containers`_ along with support, implementation, and history details.

- `Development and Support`_
- `Implementation Details`_
- `Release History`_

.. _`Development and Support`: http://www.grantjenks.com/docs/sortedcontainers/development.html
.. _`Implementation Details`: http://www.grantjenks.com/docs/sortedcontainers/implementation.html
.. _`Release History`: http://www.grantjenks.com/docs/sortedcontainers/history.html

API Documentation
.................

The API documentation provides information on specific functions, classes, and
modules in the `Sorted Containers`_ package.

- `Sorted List`_
- `Sorted Dict`_
- `Sorted Set`_

.. _`Sorted List`: http://www.grantjenks.com/docs/sortedcontainers/sortedlist.html
.. _`Sorted Dict`: http://www.grantjenks.com/docs/sortedcontainers/sorteddict.html
.. _`Sorted Set`: http://www.grantjenks.com/docs/sortedcontainers/sortedset.html

Talks
-----

- `Python Sorted Collections | PyCon 2016 Talk`_
- `SF Python Holiday Party 2015 Lightning Talk`_
- `DjangoCon 2015 Lightning Talk`_

.. _`Python Sorted Collections | PyCon 2016 Talk`: http://www.grantjenks.com/docs/sortedcontainers/pycon-2016-talk.html
.. _`SF Python Holiday Party 2015 Lightning Talk`: http://www.grantjenks.com/docs/sortedcontainers/sf-python-2015-lightning-talk.html
.. _`DjangoCon 2015 Lightning Talk`: http://www.grantjenks.com/docs/sortedcontainers/djangocon-2015-lightning-talk.html

Resources
---------

- `Sorted Containers Documentation`_
- `Sorted Containers at PyPI`_
- `Sorted Containers at Github`_
- `Sorted Containers Issue Tracker`_

.. _`Sorted Containers Documentation`: http://www.grantjenks.com/docs/sortedcontainers/
.. _`Sorted Containers at PyPI`: https://pypi.org/project/sortedcontainers/
.. _`Sorted Containers at Github`: https://github.com/grantjenks/python-sortedcontainers
.. _`Sorted Containers Issue Tracker`: https://github.com/grantjenks/python-sortedcontainers/issues

Sorted Containers License
-------------------------

Copyright 2014-2019 Grant Jenks

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


"""^^xsd:string ;
    schema1:name "sortedcontainers"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.4.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/stack-data> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/asttokens>,
        <https://data.siemens-energy.com/cybersec/package/executing> ;
    schema1:description """# stack_data

[![Tests](https://github.com/alexmojaki/stack_data/actions/workflows/pytest.yml/badge.svg)](https://github.com/alexmojaki/stack_data/actions/workflows/pytest.yml) [![Coverage Status](https://coveralls.io/repos/github/alexmojaki/stack_data/badge.svg?branch=master)](https://coveralls.io/github/alexmojaki/stack_data?branch=master) [![Supports Python versions 3.5+](https://img.shields.io/pypi/pyversions/stack_data.svg)](https://pypi.python.org/pypi/stack_data)

This is a library that extracts data from stack frames and tracebacks, particularly to display more useful tracebacks than the default. It powers the tracebacks in IPython and [futurecoder](https://futurecoder.io/):

![futurecoder example](https://futurecoder.io/static/img/features/traceback.png)

You can install it from PyPI:

    pip install stack_data
    
## Basic usage

Here's some code we'd like to inspect:

```python
def foo():
    result = []
    for i in range(5):
        row = []
        result.append(row)
        print_stack()
        for j in range(5):
            row.append(i * j)
    return result
```

Note that `foo` calls a function `print_stack()`. In reality we can imagine that an exception was raised at this line, or a debugger stopped there, but this is easy to play with directly. Here's a basic implementation:

```python
import inspect
import stack_data


def print_stack():
    frame = inspect.currentframe().f_back
    frame_info = stack_data.FrameInfo(frame)
    print(f"{frame_info.code.co_name} at line {frame_info.lineno}")
    print("-----------")
    for line in frame_info.lines:
        print(f"{'-->' if line.is_current else '   '} {line.lineno:4} | {line.render()}")
```

(Beware that this has a major bug - it doesn't account for line gaps, which we'll learn about later)

The output of one call to `print_stack()` looks like:

```
foo at line 9
-----------
       6 | for i in range(5):
       7 |     row = []
       8 |     result.append(row)
-->    9 |     print_stack()
      10 |     for j in range(5):
```

The code for `print_stack()` is fairly self-explanatory. If you want to learn more details about a particular class or method I suggest looking through some docstrings. `FrameInfo` is a class that accepts either a frame or a traceback object and provides a bunch of nice attributes and properties (which are cached so you don't need to worry about performance). In particular `frame_info.lines` is a list of `Line` objects. `line.render()` returns the source code of that line suitable for display. Without any arguments it simply strips any common leading indentation. Later on we'll see a more powerful use for it.

You can see that `frame_info.lines` includes some lines of surrounding context. By default it includes 3 pieces of context before the main line and 1 piece after. We can configure the amount of context by passing options:

```python
options = stack_data.Options(before=1, after=0)
frame_info = stack_data.FrameInfo(frame, options)
```

Then the output looks like:

```
foo at line 9
-----------
       8 | result.append(row)
-->    9 | print_stack()
```

Note that these parameters are not the number of *lines* before and after to include, but the number of *pieces*. A piece is a range of one or more lines in a file that should logically be grouped together. A piece contains either a single simple statement or a part of a compound statement (loops, if, try/except, etc) that doesn't contain any other statements. Most pieces are a single line, but a multi-line statement or `if` condition is a single piece. In the example above, all pieces are one line, because nothing is spread across multiple lines. If we change our code to include some multiline bits:


```python
def foo():
    result = []
    for i in range(5):
        row = []
        result.append(
            row
        )
        print_stack()
        for j in range(
                5
        ):
            row.append(i * j)
    return result
```

and then run the original code with the default options, then the output is:

```
foo at line 11
-----------
       6 | for i in range(5):
       7 |     row = []
       8 |     result.append(
       9 |         row
      10 |     )
-->   11 |     print_stack()
      12 |     for j in range(
      13 |             5
      14 |     ):
```

Now lines 8-10 and lines 12-14 are each a single piece. Note that the output is essentially the same as the original in terms of the amount of code. The division of files into pieces means that the edge of the context is intuitive and doesn't crop out parts of statements or expressions. For example, if context was measured in lines instead of pieces, the last line of the above would be `for j in range(` which is much less useful.

However, if a piece is very long, including all of it could be cumbersome. For this, `Options` has a parameter `max_lines_per_piece`, which is 6 by default. Suppose we have a piece in our code that's longer than that:

```python
        row = [
            1,
            2,
            3,
            4,
            5,
        ]
```

`frame_info.lines` will truncate this piece so that instead of 7 `Line` objects it will produce 5 `Line` objects and one `LINE_GAP` in the middle, making 6 objects in total for the piece. Our code doesn't currently handle gaps, so it will raise an exception. We can modify it like so:

```python
    for line in frame_info.lines:
        if line is stack_data.LINE_GAP:
            print("       (...)")
        else:
            print(f"{'-->' if line.is_current else '   '} {line.lineno:4} | {line.render()}")
```

Now the output looks like:

```
foo at line 15
-----------
       6 | for i in range(5):
       7 |     row = [
       8 |         1,
       9 |         2,
       (...)
      12 |         5,
      13 |     ]
      14 |     result.append(row)
-->   15 |     print_stack()
      16 |     for j in range(5):
```

Alternatively, you can flip the condition around and check `if isinstance(line, stack_data.Line):`. Either way, you should always check for line gaps, or your code may appear to work at first but fail when it encounters a long piece.

Note that the executing piece, i.e. the piece containing the current line being executed (line 15 in this case) is never truncated, no matter how long it is.

The lines of context never stray outside `frame_info.scope`, which is the innermost function or class definition containing the current line. For example, this is the output for a short function which has neither 3 lines before nor 1 line after the current line:

```
bar at line 6
-----------
       4 | def bar():
       5 |     foo()
-->    6 |     print_stack()
```

Sometimes it's nice to ensure that the function signature is always showing. This can be done with `Options(include_signature=True)`. The result looks like this:

```
foo at line 14
-----------
       9 | def foo():
       (...)
      11 |     for i in range(5):
      12 |         row = []
      13 |         result.append(row)
-->   14 |         print_stack()
      15 |         for j in range(5):
```

To avoid wasting space, pieces never start or end with a blank line, and blank lines between pieces are excluded. So if our code looks like this:


```python
    for i in range(5):
        row = []

        result.append(row)
        print_stack()

        for j in range(5):
```

The output doesn't change much, except you can see jumps in the line numbers:

```
      11 |     for i in range(5):
      12 |         row = []
      14 |         result.append(row)
-->   15 |         print_stack()
      17 |         for j in range(5):
```

## Variables

You can also inspect variables and other expressions in a frame, e.g:

```python
    for var in frame_info.variables:
        print(f"{var.name} = {repr(var.value)}")
```

which may output:

```python
result = [[0, 0, 0, 0, 0], [0, 1, 2, 3, 4], [0, 2, 4, 6, 8], [0, 3, 6, 9, 12], []]
i = 4
row = []
j = 4
```

`frame_info.variables` returns a list of `Variable` objects, which have attributes `name`, `value`, and `nodes`, which is a list of all AST representing that expression.

A `Variable` may refer to an expression other than a simple variable name. It can be any expression evaluated by the library [`pure_eval`](https://github.com/alexmojaki/pure_eval) which it deems 'interesting' (see those docs for more info). This includes expressions like `foo.bar` or `foo[bar]`. In these cases `name` is the source code of that expression. `pure_eval` ensures that it only evaluates expressions that won't have any side effects, e.g. where `foo.bar` is a normal attribute rather than a descriptor such as a property.

`frame_info.variables` is a list of all the interesting expressions found in `frame_info.scope`, e.g. the current function, which may include expressions not visible in `frame_info.lines`. You can restrict the list by using `frame_info.variables_in_lines` or even `frame_info.variables_in_executing_piece`. For more control you can use `frame_info.variables_by_lineno`. See the docstrings for more information.

## Rendering lines with ranges and markers

Sometimes you may want to insert special characters into the text for display purposes, e.g. HTML or ANSI color codes. `stack_data` provides a few tools to make this easier.

Let's say we have a `Line` object where `line.text` (the original raw source code of that line) is `"foo = bar"`, so `line.text[6:9]` is `"bar"`, and we want to emphasise that part by inserting HTML at positions 6 and 9 in the text. Here's how we can do that directly:

```python
markers = [
    stack_data.MarkerInLine(position=6, is_start=True, string="<b>"),
    stack_data.MarkerInLine(position=9, is_start=False, string="</b>"),
]
line.render(markers)  # returns "foo = <b>bar</b>"
```

Here `is_start=True` indicates that the marker is the first of a pair. This helps `line.render()` sort and insert the markers correctly so you don't end up with malformed HTML like `foo<b>.<i></b>bar</i>` where tags overlap.

Since we're inserting HTML, we should actually use `line.render(markers, escape_html=True)` which will escape special HTML characters in the Python source (but not the markers) so for example `foo = bar < spam` would be rendered as `foo = <b>bar</b> &lt; spam`.

Usually though you wouldn't create markers directly yourself. Instead you would start with one or more ranges and then convert them, like so:

```python
ranges = [
    stack_data.RangeInLine(start=0, end=3, data="foo"),
    stack_data.RangeInLine(start=6, end=9, data="bar"),
]

def convert_ranges(r):
    if r.data == "bar":
        return "<b>", "</b>"        

# This results in `markers` being the same as in the above example.
markers = stack_data.markers_from_ranges(ranges, convert_ranges)
```

`RangeInLine` has a `data` attribute which can be any object. `markers_from_ranges` accepts a converter function to which it passes all the `RangeInLine` objects. If the converter function returns a pair of strings, it creates two markers from them. Otherwise it should return `None` to indicate that the range should be ignored, as with the first range containing `"foo"` in this example.

The reason this is useful is because there are built in tools to create these ranges for you. For example, if we change our `print_stack()` function to contain this:

```python
def convert_variable_ranges(r):
    variable, _node = r.data
    return f'<span data-value="{repr(variable.value)}">', '</span>'

markers = stack_data.markers_from_ranges(line.variable_ranges, convert_variable_ranges)
print(f"{'-->' if line.is_current else '   '} {line.lineno:4} | {line.render(markers, escape_html=True)}")
```

Then the output becomes:

```
foo at line 15
-----------
       9 | def foo():
       (...)
      11 |     for <span data-value="4">i</span> in range(5):
      12 |         <span data-value="[]">row</span> = []
      14 |         <span data-value="[[0, 0, 0, 0, 0], [0, 1, 2, 3, 4], [0, 2, 4, 6, 8], [0, 3, 6, 9, 12], []]">result</span>.append(<span data-value="[]">row</span>)
-->   15 |         print_stack()
      17 |         for <span data-value="4">j</span> in range(5):
```

`line.variable_ranges` is a list of RangeInLines for each Variable that appears at least partially in this line. The data attribute of the range is a pair `(variable, node)` where node is the particular AST node from the list `variable.nodes` that corresponds to this range.

You can also use `line.token_ranges` (e.g. if you want to do your own syntax highlighting) or `line.executing_node_ranges` if you want to highlight the currently executing node identified by the [`executing`](https://github.com/alexmojaki/executing) library. Or if you want to make your own range from an AST node, use `line.range_from_node(node, data)`. See the docstrings for more info.

### Syntax highlighting with Pygments

If you'd like pretty colored text without the work, you can let [Pygments](https://pygments.org/) do it for you. Just follow these steps:

1. `pip install pygments` separately as it's not a dependency of `stack_data`.
2. Create a pygments formatter object such as `HtmlFormatter` or `Terminal256Formatter`.
3. Pass the formatter to `Options` in the argument `pygments_formatter`.
4. Use `line.render(pygmented=True)` to get your formatted text. In this case you can't pass any markers to `render`.

If you want, you can also highlight the executing node in the frame in combination with the pygments syntax highlighting. For this you will need:

1. A pygments style - either a style class or a string that names it. See the [documentation on styles](https://pygments.org/docs/styles/) and the [styles gallery](https://blog.yjl.im/2015/08/pygments-styles-gallery.html).
2. A modification to make to the style for the executing node, which is a string such as `"bold"` or `"bg:#ffff00"` (yellow background). See the [documentation on style rules](https://pygments.org/docs/styles/#style-rules).
3. Pass these two things to `stack_data.style_with_executing_node(style, modifier)` to get a new style class.
4. Pass the new style to your formatter when you create it.

Note that this doesn't work with `TerminalFormatter` which just uses the basic ANSI colors and doesn't use the style passed to it in general.

## Getting the full stack

Currently `print_stack()` doesn't actually print the stack, it just prints one frame. Instead of `frame_info = FrameInfo(frame, options)`, let's do this:

```python
for frame_info in FrameInfo.stack_data(frame, options):
```

Now the output looks something like this:

```
<module> at line 18
-----------
      14 |         for j in range(5):
      15 |             row.append(i * j)
      16 |     return result
-->   18 | bar()

bar at line 5
-----------
       4 | def bar():
-->    5 |     foo()

foo at line 13
-----------
      10 | for i in range(5):
      11 |     row = []
      12 |     result.append(row)
-->   13 |     print_stack()
      14 |     for j in range(5):
```

However, just as `frame_info.lines` doesn't always yield `Line` objects, `FrameInfo.stack_data` doesn't always yield `FrameInfo` objects, and we must modify our code to handle that. Let's look at some different sample code:

```python
def factorial(x):
    return x * factorial(x - 1)


try:
    print(factorial(5))
except:
    print_stack()
```

In this code we've forgotten to include a base case in our `factorial` function so it will fail with a `RecursionError` and there'll be many frames with similar information. Similar to the built in Python traceback, `stack_data` avoids showing all of these frames. Instead you will get a `RepeatedFrames` object which summarises the information. See its docstring for more details.

Here is our updated implementation:

```python
def print_stack():
    for frame_info in FrameInfo.stack_data(sys.exc_info()[2]):
        if isinstance(frame_info, FrameInfo):
            print(f"{frame_info.code.co_name} at line {frame_info.lineno}")
            print("-----------")
            for line in frame_info.lines:
                print(f"{'-->' if line.is_current else '   '} {line.lineno:4} | {line.render()}")

            for var in frame_info.variables:
                print(f"{var.name} = {repr(var.value)}")

            print()
        else:
            print(f"... {frame_info.description} ...\\n")
```

And the output:

```
<module> at line 9
-----------
       4 | def factorial(x):
       5 |     return x * factorial(x - 1)
       8 | try:
-->    9 |     print(factorial(5))
      10 | except:

factorial at line 5
-----------
       4 | def factorial(x):
-->    5 |     return x * factorial(x - 1)
x = 5

factorial at line 5
-----------
       4 | def factorial(x):
-->    5 |     return x * factorial(x - 1)
x = 4

... factorial at line 5 (996 times) ...

factorial at line 5
-----------
       4 | def factorial(x):
-->    5 |     return x * factorial(x - 1)
x = -993
```

In addition to handling repeated frames, we've passed a traceback object to `FrameInfo.stack_data` instead of a frame.

If you want, you can pass `collapse_repeated_frames=False` to `FrameInfo.stack_data` (not to `Options`) and it will just yield `FrameInfo` objects for the full stack.
"""^^xsd:string ;
    schema1:name "stack-data"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.6.3"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/sw360> a software_vocab:Software_package ;
    schema1:description """<!--
# SPDX-FileCopyrightText: (c) 2019-2024 Siemens
# SPDX-License-Identifier: MIT
-->

# SW360 Base Library for Python

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/sw360/sw360python/blob/master/License.md)
[![Python Version](https://img.shields.io/badge/python-3.8%2C3.9%2C3.10%2C3.11-yellow?logo=python)](https://www.python.org/doc/versions/)
[![PyPI](https://shields.io/pypi/v/sw360)](https://pypi.org/project/sw360/)
[![Static checks](https://github.com/sw360/sw360python/actions/workflows/static-checks.yml/badge.svg)](https://github.com/sw360/sw360python/actions/workflows/static-checks.yml)
[![Unit tests](https://github.com/sw360/sw360python/actions/workflows/unit-test.yml/badge.svg)](https://github.com/sw360/sw360python/actions/workflows/unit-test.yml)
[![Coverage](https://img.shields.io/endpoint?url=https://gist.githubusercontent.com/tngraf/6ab639b6f9d1f6161d3db52d348d2997/raw/666fa870981726e1fa3469b6aa668c20fdd9d1b2/sw360python-cobertura-coverage.json&color=green)](https://github.com/sw360/sw360python/actions/workflows/unit-test.yml)
[![REUSE status](https://api.reuse.software/badge/git.fsfe.org/reuse/api)](https://api.reuse.software/info/git.fsfe.org/reuse/api)

This Python project implements the REST API of [SW360](https://www.eclipse.org/sw360/)
and allows an easy way to interact with SW360.

## Documentation

Have a look at the documentation: https://sw360.github.io/sw360python/

## Usage

### Installation

This project is available as [Python package on PyPi.org](https://pypi.org/project/sw360/).  
Install sw360 and required dependencies:

```shell
  pip install sw360 requests
  ```

### Using the API

* Get a REST API token from your SW360 server
* Export required environment variables (optionally but recommended):

  ```shell
  export SW360ProductionToken=<your_api_token>
  ```

* Start using the API:

  ```python
  import sw360
  client = sw360.SW360(sw360_url, sw360_api_token)
  ```

### Contribute

* All contributions in form of bug reports, feature requests or merge requests!
* Use proper [docstrings](https://realpython.com/documenting-python-code/) to document  
  functions and classes
* Extend the testsuite **poetry run pytest** with the new functions/classes
* The **documentation website** can automatically be generated by the [Sphinx autodoc extension](https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html)

### Build

#### Building the Documentation

The documentation of the project is built using Sphinx:

```python
poetry run sphinx-build .\\docs-source\\ .\\docs\\
```

#### Building Python package

For building the library, you need [Poetry](https://python-poetry.org/).  
The build is then triggered using

```shell
poetry build
```

This creates the source and wheel files in ```dist/``` subdirectory -- which can then be  
uploaded or installed locally using ```pip```.

## Test

Start the complete test suite or a specific test case (and generate coverage report):

```shell
poetry run pytest
```

or

```shell
poetry run coverage run -m pytest
poetry run coverage report -m --omit "*/site-packages/*.py"
poetry run coverage html --omit "*/site-packages/*.py"
```

## Demo

The script ``check_project.py`` shows how to use the library to retrieve some information  
of a given project on SW360. This requires colorama>=0.4.1.

## License

Copyright 2019-2024 Siemens

The project is licensed under the MIT license.
SPDX-License-Identifier: MIT

"""^^xsd:string ;
    schema1:name "sw360"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.5.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/toml> a software_vocab:Software_package ;
    schema1:description """****
TOML
****

.. image:: https://img.shields.io/pypi/v/toml
    :target: https://pypi.org/project/toml/

.. image:: https://travis-ci.org/uiri/toml.svg?branch=master
    :target: https://travis-ci.org/uiri/toml

.. image:: https://img.shields.io/pypi/pyversions/toml.svg
    :target: https://pypi.org/project/toml/


A Python library for parsing and creating `TOML <https://en.wikipedia.org/wiki/TOML>`_.

The module passes `the TOML test suite <https://github.com/BurntSushi/toml-test>`_.

See also:

* `The TOML Standard <https://github.com/toml-lang/toml>`_
* `The currently supported TOML specification <https://github.com/toml-lang/toml/blob/v0.5.0/README.md>`_

Installation
============

To install the latest release on `PyPI <https://pypi.org/project/toml/>`_,
simply run:

::

  pip install toml

Or to install the latest development version, run:

::

  git clone https://github.com/uiri/toml.git
  cd toml
  python setup.py install

Quick Tutorial
==============

*toml.loads* takes in a string containing standard TOML-formatted data and
returns a dictionary containing the parsed data.

.. code:: pycon

  >>> import toml
  >>> toml_string = \"\"\"
  ... # This is a TOML document.
  ...
  ... title = "TOML Example"
  ...
  ... [owner]
  ... name = "Tom Preston-Werner"
  ... dob = 1979-05-27T07:32:00-08:00 # First class dates
  ...
  ... [database]
  ... server = "192.168.1.1"
  ... ports = [ 8001, 8001, 8002 ]
  ... connection_max = 5000
  ... enabled = true
  ...
  ... [servers]
  ...
  ...   # Indentation (tabs and/or spaces) is allowed but not required
  ...   [servers.alpha]
  ...   ip = "10.0.0.1"
  ...   dc = "eqdc10"
  ...
  ...   [servers.beta]
  ...   ip = "10.0.0.2"
  ...   dc = "eqdc10"
  ...
  ... [clients]
  ... data = [ ["gamma", "delta"], [1, 2] ]
  ...
  ... # Line breaks are OK when inside arrays
  ... hosts = [
  ...   "alpha",
  ...   "omega"
  ... ]
  ... \"\"\"
  >>> parsed_toml = toml.loads(toml_string)


*toml.dumps* takes a dictionary and returns a string containing the
corresponding TOML-formatted data.

.. code:: pycon

  >>> new_toml_string = toml.dumps(parsed_toml)
  >>> print(new_toml_string)
  title = "TOML Example"
  [owner]
  name = "Tom Preston-Werner"
  dob = 1979-05-27T07:32:00Z
  [database]
  server = "192.168.1.1"
  ports = [ 8001, 8001, 8002,]
  connection_max = 5000
  enabled = true
  [clients]
  data = [ [ "gamma", "delta",], [ 1, 2,],]
  hosts = [ "alpha", "omega",]
  [servers.alpha]
  ip = "10.0.0.1"
  dc = "eqdc10"
  [servers.beta]
  ip = "10.0.0.2"
  dc = "eqdc10"

*toml.dump* takes a dictionary and a file descriptor and returns a string containing the
corresponding TOML-formatted data.

.. code:: pycon

  >>> with open('new_toml_file.toml', 'w') as f:
  ...     new_toml_string = toml.dump(parsed_toml, f)
  >>> print(new_toml_string)
  title = "TOML Example"
  [owner]
  name = "Tom Preston-Werner"
  dob = 1979-05-27T07:32:00Z
  [database]
  server = "192.168.1.1"
  ports = [ 8001, 8001, 8002,]
  connection_max = 5000
  enabled = true
  [clients]
  data = [ [ "gamma", "delta",], [ 1, 2,],]
  hosts = [ "alpha", "omega",]
  [servers.alpha]
  ip = "10.0.0.1"
  dc = "eqdc10"
  [servers.beta]
  ip = "10.0.0.2"
  dc = "eqdc10"

For more functions, view the API Reference below.

Note
----

For Numpy users, by default the data types ``np.floatX`` will not be translated to floats by toml, but will instead be encoded as strings. To get around this, specify the ``TomlNumpyEncoder`` when saving your data.

.. code:: pycon

  >>> import toml
  >>> import numpy as np
  >>> a = np.arange(0, 10, dtype=np.double)
  >>> output = {'a': a}
  >>> toml.dumps(output)
  'a = [ "0.0", "1.0", "2.0", "3.0", "4.0", "5.0", "6.0", "7.0", "8.0", "9.0",]\\n'
  >>> toml.dumps(output, encoder=toml.TomlNumpyEncoder())
  'a = [ 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0,]\\n'

API Reference
=============

``toml.load(f, _dict=dict)``
  Parse a file or a list of files as TOML and return a dictionary.

  :Args:
    * ``f``: A path to a file, list of filepaths (to be read into single
      object) or a file descriptor
    * ``_dict``: The class of the dictionary object to be returned

  :Returns:
    A dictionary (or object ``_dict``) containing parsed TOML data

  :Raises:
    * ``TypeError``: When ``f`` is an invalid type or is a list containing
      invalid types
    * ``TomlDecodeError``: When an error occurs while decoding the file(s)

``toml.loads(s, _dict=dict)``
  Parse a TOML-formatted string to a dictionary.

  :Args:
    * ``s``: The TOML-formatted string to be parsed
    * ``_dict``: Specifies the class of the returned toml dictionary

  :Returns:
    A dictionary (or object ``_dict``) containing parsed TOML data

  :Raises:
    * ``TypeError``: When a non-string object is passed
    * ``TomlDecodeError``: When an error occurs while decoding the
      TOML-formatted string

``toml.dump(o, f, encoder=None)``
  Write a dictionary to a file containing TOML-formatted data

  :Args:
    * ``o``: An object to be converted into TOML
    * ``f``: A File descriptor where the TOML-formatted output should be stored
    * ``encoder``: An instance of ``TomlEncoder`` (or subclass) for encoding the object. If ``None``, will default to ``TomlEncoder``

  :Returns:
    A string containing the TOML-formatted data corresponding to object ``o``

  :Raises:
    * ``TypeError``: When anything other than file descriptor is passed

``toml.dumps(o, encoder=None)``
  Create a TOML-formatted string from an input object

  :Args:
    * ``o``: An object to be converted into TOML
    * ``encoder``: An instance of ``TomlEncoder`` (or subclass) for encoding the object. If ``None``, will default to ``TomlEncoder``

  :Returns:
    A string containing the TOML-formatted data corresponding to object ``o``



Licensing
=========

This project is released under the terms of the MIT Open Source License. View
*LICENSE.txt* for more information.


"""^^xsd:string ;
    schema1:name "toml"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.10.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/types-setuptools> a software_vocab:Software_package ;
    schema1:description """## Typing stubs for setuptools

This is a [PEP 561](https://peps.python.org/pep-0561/)
type stub package for the [`setuptools`](https://github.com/pypa/setuptools) package.
It can be used by type-checking tools like
[mypy](https://github.com/python/mypy/),
[pyright](https://github.com/microsoft/pyright),
[pytype](https://github.com/google/pytype/),
PyCharm, etc. to check code that uses
`setuptools`.

This version of `types-setuptools` aims to provide accurate annotations
for `setuptools==73.0.*`.
The source for this package can be found at
https://github.com/python/typeshed/tree/main/stubs/setuptools. All fixes for
types and metadata should be contributed there.

If using `setuptools >= 71.1` *only* for `pkg_resources`,
you don't need `types-setuptools` since `pkg_resources` is now typed.

See https://github.com/python/typeshed/blob/main/README.md for more details.
This package was generated from typeshed commit
[`7865a78de1929ee54797baca0fe07ac33567739f`](https://github.com/python/typeshed/commit/7865a78de1929ee54797baca0fe07ac33567739f) and was tested
with mypy 1.11.1, pyright 1.1.377, and
pytype 2024.4.11.
"""^^xsd:string ;
    schema1:name "types-setuptools"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "73.0.0.20240822"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/typing_extensions> a software_vocab:Software_package ;
    schema1:description """# Typing Extensions

[![Chat at https://gitter.im/python/typing](https://badges.gitter.im/python/typing.svg)](https://gitter.im/python/typing)

[Documentation](https://typing-extensions.readthedocs.io/en/latest/#) â€“
[PyPI](https://pypi.org/project/typing-extensions/)

## Overview

The `typing_extensions` module serves two related purposes:

- Enable use of new type system features on older Python versions. For example,
  `typing.TypeGuard` is new in Python 3.10, but `typing_extensions` allows
  users on previous Python versions to use it too.
- Enable experimentation with new type system PEPs before they are accepted and
  added to the `typing` module.

`typing_extensions` is treated specially by static type checkers such as
mypy and pyright. Objects defined in `typing_extensions` are treated the same
way as equivalent forms in `typing`.

`typing_extensions` uses
[Semantic Versioning](https://semver.org/). The
major version will be incremented only for backwards-incompatible changes.
Therefore, it's safe to depend
on `typing_extensions` like this: `typing_extensions >=x.y, <(x+1)`,
where `x.y` is the first version that includes all features you need.

## Included items

See [the documentation](https://typing-extensions.readthedocs.io/en/latest/#) for a
complete listing of module contents.

## Contributing

See [CONTRIBUTING.md](https://github.com/python/typing_extensions/blob/main/CONTRIBUTING.md)
for how to contribute to `typing_extensions`.

"""^^xsd:string ;
    schema1:name "typing_extensions"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "4.12.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/tzlocal> a software_vocab:Software_package ;
    schema1:description """tzlocal
=======

API CHANGE!
-----------

With version 3.0 of tzlocal, tzlocal no longer returned `pytz` objects, but
`zoneinfo` objects, which has a different API. Since 4.0, it now restored
partial compatibility for `pytz` users through Paul Ganssle's
`pytz_deprecation_shim`.

tzlocal 4.0 also adds an official function `get_localzone_name()` to get only
the timezone name, instead of a timezone object. On unix, it can raise an
error if you don't have a timezone name configured, where `get_localzone()`
will succeed, so only use that if you need the timezone name.

4.0 also adds way more information on what is going wrong in your
configuration when the configuration files are unclear or contradictory.

Version 5.0 removes the `pytz_deprecation_shim`, and now only returns
`zoneinfo` objects, like verion 3.0 did. If you need `pytz` objects, you have
to stay on version 4.0. If there are bugs in version 4.0, I will release
updates, but there will be no further functional changes on the 4.x branch.


Info
----

This Python module returns the `IANA time zone name
<https://www.iana.org/time-zones>`_ for your local time zone or a ``tzinfo``
object with the local timezone information, under Unix and Windows.

It requires Python 3.8 or later, and will use the ``backports.tzinfo``
package, for Python 3.8.

This module attempts to fix a glaring hole in the ``pytz`` and ``zoneinfo``
modules, that there is no way to get the local timezone information, unless
you know the zoneinfo name, and under several Linux distros that's hard or
impossible to figure out.

With ``tzlocal`` you only need to call ``get_localzone()`` and you will get a
``tzinfo`` object with the local time zone info. On some Unices you will
still not get to know what the timezone name is, but you don't need that when
you have the tzinfo file. However, if the timezone name is readily available
it will be used.

What it's not for
-----------------

It's not for converting the current time between UTC and your local time. There are
other, simpler ways of doing this. This is ig you need to know things like the name
of the time zone, or if you need to be able to convert between your time zone and
another time zone for times that are in the future or in the past.

For current time conversions to and from UTC, look in the Python ``time`` module.


Supported systems
-----------------

These are the systems that are in theory supported:

 * Windows 2000 and later

 * Any unix-like system with a ``/etc/localtime`` or ``/usr/local/etc/localtime``

If you have one of the above systems and it does not work, it's a bug.
Please report it.

Please note that if you are getting a time zone called ``local``, this is not
a bug, it's actually the main feature of ``tzlocal``, that even if your
system does NOT have a configuration file with the zoneinfo name of your time
zone, it will still work.

You can also use ``tzlocal`` to get the name of your local timezone, but only
if your system is configured to make that possible. ``tzlocal`` looks for the
timezone name in ``/etc/timezone``, ``/var/db/zoneinfo``,
``/etc/sysconfig/clock`` and ``/etc/conf.d/clock``. If your
``/etc/localtime`` is a symlink it can also extract the name from that
symlink.

If you need the name of your local time zone, then please make sure your
system is properly configured to allow that.

If your unix system doesn't have a timezone configured, tzlocal will default
to UTC.

Notes on Docker
---------------

It turns out that Docker images frequently have broken timezone setups.
This usually resuts in a warning that the configuration is wrong, or that
the timezone offset doesn't match the found timezone.

The easiest way to fix that is to set a TZ variable in your docker setup
to whatever timezone you want, which is usually the timezone your host
computer has.

Usage
-----

Load the local timezone:

    >>> from tzlocal import get_localzone
    >>> tz = get_localzone()
    >>> tz
    zoneinfo.ZoneInfo(key='Europe/Warsaw')

Create a local datetime:

    >>> from datetime import datetime
    >>> dt = datetime(2015, 4, 10, 7, 22, tzinfo=tz)
    >>> dt
    datetime.datetime(2015, 4, 10, 7, 22, tzinfo=zoneinfo.ZoneInfo(key='Europe/Warsaw'))

Lookup another timezone with ``zoneinfo`` (``backports.zoneinfo`` on Python 3.8 or earlier):

    >>> from zoneinfo import ZoneInfo
    >>> eastern = ZoneInfo('US/Eastern')

Convert the datetime:

    >>> dt.astimezone(eastern)
    datetime.datetime(2015, 4, 10, 1, 22, tzinfo=zoneinfo.ZoneInfo(key='US/Eastern'))

If you just want the name of the local timezone, use `get_localzone_name()`:

    >>> from tzlocal import get_localzone_name
    >>> get_localzone_name()
    "Europe/Warsaw"

Please note that under Unix, `get_localzone_name()` may fail if there is no zone
configured, where `get_localzone()` would generally succeed.

Troubleshooting
---------------

If you don't get the result you expect, try running it with debugging turned on.
Start a python interpreter that has tzlocal installed, and run the following code::

    import logging
    logging.basicConfig(level="DEBUG")
    import tzlocal
    tzlocal.get_localzone()

The output should look something like this, and this will tell you what
configurations were found::

    DEBUG:root:/etc/timezone found, contents:
     Europe/Warsaw

    DEBUG:root:/etc/localtime found
    DEBUG:root:2 found:
     {'/etc/timezone': 'Europe/Warsaw', '/etc/localtime is a symlink to': 'Europe/Warsaw'}
    zoneinfo.ZoneInfo(key='Europe/Warsaw')


Development
-----------

For ease of development, there is a Makefile that will help you with basic tasks,
like creating a development environment with all the necessary tools (although
you need a supported Python version installed first)::

    $ make devenv

To run tests::

    $ make test

Check the syntax::

    $ make check


Maintainer
----------

* Lennart Regebro, regebro@gmail.com

Contributors
------------

* Marc Van Olmen
* Benjamen Meyer
* Manuel Ebert
* Xiaokun Zhu
* Cameris
* Edward Betts
* McK KIM
* Cris Ewing
* Ayala Shachar
* Lev Maximov
* Jakub Wilk
* John Quarles
* Preston Landers
* Victor Torres
* Jean Jordaan
* Zackary Welch
* MickaÃ«l Schoentgen
* Gabriel Corona
* Alex GrÃ¶nholm
* Julin S
* Miroslav Å edivÃ½
* revansSZ
* Sam Treweek
* Peter Di Pasquale
* Rongrong

(Sorry if I forgot someone)

License
-------

* MIT https://opensource.org/licenses/MIT
"""^^xsd:string ;
    schema1:name "tzlocal"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "5.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/uri-template> a software_vocab:Software_package ;
    schema1:description """# uri-template

An implementation of RFC 6570 URI Templates.

This packages implements URI Template expansion in strict adherence to RFC 6570,
but adds a few extensions.

## RFC 6570 Extensions

### Non-string Values

RFC 6570 is silent regarding variable values that are not strings, lists, associative arrays, or null.

This package handles value types as follows:

  * Values that are instances of `str` are treated as strings.
  * Values implementing `collections.abc.Sequence` are treated as lists.
  * Values implementing `collections.abc.Mapping` are treated as associative arrays.
  * `None` values are treated as null.
  * Boolean values are converted to the lower case strings 'true' and 'false'.
  * All other values will be converted to strings using the Python `str()` function.

### Nested Structures

This package handles variable values with nested structure,
for example, lists containing other lists or associative arrays,
or associative arrays containing lists or other associative arrays.

Nested values for variables that do not use the array modifier ('[]') are treated as follows:

  * Lists containing lists are flattened into a single list.
  * Lists containing associative arrays are treated as a single combined associative array.
  * Associative arrays represent nested data using dot notation (".") for the variable names.

Nested values for variables that use the array modifier extend the variable name with 
the value's index or key written as an array subscript, e.g. "foo[0]" or "foo[bar]".

### Default Values

This package allows default string values for variables per early drafts of RFC 6570.
e.g. "{foo=bar}" will expand to "bar" if a value for `foo` is not given.

List and associtative array default values are not supported at this time.

### Specifying Value Keys

Sometimes a URI Template is used to provide glue between an API and a given set of data.
In this case, the names of values needed in the final URL may not match the data provided 
for the expansion.

This package allows specifying the key used to pass data into the template. 
e.g. "{?foo/bar}" will expand to "?foo=<the value provided as bar>"

### Partial expansion

This package allows partial expansion of URI Templates.

In a partial expansion, missing values preseve their expansion in the resultant output.
e.g. a partial expansion of "{one}/{two}" with a value for `one` of "foo" and `two` missing will result in:
"foo/{two}".

In order to allow partial expansions to preserve value joiners with expanded output,
expansions accept an optional "trailing joiner" of ",", ".", "/", ";", or "&",
if this joiner is present after all variables, 
it will be appended to the output of the expansion and will suppress the output prefix.
e.g.: "{#one,two}" with a missing value for `one` and a value of "bar" for `two`, 
will partially expand to: "#{#one,}bar", which when provided with a value of "foo" for `one` 
will expand to "#foo,bar"

Some partial expansions that have some output, but have missing values, 
will convert the remaining variables to a different type of expansion so that 
further expansions will produce the same output as if all values were originally present.

   * Partial Simple String Expansions will convert to Comma Expansions.
   * Partial Reserved Expansions Partial Fragment Expansions will convert to Reserved Comma Expansions.
   * Partial Form-Style Query Expansions will convert to Form-Style Query Continuations.

In order to preserve the resultant value of templates that are paritally expanded, 
the following additional Expression Expansions are supported:

#### Comma Expansion: {,var}

Similar to Label Expansion with Dot-Prefix, 
Comma Expansion prefixes the expansion output with a single comma ",".

#### Reserved Comma Expansion: {,+var}

Similar to Comma Expansion, 
Reserved Comma Expansion prefixes the expansion output with a single comma ",",
but otherwise performs a Reserved Expansion ({+var}).

## API 

The package provides three functions:

#### uri_template.expand(template: str, **kwargs) -> (str | None): ...

Expand the given template, skipping missing values per RFC 6570.

Returns `None` if the template is invalid or expansion fails.


#### uri_template.partial(template: str, **kwargs) -> (str | None): ...

Partially expand the given template, 
replacing missing variables with further expansions.

Returns `None` if the template is invalid or expansion fails.


#### uri_template.validate(template: str) -> bool: ...

Return `True` if the template is valid.

---

And the following classes:

### uri_template.URITemplate

#### URITemplate(template: str)

Construct a URITemplate for a given template string.

Raises `ExpansionInvalid`, `ExpansionReserved`, or `VariableInvalid` if the template is invalid or unsupported.

#### URITemplate.variables: Iterable[Variable]

All variables present in the template.
Duplicates are returned once, order is preserved.

#### URITemplate.variable_names: Iterable[str]

The names of all variables present in the template.
Duplicates are returned once, order is preserved.

#### URITemplate.expanded: bool

Determine if template is fully expanded.

#### URITemplate.expand(**kwargs) -> str

Returns the result of the expansion, skips missing variables.

Raises `ExpansionFailed` if the expansion fails due to a composite value being passed to a variable with a prefix modifier.

#### URITemplate.partial(**kwargs) -> URITemplate

Expand the template, replacing missing variables with further expansions.

Raises `ExpansionFailed` if the expansion fails due to a composite value being passed to a variable with a prefix modifier.

#### URITemplate.__str__() -> str

Convert the URITemplate object back into its original string form.

---

### uri_template.Variable

#### Variable(var_spec: str)

Construct a Variable.

#### Variable.name: str

The name of the variable

#### Variable.max_length: int

The speicified max length, or `0`.

#### Variable.explode: bool

Explode modifier is present.

#### Variable.array: bool

Array modifier is present.

#### Variable.default: (str | None)

Specified default value, or `None`.

#### Variable.__str__() -> str

Convert the variable back to its original string form.

---

And the following exceptions:

#### uri_template.ExpansionInvalid

Expansion specification is invalid. 

Raised by URITemplate constructor.

#### uri_template.ExpansionReserved

Expansion contains a reserved operator.

Raised by URITemplate constructor.

#### uri_template.VariableInvalid

Variable specification is invalid.

Raised by URITemplate constructor.

#### uri_template.ExpansionFailed

Expansion failed, currently only possible when a composite value is passed to a variable with a prefix modifier.

Raised by URITemplate.expand() or URITemplate.partial() methods.


## Installation

Install with pip:

    pip install uri-template
"""^^xsd:string ;
    schema1:name "uri-template"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.3.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/wcwidth> a software_vocab:Software_package ;
    schema1:description """|pypi_downloads| |codecov| |license|

============
Introduction
============

This library is mainly for CLI programs that carefully produce output for
Terminals, or make pretend to be an emulator.

**Problem Statement**: The printable length of *most* strings are equal to the
number of cells they occupy on the screen ``1 character : 1 cell``.  However,
there are categories of characters that *occupy 2 cells* (full-wide), and
others that *occupy 0* cells (zero-width).

**Solution**: POSIX.1-2001 and POSIX.1-2008 conforming systems provide
`wcwidth(3)`_ and `wcswidth(3)`_ C functions of which this python module's
functions precisely copy.  *These functions return the number of cells a
unicode string is expected to occupy.*

Installation
------------

The stable version of this package is maintained on pypi, install using pip::

    pip install wcwidth

Example
-------

**Problem**: given the following phrase (Japanese),

   >>>  text = u'ã‚³ãƒ³ãƒ‹ãƒãƒ'

Python **incorrectly** uses the *string length* of 5 codepoints rather than the
*printable length* of 10 cells, so that when using the `rjust` function, the
output length is wrong::

    >>> print(len('ã‚³ãƒ³ãƒ‹ãƒãƒ'))
    5

    >>> print('ã‚³ãƒ³ãƒ‹ãƒãƒ'.rjust(20, '_'))
    _______________ã‚³ãƒ³ãƒ‹ãƒãƒ

By defining our own "rjust" function that uses wcwidth, we can correct this::

   >>> def wc_rjust(text, length, padding=' '):
   ...    from wcwidth import wcswidth
   ...    return padding * max(0, (length - wcswidth(text))) + text
   ...

Our **Solution** uses wcswidth to determine the string length correctly::

   >>> from wcwidth import wcswidth
   >>> print(wcswidth('ã‚³ãƒ³ãƒ‹ãƒãƒ'))
   10

   >>> print(wc_rjust('ã‚³ãƒ³ãƒ‹ãƒãƒ', 20, '_'))
   __________ã‚³ãƒ³ãƒ‹ãƒãƒ


Choosing a Version
------------------

Export an environment variable, ``UNICODE_VERSION``. This should be done by
*terminal emulators* or those developers experimenting with authoring one of
their own, from shell::

   $ export UNICODE_VERSION=13.0

If unspecified, the latest version is used. If your Terminal Emulator does not
export this variable, you can use the `jquast/ucs-detect`_ utility to
automatically detect and export it to your shell.

wcwidth, wcswidth
-----------------
Use function ``wcwidth()`` to determine the length of a *single unicode
character*, and ``wcswidth()`` to determine the length of many, a *string
of unicode characters*.

Briefly, return values of function ``wcwidth()`` are:

``-1``
  Indeterminate (not printable).

``0``
  Does not advance the cursor, such as NULL or Combining.

``2``
  Characters of category East Asian Wide (W) or East Asian
  Full-width (F) which are displayed using two terminal cells.

``1``
  All others.

Function ``wcswidth()`` simply returns the sum of all values for each character
along a string, or ``-1`` when it occurs anywhere along a string.

Full API Documentation at https://wcwidth.readthedocs.org

==========
Developing
==========

Install wcwidth in editable mode::

   pip install -e .

Execute unit tests using tox_::

   tox -e py27,py35,py36,py37,py38,py39,py310,py311,py312

Updating Unicode Version
------------------------

Regenerate python code tables from latest Unicode Specification data files::

   tox -e update

The script is located at ``bin/update-tables.py``, requires Python 3.9 or
later. It is recommended but not necessary to run this script with the newest
Python, because the newest Python has the latest ``unicodedata`` for generating
comments.

Building Documentation
----------------------

This project is using `sphinx`_ 4.5 to build documentation::

   tox -e sphinx

The output will be in ``docs/_build/html/``.

Updating Requirements
---------------------

This project is using `pip-tools`_ to manage requirements.

To upgrade requirements for updating unicode version, run::

   tox -e update_requirements_update

To upgrade requirements for testing, run::

   tox -e update_requirements37,update_requirements39

To upgrade requirements for building documentation, run::

   tox -e update_requirements_docs

Utilities
---------

Supplementary tools for browsing and testing terminals for wide unicode
characters are found in the `bin/`_ of this project's source code.  Just ensure
to first ``pip install -r requirements-develop.txt`` from this projects main
folder. For example, an interactive browser for testing::

  python ./bin/wcwidth-browser.py

====
Uses
====

This library is used in:

- `jquast/blessed`_: a thin, practical wrapper around terminal capabilities in
  Python.

- `prompt-toolkit/python-prompt-toolkit`_: a Library for building powerful
  interactive command lines in Python.

- `dbcli/pgcli`_: Postgres CLI with autocompletion and syntax highlighting.

- `thomasballinger/curtsies`_: a Curses-like terminal wrapper with a display
  based on compositing 2d arrays of text.

- `selectel/pyte`_: Simple VTXXX-compatible linux terminal emulator.

- `astanin/python-tabulate`_: Pretty-print tabular data in Python, a library
  and a command-line utility.

- `rspeer/python-ftfy`_: Fixes mojibake and other glitches in Unicode
  text.

- `nbedos/termtosvg`_: Terminal recorder that renders sessions as SVG
  animations.

- `peterbrittain/asciimatics`_: Package to help people create full-screen text
  UIs.

- `python-cmd2/cmd2`_: A tool for building interactive command line apps

- `stratis-storage/stratis-cli`_: CLI for the Stratis project

- `ihabunek/toot`_: A Mastodon CLI/TUI client

- `saulpw/visidata`_: Terminal spreadsheet multitool for discovering and
  arranging data

===============
Other Languages
===============

- `timoxley/wcwidth`_: JavaScript
- `janlelis/unicode-display_width`_: Ruby
- `alecrabbit/php-wcwidth`_: PHP
- `Text::CharWidth`_: Perl
- `bluebear94/Terminal-WCWidth`_: Perl 6
- `mattn/go-runewidth`_: Go
- `grepsuzette/wcwidth`_: Haxe
- `aperezdc/lua-wcwidth`_: Lua
- `joachimschmidt557/zig-wcwidth`_: Zig
- `fumiyas/wcwidth-cjk`_: `LD_PRELOAD` override
- `joshuarubin/wcwidth9`_: Unicode version 9 in C

=======
History
=======

0.2.13 *2024-01-06*
  * **Bugfix** zero-width support for Hangul Jamo (Korean)

0.2.12 *2023-11-21*
  * re-release to remove .pyi file misplaced in wheel files `Issue #101`_.

0.2.11 *2023-11-20*
  * Include tests files in the source distribution (`PR #98`_, `PR #100`_).

0.2.10 *2023-11-13*
  * **Bugfix** accounting of some kinds of emoji sequences using U+FE0F
    Variation Selector 16 (`PR #97`_).
  * **Updated** `Specification <Specification_from_pypi_>`_.

0.2.9 *2023-10-30*
  * **Bugfix** zero-width characters used in Emoji ZWJ sequences, Balinese,
    Jamo, Devanagari, Tamil, Kannada and others (`PR #91`_).
  * **Updated** to include `Specification <Specification_from_pypi_>`_ of
    character measurements.

0.2.8 *2023-09-30*
  * Include requirements files in the source distribution (`PR #82`_).

0.2.7 *2023-09-28*
  * **Updated** tables to include Unicode Specification 15.1.0.
  * Include ``bin``, ``docs``, and ``tox.ini`` in the source distribution

0.2.6 *2023-01-14*
  * **Updated** tables to include Unicode Specification 14.0.0 and 15.0.0.
  * **Changed** developer tools to use pip-compile, and to use jinja2 templates
    for code generation in `bin/update-tables.py` to prepare for possible
    compiler optimization release.

0.2.1 .. 0.2.5 *2020-06-23*
  * **Repository** changes to update tests and packaging issues, and
    begin tagging repository with matching release versions.

0.2.0 *2020-06-01*
  * **Enhancement**: Unicode version may be selected by exporting the
    Environment variable ``UNICODE_VERSION``, such as ``13.0``, or ``6.3.0``.
    See the `jquast/ucs-detect`_ CLI utility for automatic detection.
  * **Enhancement**:
    API Documentation is published to readthedocs.org.
  * **Updated** tables for *all* Unicode Specifications with files
    published in a programmatically consumable format, versions 4.1.0
    through 13.0

0.1.9 *2020-03-22*
  * **Performance** optimization by `Avram Lubkin`_, `PR #35`_.
  * **Updated** tables to Unicode Specification 13.0.0.

0.1.8 *2020-01-01*
  * **Updated** tables to Unicode Specification 12.0.0. (`PR #30`_).

0.1.7 *2016-07-01*
  * **Updated** tables to Unicode Specification 9.0.0. (`PR #18`_).

0.1.6 *2016-01-08 Production/Stable*
  * ``LICENSE`` file now included with distribution.

0.1.5 *2015-09-13 Alpha*
  * **Bugfix**:
    Resolution of "combining_ character width" issue, most especially
    those that previously returned -1 now often (correctly) return 0.
    resolved by `Philip Craig`_ via `PR #11`_.
  * **Deprecated**:
    The module path ``wcwidth.table_comb`` is no longer available,
    it has been superseded by module path ``wcwidth.table_zero``.

0.1.4 *2014-11-20 Pre-Alpha*
  * **Feature**: ``wcswidth()`` now determines printable length
    for (most) combining_ characters.  The developer's tool
    `bin/wcwidth-browser.py`_ is improved to display combining_
    characters when provided the ``--combining`` option
    (`Thomas Ballinger`_ and `Leta Montopoli`_ `PR #5`_).
  * **Feature**: added static analysis (prospector_) to testing
    framework.

0.1.3 *2014-10-29 Pre-Alpha*
  * **Bugfix**: 2nd parameter of wcswidth was not honored.
    (`Thomas Ballinger`_, `PR #4`_).

0.1.2 *2014-10-28 Pre-Alpha*
  * **Updated** tables to Unicode Specification 7.0.0.
    (`Thomas Ballinger`_, `PR #3`_).

0.1.1 *2014-05-14 Pre-Alpha*
  * Initial release to pypi, Based on Unicode Specification 6.3.0

This code was originally derived directly from C code of the same name,
whose latest version is available at
https://www.cl.cam.ac.uk/~mgk25/ucs/wcwidth.c::

 * Markus Kuhn -- 2007-05-26 (Unicode 5.0)
 *
 * Permission to use, copy, modify, and distribute this software
 * for any purpose and without fee is hereby granted. The author
 * disclaims all warranties with regard to this software.

.. _`Specification_from_pypi`: https://wcwidth.readthedocs.io/en/latest/specs.html
.. _`tox`: https://tox.wiki/en/latest/
.. _`prospector`: https://github.com/landscapeio/prospector
.. _`combining`: https://en.wikipedia.org/wiki/Combining_character
.. _`bin/`: https://github.com/jquast/wcwidth/tree/master/bin
.. _`bin/wcwidth-browser.py`: https://github.com/jquast/wcwidth/blob/master/bin/wcwidth-browser.py
.. _`Thomas Ballinger`: https://github.com/thomasballinger
.. _`Leta Montopoli`: https://github.com/lmontopo
.. _`Philip Craig`: https://github.com/philipc
.. _`PR #3`: https://github.com/jquast/wcwidth/pull/3
.. _`PR #4`: https://github.com/jquast/wcwidth/pull/4
.. _`PR #5`: https://github.com/jquast/wcwidth/pull/5
.. _`PR #11`: https://github.com/jquast/wcwidth/pull/11
.. _`PR #18`: https://github.com/jquast/wcwidth/pull/18
.. _`PR #30`: https://github.com/jquast/wcwidth/pull/30
.. _`PR #35`: https://github.com/jquast/wcwidth/pull/35
.. _`PR #82`: https://github.com/jquast/wcwidth/pull/82
.. _`PR #91`: https://github.com/jquast/wcwidth/pull/91
.. _`PR #97`: https://github.com/jquast/wcwidth/pull/97
.. _`PR #98`: https://github.com/jquast/wcwidth/pull/98
.. _`PR #100`: https://github.com/jquast/wcwidth/pull/100
.. _`Issue #101`: https://github.com/jquast/wcwidth/issues/101
.. _`jquast/blessed`: https://github.com/jquast/blessed
.. _`selectel/pyte`: https://github.com/selectel/pyte
.. _`thomasballinger/curtsies`: https://github.com/thomasballinger/curtsies
.. _`dbcli/pgcli`: https://github.com/dbcli/pgcli
.. _`prompt-toolkit/python-prompt-toolkit`: https://github.com/prompt-toolkit/python-prompt-toolkit
.. _`timoxley/wcwidth`: https://github.com/timoxley/wcwidth
.. _`wcwidth(3)`:  https://man7.org/linux/man-pages/man3/wcwidth.3.html
.. _`wcswidth(3)`: https://man7.org/linux/man-pages/man3/wcswidth.3.html
.. _`astanin/python-tabulate`: https://github.com/astanin/python-tabulate
.. _`janlelis/unicode-display_width`: https://github.com/janlelis/unicode-display_width
.. _`rspeer/python-ftfy`: https://github.com/rspeer/python-ftfy
.. _`alecrabbit/php-wcwidth`: https://github.com/alecrabbit/php-wcwidth
.. _`Text::CharWidth`: https://metacpan.org/pod/Text::CharWidth
.. _`bluebear94/Terminal-WCWidth`: https://github.com/bluebear94/Terminal-WCWidth
.. _`mattn/go-runewidth`: https://github.com/mattn/go-runewidth
.. _`grepsuzette/wcwidth`: https://github.com/grepsuzette/wcwidth
.. _`jquast/ucs-detect`: https://github.com/jquast/ucs-detect
.. _`Avram Lubkin`: https://github.com/avylove
.. _`nbedos/termtosvg`: https://github.com/nbedos/termtosvg
.. _`peterbrittain/asciimatics`: https://github.com/peterbrittain/asciimatics
.. _`aperezdc/lua-wcwidth`: https://github.com/aperezdc/lua-wcwidth
.. _`joachimschmidt557/zig-wcwidth`: https://github.com/joachimschmidt557/zig-wcwidth
.. _`fumiyas/wcwidth-cjk`: https://github.com/fumiyas/wcwidth-cjk
.. _`joshuarubin/wcwidth9`: https://github.com/joshuarubin/wcwidth9
.. _`python-cmd2/cmd2`: https://github.com/python-cmd2/cmd2
.. _`stratis-storage/stratis-cli`: https://github.com/stratis-storage/stratis-cli
.. _`ihabunek/toot`: https://github.com/ihabunek/toot
.. _`saulpw/visidata`: https://github.com/saulpw/visidata
.. _`pip-tools`: https://pip-tools.readthedocs.io/
.. _`sphinx`: https://www.sphinx-doc.org/
.. |pypi_downloads| image:: https://img.shields.io/pypi/dm/wcwidth.svg?logo=pypi
    :alt: Downloads
    :target: https://pypi.org/project/wcwidth/
.. |codecov| image:: https://codecov.io/gh/jquast/wcwidth/branch/master/graph/badge.svg
    :alt: codecov.io Code Coverage
    :target: https://app.codecov.io/gh/jquast/wcwidth/
.. |license| image:: https://img.shields.io/pypi/l/wcwidth.svg
    :target: https://pypi.org/project/wcwidth/
    :alt: MIT License
"""^^xsd:string ;
    schema1:name "wcwidth"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.2.13"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/yarg> a software_vocab:Software_package ;
    schema1:description """yarg(1) -- A semi hard Cornish cheese, also queries PyPI
========================================================

.. image:: https://img.shields.io/travis/kura/yarg.svg?style=flat

.. image:: https://img.shields.io/coveralls/kura/yarg.svg?style=flat

.. image:: https://pypip.in/version/yarg/badge.svg?style=flat

.. image:: https://pypip.in/download/yarg/badge.svg?style=flat

.. image:: https://pypip.in/py_versions/yarg/badge.svg?style=flat

.. image:: https://pypip.in/implementation/yarg/badge.svg?style=flat

.. image:: https://pypip.in/status/yarg/badge.svg?style=flat

.. image:: https://pypip.in/wheel/yarg/badge.svg?style=flat

.. image:: https://pypip.in/license/yarg/badge.svg?style=flat

Yarg is a PyPI client.

.. code-block:: python

    >>> import yarg
    >>> package = yarg.get("yarg")
    >>> package.name
    u'yarg'
    >>> package.author
    Author(name=u'Kura', email=u'kura@kura.io')

Full documentation is at <https://yarg.readthedocs.org>.

Yarg is released under the `MIT license
<https://github.com/kura/yarg/blob/master/LICENSE>`_. The `source code is on
GitHub <https://github.com/kura/yarg>`_ and `issues are also tracked on
GitHub <https://github.com/kura/yarg/issues>`_.


Release History
===============

0.1.8 (2014-08-10)
------------------

Splatting bugs
~~~~~~~~~~~~~~

- Integration issue with Python 3, requests, yarg and JSON. Attempt to decode
  requests response if decode attribute exists.

0.1.6 & 0.1.7 (2014-08-10)
--------------------------

Splatting bugs
~~~~~~~~~~~~~~

- Bug in setup.py causing installs to fail for sdist (source) releases.

0.1.5 (2014-08-10)
------------------

API changes
~~~~~~~~~~~

- Changed sort order of `yarg.package.Package.release_ids` to sort
  based on the upload time of the release ID.

Splatting bugs
~~~~~~~~~~~~~~

- `yarg.package.Package.latest_release_id` will now return the latest
  release ID from the PyPI info source, rather than the final list item in
  `yarg.package.Package.release_ids`.

  Addtionally `yarg.package.Package.latest_release` will do the same as
  it gets the latest release information from
  `yarg.package.Package.latest_release_id`.

0.1.4 (2014-08-09)
------------------

API changes
~~~~~~~~~~~

- New method `yarg.newest_packages` for querying new packages
  from the PyPI RSS feed.
- New method `yarg.latest_updated_packages` for querying
  the latest updated packages from the PyPI RSS feed.

Other
~~~~~

- Additional test coverage
- Additional documentation coverage

0.1.2 (2014-08-08)
------------------

Bug fixes
~~~~~~~~~

- `yarg.get` will now raise an Exception for errors **including**
  300 and above. Previously only raised for above 300.
- Fix an issue on Python 3.X and PyPy3 where
  `yarg.exceptions.HTTPError` was using a method that was
  removed in Python 3.
- Added dictionary key lookups for `home_page`, `bugtrack_url`
  and `docs_url`. Caused `KeyError` exceptions if they were not
  returned by PyPI.

Other
~~~~~

- More test coverage.

0.1.1 (2014-08-08)
------------------

API changes
~~~~~~~~~~~

- New `yarg.package.Package` property `has_wheel`.
- New `yarg.package.Package` property `has_egg`.
- New `yarg.package.Package` property `has_source`.
- New `yarg.package.Package` property `python_versions`.
- New `yarg.package.Package` property `python_implementations`.
- Added `yarg.exceptions.HTTPError` to `yarg.__init__`
  for easier access.
- Added `yarg.json2package` to `yarg.__init__` to expose it for
  use.

0.1.0 (2014-08-08)
------------------

- Initial release


"""^^xsd:string ;
    schema1:name "yarg"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.1.9"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/Babel> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.7> .

<https://data.siemens-energy.com/cybersec/package/Cython> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.0.11> .

<https://data.siemens-energy.com/cybersec/package/Jinja2> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/Babel>,
        <https://data.siemens-energy.com/cybersec/package/MarkupSafe> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.1.2> ;
    schema1:description """# Jinja

Jinja is a fast, expressive, extensible templating engine. Special
placeholders in the template allow writing code similar to Python
syntax. Then the template is passed data to render the final document.

It includes:

-   Template inheritance and inclusion.
-   Define and import macros within templates.
-   HTML templates can use autoescaping to prevent XSS from untrusted
    user input.
-   A sandboxed environment can safely render untrusted templates.
-   AsyncIO support for generating templates and calling async
    functions.
-   I18N support with Babel.
-   Templates are compiled to optimized Python code just-in-time and
    cached, or can be compiled ahead-of-time.
-   Exceptions point to the correct line in templates to make debugging
    easier.
-   Extensible filters, tests, functions, and even syntax.

Jinja's philosophy is that while application logic belongs in Python if
possible, it shouldn't make the template designer's job difficult by
restricting functionality too much.


## In A Nutshell

.. code-block:: jinja

    {% extends "base.html" %}
    {% block title %}Members{% endblock %}
    {% block content %}
      <ul>
      {% for user in users %}
        <li><a href="{{ user.url }}">{{ user.username }}</a></li>
      {% endfor %}
      </ul>
    {% endblock %}


## Donate

The Pallets organization develops and supports Jinja and other popular
packages. In order to grow the community of contributors and users, and
allow the maintainers to devote more time to the projects, [please
donate today][].

[please donate today]: https://palletsprojects.com/donate

"""^^xsd:string ;
    schema1:name "Jinja2"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.1.4"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/PyQt5> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/5.15.9> .

<https://data.siemens-energy.com/cybersec/package/PySocks> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.5.7> .

<https://data.siemens-energy.com/cybersec/package/SQLAlchemy> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.0.0> .

<https://data.siemens-energy.com/cybersec/package/Sphinx> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/5.0.2> .

<https://data.siemens-energy.com/cybersec/package/Werkzeug> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/MarkupSafe>,
        <https://data.siemens-energy.com/cybersec/package/watchdog> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.1> ;
    schema1:description """# Werkzeug

*werkzeug* German noun: "tool". Etymology: *werk* ("work"), *zeug* ("stuff")

Werkzeug is a comprehensive [WSGI][] web application library. It began as
a simple collection of various utilities for WSGI applications and has
become one of the most advanced WSGI utility libraries.

It includes:

-   An interactive debugger that allows inspecting stack traces and
    source code in the browser with an interactive interpreter for any
    frame in the stack.
-   A full-featured request object with objects to interact with
    headers, query args, form data, files, and cookies.
-   A response object that can wrap other WSGI applications and handle
    streaming data.
-   A routing system for matching URLs to endpoints and generating URLs
    for endpoints, with an extensible system for capturing variables
    from URLs.
-   HTTP utilities to handle entity tags, cache control, dates, user
    agents, cookies, files, and more.
-   A threaded WSGI server for use while developing applications
    locally.
-   A test client for simulating HTTP requests during testing without
    requiring running a server.

Werkzeug doesn't enforce any dependencies. It is up to the developer to
choose a template engine, database adapter, and even how to handle
requests. It can be used to build all sorts of end user applications
such as blogs, wikis, or bulletin boards.

[Flask][] wraps Werkzeug, using it to handle the details of WSGI while
providing more structure and patterns for defining powerful
applications.

[WSGI]: https://wsgi.readthedocs.io/en/latest/
[Flask]: https://www.palletsprojects.com/p/flask/


## A Simple Example

```python
# save this as app.py
from werkzeug.wrappers import Request, Response

@Request.application
def application(request: Request) -> Response:
    return Response("Hello, World!")

if __name__ == "__main__":
    from werkzeug.serving import run_simple
    run_simple("127.0.0.1", 5000, application)
```

```
$ python -m app
  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
```


## Donate

The Pallets organization develops and supports Werkzeug and other
popular packages. In order to grow the community of contributors and
users, and allow the maintainers to devote more time to the projects,
[please donate today][].

[please donate today]: https://palletsprojects.com/donate

"""^^xsd:string ;
    schema1:name "Werkzeug"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.1.3"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/adbc-driver-postgresql> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.8.0> .

<https://data.siemens-energy.com/cybersec/package/adbc-driver-sqlite> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.8.0> .

<https://data.siemens-energy.com/cybersec/package/annotated-types> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/typing-extensions> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.6.0> ;
    schema1:description """# annotated-types

[![CI](https://github.com/annotated-types/annotated-types/workflows/CI/badge.svg?event=push)](https://github.com/annotated-types/annotated-types/actions?query=event%3Apush+branch%3Amain+workflow%3ACI)
[![pypi](https://img.shields.io/pypi/v/annotated-types.svg)](https://pypi.python.org/pypi/annotated-types)
[![versions](https://img.shields.io/pypi/pyversions/annotated-types.svg)](https://github.com/annotated-types/annotated-types)
[![license](https://img.shields.io/github/license/annotated-types/annotated-types.svg)](https://github.com/annotated-types/annotated-types/blob/main/LICENSE)

[PEP-593](https://peps.python.org/pep-0593/) added `typing.Annotated` as a way of
adding context-specific metadata to existing types, and specifies that
`Annotated[T, x]` _should_ be treated as `T` by any tool or library without special
logic for `x`.

This package provides metadata objects which can be used to represent common
constraints such as upper and lower bounds on scalar values and collection sizes,
a `Predicate` marker for runtime checks, and
descriptions of how we intend these metadata to be interpreted. In some cases,
we also note alternative representations which do not require this package.

## Install

```bash
pip install annotated-types
```

## Examples

```python
from typing import Annotated
from annotated_types import Gt, Len, Predicate

class MyClass:
    age: Annotated[int, Gt(18)]                         # Valid: 19, 20, ...
                                                        # Invalid: 17, 18, "19", 19.0, ...
    factors: list[Annotated[int, Predicate(is_prime)]]  # Valid: 2, 3, 5, 7, 11, ...
                                                        # Invalid: 4, 8, -2, 5.0, "prime", ...

    my_list: Annotated[list[int], Len(0, 10)]           # Valid: [], [10, 20, 30, 40, 50]
                                                        # Invalid: (1, 2), ["abc"], [0] * 20
```

## Documentation

_While `annotated-types` avoids runtime checks for performance, users should not
construct invalid combinations such as `MultipleOf("non-numeric")` or `Annotated[int, Len(3)]`.
Downstream implementors may choose to raise an error, emit a warning, silently ignore
a metadata item, etc., if the metadata objects described below are used with an
incompatible type - or for any other reason!_

### Gt, Ge, Lt, Le

Express inclusive and/or exclusive bounds on orderable values - which may be numbers,
dates, times, strings, sets, etc. Note that the boundary value need not be of the
same type that was annotated, so long as they can be compared: `Annotated[int, Gt(1.5)]`
is fine, for example, and implies that the value is an integer x such that `x > 1.5`.

We suggest that implementors may also interpret `functools.partial(operator.le, 1.5)`
as being equivalent to `Gt(1.5)`, for users who wish to avoid a runtime dependency on
the `annotated-types` package.

To be explicit, these types have the following meanings:

* `Gt(x)` - value must be "Greater Than" `x` - equivalent to exclusive minimum
* `Ge(x)` - value must be "Greater than or Equal" to `x` - equivalent to inclusive minimum
* `Lt(x)` - value must be "Less Than" `x` - equivalent to exclusive maximum
* `Le(x)` - value must be "Less than or Equal" to `x` - equivalent to inclusive maximum

### Interval

`Interval(gt, ge, lt, le)` allows you to specify an upper and lower bound with a single
metadata object. `None` attributes should be ignored, and non-`None` attributes
treated as per the single bounds above.

### MultipleOf

`MultipleOf(multiple_of=x)` might be interpreted in two ways:

1. Python semantics, implying `value % multiple_of == 0`, or
2. [JSONschema semantics](https://json-schema.org/draft/2020-12/json-schema-validation.html#rfc.section.6.2.1),
   where `int(value / multiple_of) == value / multiple_of`.

We encourage users to be aware of these two common interpretations and their
distinct behaviours, especially since very large or non-integer numbers make
it easy to cause silent data corruption due to floating-point imprecision.

We encourage libraries to carefully document which interpretation they implement.

### MinLen, MaxLen, Len

`Len()` implies that `min_length <= len(value) <= max_length` - lower and upper bounds are inclusive.

As well as `Len()` which can optionally include upper and lower bounds, we also
provide `MinLen(x)` and `MaxLen(y)` which are equivalent to `Len(min_length=x)`
and `Len(max_length=y)` respectively.

`Len`, `MinLen`, and `MaxLen` may be used with any type which supports `len(value)`.

Examples of usage:

* `Annotated[list, MaxLen(10)]` (or `Annotated[list, Len(max_length=10))`) - list must have a length of 10 or less
* `Annotated[str, MaxLen(10)]` - string must have a length of 10 or less
* `Annotated[list, MinLen(3))` (or `Annotated[list, Len(min_length=3))`) - list must have a length of 3 or more
* `Annotated[list, Len(4, 6)]` - list must have a length of 4, 5, or 6
* `Annotated[list, Len(8, 8)]` - list must have a length of exactly 8

#### Changed in v0.4.0

* `min_inclusive` has been renamed to `min_length`, no change in meaning
* `max_exclusive` has been renamed to `max_length`, upper bound is now **inclusive** instead of **exclusive**
* The recommendation that slices are interpreted as `Len` has been removed due to ambiguity and different semantic
  meaning of the upper bound in slices vs. `Len`

See [issue #23](https://github.com/annotated-types/annotated-types/issues/23) for discussion.

### Timezone

`Timezone` can be used with a `datetime` or a `time` to express which timezones
are allowed. `Annotated[datetime, Timezone(None)]` must be a naive datetime.
`Timezone[...]` ([literal ellipsis](https://docs.python.org/3/library/constants.html#Ellipsis))
expresses that any timezone-aware datetime is allowed. You may also pass a specific
timezone string or [`tzinfo`](https://docs.python.org/3/library/datetime.html#tzinfo-objects)
object such as `Timezone(timezone.utc)` or `Timezone("Africa/Abidjan")` to express that you only
allow a specific timezone, though we note that this is often a symptom of fragile design.

#### Changed in v0.x.x

* `Timezone` accepts [`tzinfo`](https://docs.python.org/3/library/datetime.html#tzinfo-objects) objects instead of
  `timezone`, extending compatibility to [`zoneinfo`](https://docs.python.org/3/library/zoneinfo.html) and third party libraries.

### Unit

`Unit(unit: str)` expresses that the annotated numeric value is the magnitude of
a quantity with the specified unit. For example, `Annotated[float, Unit("m/s")]`
would be a float representing a velocity in meters per second.

Please note that `annotated_types` itself makes no attempt to parse or validate
the unit string in any way. That is left entirely to downstream libraries,
such as [`pint`](https://pint.readthedocs.io) or
[`astropy.units`](https://docs.astropy.org/en/stable/units/).

An example of how a library might use this metadata:

```python
from annotated_types import Unit
from typing import Annotated, TypeVar, Callable, Any, get_origin, get_args

# given a type annotated with a unit:
Meters = Annotated[float, Unit("m")]


# you can cast the annotation to a specific unit type with any
# callable that accepts a string and returns the desired type
T = TypeVar("T")
def cast_unit(tp: Any, unit_cls: Callable[[str], T]) -> T | None:
    if get_origin(tp) is Annotated:
        for arg in get_args(tp):
            if isinstance(arg, Unit):
                return unit_cls(arg.unit)
    return None


# using `pint`
import pint
pint_unit = cast_unit(Meters, pint.Unit)


# using `astropy.units`
import astropy.units as u
astropy_unit = cast_unit(Meters, u.Unit)
```

### Predicate

`Predicate(func: Callable)` expresses that `func(value)` is truthy for valid values.
Users should prefer the statically inspectable metadata above, but if you need
the full power and flexibility of arbitrary runtime predicates... here it is.

For some common constraints, we provide generic types:

* `IsLower       = Annotated[T, Predicate(str.islower)]`
* `IsUpper       = Annotated[T, Predicate(str.isupper)]`
* `IsDigit       = Annotated[T, Predicate(str.isdigit)]`
* `IsFinite      = Annotated[T, Predicate(math.isfinite)]`
* `IsNotFinite   = Annotated[T, Predicate(Not(math.isfinite))]`
* `IsNan         = Annotated[T, Predicate(math.isnan)]`
* `IsNotNan      = Annotated[T, Predicate(Not(math.isnan))]`
* `IsInfinite    = Annotated[T, Predicate(math.isinf)]`
* `IsNotInfinite = Annotated[T, Predicate(Not(math.isinf))]`

so that you can write e.g. `x: IsFinite[float] = 2.0` instead of the longer
(but exactly equivalent) `x: Annotated[float, Predicate(math.isfinite)] = 2.0`.

Some libraries might have special logic to handle known or understandable predicates,
for example by checking for `str.isdigit` and using its presence to both call custom
logic to enforce digit-only strings, and customise some generated external schema.
Users are therefore encouraged to avoid indirection like `lambda s: s.lower()`, in
favor of introspectable methods such as `str.lower` or `re.compile("pattern").search`.

To enable basic negation of commonly used predicates like `math.isnan` without introducing introspection that makes it impossible for implementers to introspect the predicate we provide a `Not` wrapper that simply negates the predicate in an introspectable manner. Several of the predicates listed above are created in this manner.

We do not specify what behaviour should be expected for predicates that raise
an exception.  For example `Annotated[int, Predicate(str.isdigit)]` might silently
skip invalid constraints, or statically raise an error; or it might try calling it
and then propagate or discard the resulting
`TypeError: descriptor 'isdigit' for 'str' objects doesn't apply to a 'int' object`
exception.  We encourage libraries to document the behaviour they choose.

### Doc

`doc()` can be used to add documentation information in `Annotated`, for function and method parameters, variables, class attributes, return types, and any place where `Annotated` can be used.

It expects a value that can be statically analyzed, as the main use case is for static analysis, editors, documentation generators, and similar tools.

It returns a `DocInfo` class with a single attribute `documentation` containing the value passed to `doc()`.

This is the early adopter's alternative form of the [`typing-doc` proposal](https://github.com/tiangolo/fastapi/blob/typing-doc/typing_doc.md).

### Integrating downstream types with `GroupedMetadata`

Implementers may choose to provide a convenience wrapper that groups multiple pieces of metadata.
This can help reduce verbosity and cognitive overhead for users.
For example, an implementer like Pydantic might provide a `Field` or `Meta` type that accepts keyword arguments and transforms these into low-level metadata:

```python
from dataclasses import dataclass
from typing import Iterator
from annotated_types import GroupedMetadata, Ge

@dataclass
class Field(GroupedMetadata):
    ge: int | None = None
    description: str | None = None

    def __iter__(self) -> Iterator[object]:
        # Iterating over a GroupedMetadata object should yield annotated-types
        # constraint metadata objects which describe it as fully as possible,
        # and may include other unknown objects too.
        if self.ge is not None:
            yield Ge(self.ge)
        if self.description is not None:
            yield Description(self.description)
```

Libraries consuming annotated-types constraints should check for `GroupedMetadata` and unpack it by iterating over the object and treating the results as if they had been "unpacked" in the `Annotated` type.  The same logic should be applied to the [PEP 646 `Unpack` type](https://peps.python.org/pep-0646/), so that `Annotated[T, Field(...)]`, `Annotated[T, Unpack[Field(...)]]` and `Annotated[T, *Field(...)]` are all treated consistently.

Libraries consuming annotated-types should also ignore any metadata they do not recongize that came from unpacking a `GroupedMetadata`, just like they ignore unrecognized metadata in `Annotated` itself.

Our own `annotated_types.Interval` class is a `GroupedMetadata` which unpacks itself into `Gt`, `Lt`, etc., so this is not an abstract concern.  Similarly, `annotated_types.Len` is a `GroupedMetadata` which unpacks itself into `MinLen` (optionally) and `MaxLen`.

### Consuming metadata

We intend to not be prescriptive as to _how_ the metadata and constraints are used, but as an example of how one might parse constraints from types annotations see our [implementation in `test_main.py`](https://github.com/annotated-types/annotated-types/blob/f59cf6d1b5255a0fe359b93896759a180bec30ae/tests/test_main.py#L94-L103).

It is up to the implementer to determine how this metadata is used.
You could use the metadata for runtime type checking, for generating schemas or to generate example data, amongst other use cases.

## Design & History

This package was designed at the PyCon 2022 sprints by the maintainers of Pydantic
and Hypothesis, with the goal of making it as easy as possible for end-users to
provide more informative annotations for use by runtime libraries.

It is deliberately minimal, and following PEP-593 allows considerable downstream
discretion in what (if anything!) they choose to support. Nonetheless, we expect
that staying simple and covering _only_ the most common use-cases will give users
and maintainers the best experience we can. If you'd like more constraints for your
types - follow our lead, by defining them and documenting them downstream!
"""^^xsd:string ;
    schema1:name "annotated-types"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.7.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/appdirs> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.4.4> .

<https://data.siemens-energy.com/cybersec/package/argcomplete> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.0.3> .

<https://data.siemens-energy.com/cybersec/package/asgiref> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.2> .

<https://data.siemens-energy.com/cybersec/package/beautifulsoup4> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/soupsieve> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/4.11.2> ;
    schema1:description """Beautiful Soup is a library that makes it easy to scrape information
from web pages. It sits atop an HTML or XML parser, providing Pythonic
idioms for iterating, searching, and modifying the parse tree.

# Quick start

```
>>> from bs4 import BeautifulSoup
>>> soup = BeautifulSoup("<p>Some<b>bad<i>HTML")
>>> print(soup.prettify())
<html>
 <body>
  <p>
   Some
   <b>
    bad
    <i>
     HTML
    </i>
   </b>
  </p>
 </body>
</html>
>>> soup.find(text="bad")
'bad'
>>> soup.i
<i>HTML</i>
#
>>> soup = BeautifulSoup("<tag1>Some<tag2/>bad<tag3>XML", "xml")
#
>>> print(soup.prettify())
<?xml version="1.0" encoding="utf-8"?>
<tag1>
 Some
 <tag2/>
 bad
 <tag3>
  XML
 </tag3>
</tag1>
```

To go beyond the basics, [comprehensive documentation is available](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).

# Links

* [Homepage](https://www.crummy.com/software/BeautifulSoup/bs4/)
* [Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
* [Discussion group](https://groups.google.com/group/beautifulsoup/)
* [Development](https://code.launchpad.net/beautifulsoup/)
* [Bug tracker](https://bugs.launchpad.net/beautifulsoup/)
* [Complete changelog](https://bazaar.launchpad.net/~leonardr/beautifulsoup/bs4/view/head:/CHANGELOG)

# Note on Python 2 sunsetting

Beautiful Soup's support for Python 2 was discontinued on December 31,
2020: one year after the sunset date for Python 2 itself. From this
point onward, new Beautiful Soup development will exclusively target
Python 3. The final release of Beautiful Soup 4 to support Python 2
was 4.9.3.

# Supporting the project

If you use Beautiful Soup as part of your professional work, please consider a
[Tidelift subscription](https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=readme).
This will support many of the free software projects your organization
depends on, not just Beautiful Soup.

If you use Beautiful Soup for personal projects, the best way to say
thank you is to read
[Tool Safety](https://www.crummy.com/software/BeautifulSoup/zine/), a zine I
wrote about what Beautiful Soup has taught me about software
development.

# Building the documentation

The bs4/doc/ directory contains full documentation in Sphinx
format. Run `make html` in that directory to create HTML
documentation.

# Running the unit tests

Beautiful Soup supports unit test discovery using Pytest:

```
$ pytest
```

"""^^xsd:string ;
    schema1:name "beautifulsoup4"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "4.12.3"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/bleach> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/six>,
        <https://data.siemens-energy.com/cybersec/package/tinycss2> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/5.0.0> ;
    schema1:description """======
Bleach
======

.. image:: https://github.com/mozilla/bleach/workflows/Test/badge.svg
   :target: https://github.com/mozilla/bleach/actions?query=workflow%3ATest

.. image:: https://github.com/mozilla/bleach/workflows/Lint/badge.svg
   :target: https://github.com/mozilla/bleach/actions?query=workflow%3ALint

.. image:: https://badge.fury.io/py/bleach.svg
   :target: http://badge.fury.io/py/bleach

**NOTE: 2023-01-23: Bleach is deprecated.** See issue:
`<https://github.com/mozilla/bleach/issues/698>`__

Bleach is an allowed-list-based HTML sanitizing library that escapes or strips
markup and attributes.

Bleach can also linkify text safely, applying filters that Django's ``urlize``
filter cannot, and optionally setting ``rel`` attributes, even on links already
in the text.

Bleach is intended for sanitizing text from *untrusted* sources. If you find
yourself jumping through hoops to allow your site administrators to do lots of
things, you're probably outside the use cases. Either trust those users, or
don't.

Because it relies on html5lib_, Bleach is as good as modern browsers at dealing
with weird, quirky HTML fragments. And *any* of Bleach's methods will fix
unbalanced or mis-nested tags.

The version on GitHub_ is the most up-to-date and contains the latest bug
fixes. You can find full documentation on `ReadTheDocs`_.

:Code:           https://github.com/mozilla/bleach
:Documentation:  https://bleach.readthedocs.io/
:Issue tracker:  https://github.com/mozilla/bleach/issues
:License:        Apache License v2; see LICENSE file


Reporting Bugs
==============

For regular bugs, please report them `in our issue tracker
<https://github.com/mozilla/bleach/issues>`_.

If you believe that you've found a security vulnerability, please `file a secure
bug report in our bug tracker
<https://bugzilla.mozilla.org/enter_bug.cgi?assigned_to=nobody%40mozilla.org&product=Webtools&component=Bleach-security&groups=webtools-security>`_
or send an email to *security AT mozilla DOT org*.

For more information on security-related bug disclosure and the PGP key to use
for sending encrypted mail or to verify responses received from that address,
please read our wiki page at
`<https://www.mozilla.org/en-US/security/#For_Developers>`_.


Security
========

Bleach is a security-focused library.

We have a responsible security vulnerability reporting process. Please use
that if you're reporting a security issue.

Security issues are fixed in private. After we land such a fix, we'll do a
release.

For every release, we mark security issues we've fixed in the ``CHANGES`` in
the **Security issues** section. We include any relevant CVE links.


Installing Bleach
=================

Bleach is available on PyPI_, so you can install it with ``pip``::

    $ pip install bleach


Upgrading Bleach
================

.. warning::

   Before doing any upgrades, read through `Bleach Changes
   <https://bleach.readthedocs.io/en/latest/changes.html>`_ for backwards
   incompatible changes, newer versions, etc.

   Bleach follows `semver 2`_ versioning. Vendored libraries will not
   be changed in patch releases.


Basic use
=========

The simplest way to use Bleach is:

.. code-block:: python

    >>> import bleach

    >>> bleach.clean('an <script>evil()</script> example')
    u'an &lt;script&gt;evil()&lt;/script&gt; example'

    >>> bleach.linkify('an http://example.com url')
    u'an <a href="http://example.com" rel="nofollow">http://example.com</a> url'


Code of Conduct
===============

This project and repository is governed by Mozilla's code of conduct and
etiquette guidelines. For more details please see the `CODE_OF_CONDUCT.md
</CODE_OF_CONDUCT.md>`_


.. _html5lib: https://github.com/html5lib/html5lib-python
.. _GitHub: https://github.com/mozilla/bleach
.. _ReadTheDocs: https://bleach.readthedocs.io/
.. _PyPI: https://pypi.org/project/bleach/
.. _semver 2: https://semver.org/


Bleach changes
==============

Version 6.1.0 (October 6th, 2023)
---------------------------------

**Backwards incompatible changes**

* Dropped support for Python 3.7. (#709)

**Security fixes**

None

**Bug fixes**

* Add support for Python 3.12. (#710)
* Fix linkify with arrays in querystring (#436)
* Handle more cases with < followed by character data (#705)
* Fix entities inside a tags in linkification (#704)
* Update cap for tinycss2 to <1.3 (#702)
* Updated Sphinx requirement
* Add dependabot for github actions and update github actions


Version 6.0.0 (January 23rd, 2023)
----------------------------------

**Backwards incompatible changes**

* ``bleach.clean``, ``bleach.sanitizer.Cleaner``,
  ``bleach.html5lib_shim.BleachHTMLParser``: the ``tags`` and ``protocols``
  arguments were changed from lists to sets.

  Old pre-6.0.0:

  .. code-block:: python

     bleach.clean(
         "some text",
         tags=["a", "p", "img"],
         #    ^               ^ list
         protocols=["http", "https"],
         #         ^               ^ list
     )


  New 6.0.0 and later:

  .. code-block:: python

     bleach.clean(
         "some text",
         tags={"a", "p", "img"},
         #    ^               ^ set
         protocols={"http", "https"},
         #         ^               ^ set
     )

* ``bleach.linkify``, ``bleach.linkifier.Linker``: the ``skip_tags`` and
  ``recognized_tags`` arguments were changed from lists to sets.

  Old pre-6.0.0:

  .. code-block:: python

     bleach.linkify(
         "some text",
         skip_tags=["pre"],
         #         ^     ^ list
     )

     linker = Linker(
         skip_tags=["pre"],
         #         ^     ^ list
         recognized_tags=html5lib_shim.HTML_TAGS + ["custom-element"],
         #                                       ^ ^                ^ list
         #                                       |
         #                                       | list concatenation
     )

  New 6.0.0 and later:

  .. code-block:: python

     bleach.linkify(
         "some text",
         skip_tags={"pre"},
         #         ^     ^ set
     )

     linker = Linker(
         skip_tags={"pre"},
         #         ^     ^ set
         recognized_tags=html5lib_shim.HTML_TAGS | {"custom-element"},
         #                                       ^ ^                ^ set
         #                                       |
         #                                       | union operator
     )

* ``bleach.sanitizer.BleachSanitizerFilter``: ``strip_allowed_elements`` is now
  ``strip_allowed_tags``. We now use "tags" everywhere rather than a mishmash
  of "tags" in some places and "elements" in others.


**Security fixes**

None


**Bug fixes**

* Add support for Python 3.11. (#675)

* Fix API weirness in ``BleachSanitizerFilter``. (#649)

  We're using "tags" instead of "elements" everywhere--no more weird
  overloading of "elements" anymore.

  Also, it no longer calls the superclass constructor.

* Add warning when ``css_sanitizer`` isn't set, but the ``style``
  attribute is allowed. (#676)

* Fix linkify handling of character entities. (#501)

* Rework dev dependencies to use ``requirements-dev.txt`` and
  ``requirements-flake8.txt`` instead of extras.

* Fix project infrastructure to be tox-based so it's easier to have CI
  run the same things we're running in development and with flake8
  in an isolated environment.

* Update action versions in CI.

* Switch to f-strings where possible. Make tests parametrized to be
  easier to read/maintain.


Version 5.0.1 (June 27th, 2022)
-------------------------------

**Security fixes**

None


**Bug fixes**

* Add missing comma to tinycss2 require. Thank you, @shadchin!

* Add url parse tests based on wpt url tests. (#688)

* Support scheme-less urls if "https" is in allow list. (#662)

* Handle escaping ``<`` in edge cases where it doesn't start a tag. (#544)

* Fix reference warnings in docs. (#660)

* Correctly urlencode email address parts. Thank you, @larseggert! (#659)


Version 5.0.0 (April 7th, 2022)
-------------------------------

**Backwards incompatible changes**

* ``clean`` and ``linkify`` now preserve the order of HTML attributes. Thank
  you, @askoretskly! (#566)

* Drop support for Python 3.6. Thank you, @hugovk! (#629)

* CSS sanitization in style tags is completely different now. If you're using
  Bleach ``clean`` to sanitize css in style tags, you'll need to update your
  code and you'll need to install the ``css`` extras::

      pip install 'bleach[css]'

  See `the documentation on sanitizing CSS for how to do it
  <https://bleach.readthedocs.io/en/latest/clean.html#sanitizing-css>`_. (#633)

**Security fixes**

None

**Bug fixes**

* Rework dev dependencies. We no longer have
  ``requirements-dev.in``/``requirements-dev.txt``. Instead, we're using
  ``dev`` extras.

  See `development docs <https://bleach.readthedocs.io/en/latest/dev.html>`_
  for more details. (#620)

* Add newline when dropping block-level tags. Thank you, @jvanasco! (#369)


Version 4.1.0 (August 25th, 2021)
---------------------------------

**Features**

* Python 3.9 support

**Security fixes**

None

**Bug fixes**

* Update sanitizer clean to use vendored 3.6.14 stdlib urllib.parse to
  fix test failures on Python 3.9. (#536)


Version 4.0.0 (August 3rd, 2021)
--------------------------------

**Backwards incompatible changes**

* Drop support for unsupported Python versions <3.6. (#520)

**Security fixes**

None

**Features**

* fix attribute name in the linkify docs (thanks @CheesyFeet!)


Version 3.3.1 (July 14th, 2021)
-------------------------------

**Security fixes**

None

**Features**

* add more tests for CVE-2021-23980 / GHSA-vv2x-vrpj-qqpq
* bump python version to 3.8 for tox doc, vendorverify, and lint targets
* update bug report template tag
* update vendorverify script to detect and fail when extra files are vendored
* update release process docs to check vendorverify passes locally

**Bug fixes**

* remove extra vendored django present in the v3.3.0 whl (#595)
* duplicate h1 header doc fix (thanks Nguyá»…n Gia Phong / @McSinyx!)


Version 3.3.0 (February 1st, 2021)
----------------------------------

**Backwards incompatible changes**

* clean escapes HTML comments even when strip_comments=False

**Security fixes**

* Fix bug 1621692 / GHSA-m6xf-fq7q-8743. See the advisory for details.

**Features**

None

**Bug fixes**

None


Version 3.2.3 (January 26th, 2021)
----------------------------------

**Security fixes**

None

**Features**

None

**Bug fixes**

* fix clean and linkify raising ValueErrors for certain inputs. Thank you @Google-Autofuzz.


Version 3.2.2 (January 20th, 2021)
----------------------------------

**Security fixes**

None

**Features**

* Migrate CI to Github Actions. Thank you @hugovk.

**Bug fixes**

* fix linkify raising an IndexError on certain inputs. Thank you @Google-Autofuzz.


Version 3.2.1 (September 18th, 2020)
------------------------------------

**Security fixes**

None

**Features**

None

**Bug fixes**

* change linkifier to add rel="nofollow" as documented. Thank you @mitar.
* suppress html5lib sanitizer DeprecationWarnings (#557)


Version 3.2.0 (September 16th, 2020)
------------------------------------

**Security fixes**

None

**Features**

None

**Bug fixes**

* ``html5lib`` dependency to version 1.1.0. Thank you Sam Sneddon.
* update tests_website terminology. Thank you Thomas Grainger.


Version 3.1.5 (April 29th, 2020)
--------------------------------

**Security fixes**

None

**Features**

None

**Bug fixes**

* replace missing ``setuptools`` dependency with ``packaging``. Thank you Benjamin Peterson.


Version 3.1.4 (March 24th, 2020)
--------------------------------

**Security fixes**

* ``bleach.clean`` behavior parsing style attributes could result in a
  regular expression denial of service (ReDoS).

  Calls to ``bleach.clean`` with an allowed tag with an allowed
  ``style`` attribute were vulnerable to ReDoS. For example,
  ``bleach.clean(..., attributes={'a': ['style']})``.

  This issue was confirmed in Bleach versions v3.1.3, v3.1.2, v3.1.1,
  v3.1.0, v3.0.0, v2.1.4, and v2.1.3. Earlier versions used a similar
  regular expression and should be considered vulnerable too.

  Anyone using Bleach <=v3.1.3 is encouraged to upgrade.

  https://bugzilla.mozilla.org/show_bug.cgi?id=1623633

**Backwards incompatible changes**

* Style attributes with dashes, or single or double quoted values are
  cleaned instead of passed through.

**Features**

None

**Bug fixes**

None


Version 3.1.3 (March 17th, 2020)
--------------------------------

**Security fixes**

None

**Backwards incompatible changes**

* Drop support for Python 3.4. Thank you, @hugovk!

* Drop deprecated ``setup.py test`` support. Thank you, @jdufresne! (#507)

**Features**

* Add support for Python 3.8. Thank you, @jdufresne!

* Add support for PyPy 7. Thank you, @hugovk!

* Add pypy3 testing to tox and travis. Thank you, @jdufresne!

**Bug fixes**

* Add relative link to code of conduct. (#442)

* Fix typo: curren -> current in tests/test_clean.py Thank you, timgates42! (#504)

* Fix handling of non-ascii style attributes. Thank you, @sekineh! (#426)

* Simplify tox configuration. Thank you, @jdufresne!

* Make documentation reproducible. Thank you, @lamby!

* Fix typos in code comments. Thank you, @zborboa-g!

* Fix exception value testing. Thank you, @mastizada!

* Fix parser-tags NoneType exception. Thank you, @bope!

* Improve TLD support in linkify. Thank you, @pc-coholic!


Version 3.1.2 (March 11th, 2020)
--------------------------------

**Security fixes**

* ``bleach.clean`` behavior parsing embedded MathML and SVG content
  with RCDATA tags did not match browser behavior and could result in
  a mutation XSS.

  Calls to ``bleach.clean`` with ``strip=False`` and ``math`` or
  ``svg`` tags and one or more of the RCDATA tags ``script``,
  ``noscript``, ``style``, ``noframes``, ``iframe``, ``noembed``, or
  ``xmp`` in the allowed tags whitelist were vulnerable to a mutation
  XSS.

  This security issue was confirmed in Bleach version v3.1.1. Earlier
  versions are likely affected too.

  Anyone using Bleach <=v3.1.1 is encouraged to upgrade.

  https://bugzilla.mozilla.org/show_bug.cgi?id=1621692

**Backwards incompatible changes**

None

**Features**

None

**Bug fixes**

None


Version 3.1.1 (February 13th, 2020)
-----------------------------------

**Security fixes**

* ``bleach.clean`` behavior parsing ``noscript`` tags did not match
  browser behavior.

  Calls to ``bleach.clean`` allowing ``noscript`` and one or more of
  the raw text tags (``title``, ``textarea``, ``script``, ``style``,
  ``noembed``, ``noframes``, ``iframe``, and ``xmp``) were vulnerable
  to a mutation XSS.

  This security issue was confirmed in Bleach versions v2.1.4, v3.0.2,
  and v3.1.0. Earlier versions are probably affected too.

  Anyone using Bleach <=v3.1.0 is highly encouraged to upgrade.

  https://bugzilla.mozilla.org/show_bug.cgi?id=1615315

**Backwards incompatible changes**

None

**Features**

None

**Bug fixes**

None


Version 3.1.0 (January 9th, 2019)
---------------------------------

**Security fixes**

None

**Backwards incompatible changes**

None

**Features**

* Add ``recognized_tags`` argument to the linkify ``Linker`` class. This
  fixes issues when linkifying on its own and having some tags get escaped.
  It defaults to a list of HTML5 tags. Thank you, Chad Birch! (#409)

**Bug fixes**

* Add ``six>=1.9`` to requirements. Thank you, Dave Shawley (#416)

* Fix cases where attribute names could have invalid characters in them.
  (#419)

* Fix problems with ``LinkifyFilter`` not being able to match links
  across ``&amp;``. (#422)

* Fix ``InputStreamWithMemory`` when the ``BleachHTMLParser`` is
  parsing ``meta`` tags. (#431)

* Fix doctests. (#357)


Version 3.0.2 (October 11th, 2018)
----------------------------------

**Security fixes**

None

**Backwards incompatible changes**

None

**Features**

None

**Bug fixes**

* Merge ``Characters`` tokens after sanitizing them. This fixes issues in the
  ``LinkifyFilter`` where it was only linkifying parts of urls. (#374)


Version 3.0.1 (October 9th, 2018)
---------------------------------

**Security fixes**

None

**Backwards incompatible changes**

None

**Features**

* Support Python 3.7. It supported Python 3.7 just fine, but we added 3.7 to
  the list of Python environments we test so this is now officially supported.
  (#377)

**Bug fixes**

* Fix ``list`` object has no attribute ``lower`` in ``clean``. (#398)
* Fix ``abbr`` getting escaped in ``linkify``. (#400)


Version 3.0.0 (October 3rd, 2018)
---------------------------------

**Security fixes**

None

**Backwards incompatible changes**

* A bunch of functions were moved from one module to another.

  These were moved from ``bleach.sanitizer`` to ``bleach.html5lib_shim``:

  * ``convert_entity``
  * ``convert_entities``
  * ``match_entity``
  * ``next_possible_entity``
  * ``BleachHTMLSerializer``
  * ``BleachHTMLTokenizer``
  * ``BleachHTMLParser``

  These functions and classes weren't documented and aren't part of the
  public API, but people read code and might be using them so we're
  considering it an incompatible API change.

  If you're using them, you'll need to update your code.

**Features**

* Bleach no longer depends on html5lib. html5lib==1.0.1 is now vendored into
  Bleach. You can remove it from your requirements file if none of your other
  requirements require html5lib.

  This means Bleach will now work fine with other libraries that depend on
  html5lib regardless of what version of html5lib they require. (#386)

**Bug fixes**

* Fixed tags getting added when using clean or linkify. This was a
  long-standing regression from the Bleach 2.0 rewrite. (#280, #392)

* Fixed ``<isindex>`` getting replaced with a string. Now it gets escaped or
  stripped depending on whether it's in the allowed tags or not. (#279)


Version 2.1.4 (August 16th, 2018)
---------------------------------

**Security fixes**

None

**Backwards incompatible changes**

* Dropped support for Python 3.3. (#328)

**Features**

None

**Bug fixes**

* Handle ambiguous ampersands in correctly. (#359)


Version 2.1.3 (March 5th, 2018)
-------------------------------

**Security fixes**

* Attributes that have URI values weren't properly sanitized if the
  values contained character entities. Using character entities, it
  was possible to construct a URI value with a scheme that was not
  allowed that would slide through unsanitized.

  This security issue was introduced in Bleach 2.1. Anyone using
  Bleach 2.1 is highly encouraged to upgrade.

  https://bugzilla.mozilla.org/show_bug.cgi?id=1442745

**Backwards incompatible changes**

None

**Features**

None

**Bug fixes**

* Fixed some other edge cases for attribute URI value sanitizing and
  improved testing of this code.


Version 2.1.2 (December 7th, 2017)
----------------------------------

**Security fixes**

None

**Backwards incompatible changes**

None

**Features**

None

**Bug fixes**

* Support html5lib-python 1.0.1. (#337)

* Add deprecation warning for supporting html5lib-python < 1.0.

* Switch to semver.


Version 2.1.1 (October 2nd, 2017)
---------------------------------

**Security fixes**

None

**Backwards incompatible changes**

None

**Features**

None

**Bug fixes**

* Fix ``setup.py`` opening files when ``LANG=``. (#324)


Version 2.1 (September 28th, 2017)
----------------------------------

**Security fixes**

* Convert control characters (backspace particularly) to "?" preventing
  malicious copy-and-paste situations. (#298)

  See `<https://github.com/mozilla/bleach/issues/298>`_ for more details.

  This affects all previous versions of Bleach. Check the comments on that
  issue for ways to alleviate the issue if you can't upgrade to Bleach 2.1.


**Backwards incompatible changes**

* Redid versioning. ``bleach.VERSION`` is no longer available. Use the string
  version at ``bleach.__version__`` and parse it with
  ``pkg_resources.parse_version``. (#307)

* clean, linkify: linkify and clean should only accept text types; thank you,
  Janusz! (#292)

* clean, linkify: accept only unicode or utf-8-encoded str (#176)


**Features**


**Bug fixes**

* ``bleach.clean()`` no longer unescapes entities including ones that are missing
  a ``;`` at the end which can happen in urls and other places. (#143)

* linkify: fix http links inside of mailto links; thank you, sedrubal! (#300)

* clarify security policy in docs (#303)

* fix dependency specification for html5lib 1.0b8, 1.0b9, and 1.0b10; thank you,
  ZoltÃ¡n! (#268)

* add Bleach vs. html5lib comparison to README; thank you, Stu Cox! (#278)

* fix KeyError exceptions on tags without href attr; thank you, Alex Defsen!
  (#273)

* add test website and scripts to test ``bleach.clean()`` output in browser;
  thank you, Greg Guthe!


Version 2.0 (March 8th, 2017)
-----------------------------

**Security fixes**

* None


**Backwards incompatible changes**

* Removed support for Python 2.6. (#206)

* Removed support for Python 3.2. (#224)

* Bleach no longer supports html5lib < 0.99999999 (8 9s).

  This version is a rewrite to use the new sanitizing API since the old
  one was dropped in html5lib 0.99999999 (8 9s).

  If you're using 0.9999999 (7 9s) upgrade to 0.99999999 (8 9s) or higher.

  If you're using 1.0b8 (equivalent to 0.9999999 (7 9s)), upgrade to 1.0b9
  (equivalent to 0.99999999 (8 9s)) or higher.

* ``bleach.clean`` and friends were rewritten

  ``clean`` was reimplemented as an html5lib filter and happens at a different
  step in the HTML parsing -> traversing -> serializing process. Because of
  that, there are some differences in clean's output as compared with previous
  versions.

  Amongst other things, this version will add end tags even if the tag in
  question is to be escaped.

* ``bleach.clean`` and friends attribute callables now take three arguments:
  tag, attribute name and attribute value. Previously they only took attribute
  name and attribute value.

  All attribute callables will need to be updated.

* ``bleach.linkify`` was rewritten

  ``linkify`` was reimplemented as an html5lib Filter. As such, it no longer
  accepts a ``tokenizer`` argument.

  The callback functions for adjusting link attributes now takes a namespaced
  attribute.

  Previously you'd do something like this::

      def check_protocol(attrs, is_new):
          if not attrs.get('href', '').startswith('http:', 'https:')):
              return None
          return attrs

  Now it's more like this::

      def check_protocol(attrs, is_new):
          if not attrs.get((None, u'href'), u'').startswith(('http:', 'https:')):
              #            ^^^^^^^^^^^^^^^
              return None
          return attrs

  Further, you need to make sure you're always using unicode values. If you
  don't then html5lib will raise an assertion error that the value is not
  unicode.

  All linkify filters will need to be updated.

* ``bleach.linkify`` and friends had a ``skip_pre`` argument--that's been
  replaced with a more general ``skip_tags`` argument.

  Before, you might do::

      bleach.linkify(some_text, skip_pre=True)

  The equivalent with Bleach 2.0 is::

      bleach.linkify(some_text, skip_tags=['pre'])

  You can skip other tags, too, like ``style`` or ``script`` or other places
  where you don't want linkification happening.

  All uses of linkify that use ``skip_pre`` will need to be updated.


**Changes**

* Supports Python 3.6.

* Supports html5lib >= 0.99999999 (8 9s).

* There's a ``bleach.sanitizer.Cleaner`` class that you can instantiate with your
  favorite clean settings for easy reuse.

* There's a ``bleach.linkifier.Linker`` class that you can instantiate with your
  favorite linkify settings for easy reuse.

* There's a ``bleach.linkifier.LinkifyFilter`` which is an htm5lib filter that
  you can pass as a filter to ``bleach.sanitizer.Cleaner`` allowing you to clean
  and linkify in one pass.

* ``bleach.clean`` and friends can now take a callable as an attributes arg value.

* Tons of bug fixes.

* Cleaned up tests.

* Documentation fixes.


Version 1.5 (November 4th, 2016)
--------------------------------

**Security fixes**

* None

**Backwards incompatible changes**

* clean: The list of ``ALLOWED_PROTOCOLS`` now defaults to http, https and
  mailto.

  Previously it was a long list of protocols something like ed2k, ftp, http,
  https, irc, mailto, news, gopher, nntp, telnet, webcal, xmpp, callto, feed,
  urn, aim, rsync, tag, ssh, sftp, rtsp, afs, data. (#149)

**Changes**

* clean: Added ``protocols`` to arguments list to let you override the list of
  allowed protocols. Thank you, Andreas Malecki! (#149)

* linkify: Fix a bug involving periods at the end of an email address. Thank you,
  Lorenz Schori! (#219)

* linkify: Fix linkification of non-ascii ports. Thank you Alexandre, Macabies!
  (#207)

* linkify: Fix linkify inappropriately removing node tails when dropping nodes.
  (#132)

* Fixed a test that failed periodically. (#161)

* Switched from nose to py.test. (#204)

* Add test matrix for all supported Python and html5lib versions. (#230)

* Limit to html5lib ``>=0.999,!=0.9999,!=0.99999,<0.99999999`` because 0.9999
  and 0.99999 are busted.

* Add support for ``python setup.py test``. (#97)


Version 1.4.3 (May 23rd, 2016)
------------------------------

**Security fixes**

* None

**Changes**

* Limit to html5lib ``>=0.999,<0.99999999`` because of impending change to
  sanitizer api. #195


Version 1.4.2 (September 11, 2015)
----------------------------------

**Changes**

* linkify: Fix hang in linkify with ``parse_email=True``. (#124)

* linkify: Fix crash in linkify when removing a link that is a first-child. (#136)

* Updated TLDs.

* linkify: Don't remove exterior brackets when linkifying. (#146)


Version 1.4.1 (December 15, 2014)
---------------------------------

**Changes**

* Consistent order of attributes in output.

* Python 3.4 support.


Version 1.4 (January 12, 2014)
------------------------------

**Changes**

* linkify: Update linkify to use etree type Treewalker instead of simpletree.

* Updated html5lib to version ``>=0.999``.

* Update all code to be compatible with Python 3 and 2 using six.

* Switch to Apache License.


Version 1.3
-----------

* Used by Python 3-only fork.


Version 1.2.2 (May 18, 2013)
----------------------------

* Pin html5lib to version 0.95 for now due to major API break.


Version 1.2.1 (February 19, 2013)
---------------------------------

* ``clean()`` no longer considers ``feed:`` an acceptable protocol due to
  inconsistencies in browser behavior.


Version 1.2 (January 28, 2013)
------------------------------

* ``linkify()`` has changed considerably. Many keyword arguments have been
  replaced with a single callbacks list. Please see the documentation for more
  information.

* Bleach will no longer consider unacceptable protocols when linkifying.

* ``linkify()`` now takes a tokenizer argument that allows it to skip
  sanitization.

* ``delinkify()`` is gone.

* Removed exception handling from ``_render``. ``clean()`` and ``linkify()`` may
  now throw.

* ``linkify()`` correctly ignores case for protocols and domain names.

* ``linkify()`` correctly handles markup within an <a> tag.


Version 1.1.5
-------------


Version 1.1.4
-------------


Version 1.1.3 (July 10, 2012)
-----------------------------

* Fix parsing bare URLs when parse_email=True.


Version 1.1.2 (June 1, 2012)
----------------------------

* Fix hang in style attribute sanitizer. (#61)

* Allow ``/`` in style attribute values.


Version 1.1.1 (February 17, 2012)
---------------------------------

* Fix tokenizer for html5lib 0.9.5.


Version 1.1.0 (October 24, 2011)
--------------------------------

* ``linkify()`` now understands port numbers. (#38)

* Documented character encoding behavior. (#41)

* Add an optional target argument to ``linkify()``.

* Add ``delinkify()`` method. (#45)

* Support subdomain whitelist for ``delinkify()``. (#47, #48)


Version 1.0.4 (September 2, 2011)
---------------------------------

* Switch to SemVer git tags.

* Make ``linkify()`` smarter about trailing punctuation. (#30)

* Pass ``exc_info`` to logger during rendering issues.

* Add wildcard key for attributes. (#19)

* Make ``linkify()`` use the ``HTMLSanitizer`` tokenizer. (#36)

* Fix URLs wrapped in parentheses. (#23)

* Make ``linkify()`` UTF-8 safe. (#33)


Version 1.0.3 (June 14, 2011)
-----------------------------

* ``linkify()`` works with 3rd level domains. (#24)

* ``clean()`` supports vendor prefixes in style values. (#31, #32)

* Fix ``linkify()`` email escaping.


Version 1.0.2 (June 6, 2011)
----------------------------

* ``linkify()`` supports email addresses.

* ``clean()`` supports callables in attributes filter.


Version 1.0.1 (April 12, 2011)
------------------------------

* ``linkify()`` doesn't drop trailing slashes. (#21)
* ``linkify()`` won't linkify 'libgl.so.1'. (#22)
"""^^xsd:string ;
    schema1:name "bleach"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "6.1.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/blinker> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.9> ;
    schema1:description """# Blinker

Blinker provides a fast dispatching system that allows any number of
interested parties to subscribe to events, or "signals".


## Pallets Community Ecosystem

> [!IMPORTANT]\\
> This project is part of the Pallets Community Ecosystem. Pallets is the open
> source organization that maintains Flask; Pallets-Eco enables community
> maintenance of related projects. If you are interested in helping maintain
> this project, please reach out on [the Pallets Discord server][discord].
>
> [discord]: https://discord.gg/pallets


## Example

Signal receivers can subscribe to specific senders or receive signals
sent by any sender.

```pycon
>>> from blinker import signal
>>> started = signal('round-started')
>>> def each(round):
...     print(f"Round {round}")
...
>>> started.connect(each)

>>> def round_two(round):
...     print("This is round two.")
...
>>> started.connect(round_two, sender=2)

>>> for round in range(1, 4):
...     started.send(round)
...
Round 1!
Round 2!
This is round two.
Round 3!
```

"""^^xsd:string ;
    schema1:name "blinker"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.9.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/bottleneck> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.3.6> .

<https://data.siemens-energy.com/cybersec/package/build> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/filelock>,
        <https://data.siemens-energy.com/cybersec/package/furo>,
        <https://data.siemens-energy.com/cybersec/package/importlib-metadata>,
        <https://data.siemens-energy.com/cybersec/package/packaging>,
        <https://data.siemens-energy.com/cybersec/package/pytest>,
        <https://data.siemens-energy.com/cybersec/package/pytest-cov>,
        <https://data.siemens-energy.com/cybersec/package/pytest-mock>,
        <https://data.siemens-energy.com/cybersec/package/pytest-rerunfailures>,
        <https://data.siemens-energy.com/cybersec/package/pytest-xdist>,
        <https://data.siemens-energy.com/cybersec/package/setuptools>,
        <https://data.siemens-energy.com/cybersec/package/sphinx-argparse-cli>,
        <https://data.siemens-energy.com/cybersec/package/sphinx-autodoc-typehints>,
        <https://data.siemens-energy.com/cybersec/package/sphinx-issues>,
        <https://data.siemens-energy.com/cybersec/package/tomli>,
        <https://data.siemens-energy.com/cybersec/package/typing-extensions>,
        <https://data.siemens-energy.com/cybersec/package/uv>,
        <https://data.siemens-energy.com/cybersec/package/virtualenv>,
        <https://data.siemens-energy.com/cybersec/package/wheel> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.0.0> ;
    schema1:description """# build

[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/pypa/build/main.svg)](https://results.pre-commit.ci/latest/github/pypa/build/main)
[![CI test](https://github.com/pypa/build/actions/workflows/test.yml/badge.svg)](https://github.com/pypa/build/actions/workflows/test.yml)
[![codecov](https://codecov.io/gh/pypa/build/branch/main/graph/badge.svg)](https://codecov.io/gh/pypa/build)

[![Documentation Status](https://readthedocs.org/projects/pypa-build/badge/?version=latest)](https://build.pypa.io/en/latest/?badge=latest)
[![PyPI version](https://badge.fury.io/py/build.svg)](https://pypi.org/project/build/)
[![Discord](https://img.shields.io/discord/803025117553754132?label=Discord%20chat%20%23build)](https://discord.gg/pypa)

A simple, correct Python build frontend.

See the [documentation](https://build.pypa.io) for more information.

### Installation

`build` can be installed via `pip` or an equivalent via:

```console
$ pip install build
```

### Usage

```console
$ python -m build
```

This will build the package in an isolated environment, generating a
source-distribution and wheel in the directory `dist/`.
See the [documentation](https://build.pypa.io) for full information.

### Code of Conduct

Everyone interacting in the build's codebase, issue trackers, chat rooms, and mailing lists is expected to follow
the [PSF Code of Conduct].

[psf code of conduct]: https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md

"""^^xsd:string ;
    schema1:name "build"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.2.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/certifi> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2017.4.17> ;
    schema1:description """Certifi: Python SSL Certificates
================================

Certifi provides Mozilla's carefully curated collection of Root Certificates for
validating the trustworthiness of SSL certificates while verifying the identity
of TLS hosts. It has been extracted from the `Requests`_ project.

Installation
------------

``certifi`` is available on PyPI. Simply install it with ``pip``::

    $ pip install certifi

Usage
-----

To reference the installed certificate authority (CA) bundle, you can use the
built-in function::

    >>> import certifi

    >>> certifi.where()
    '/usr/local/lib/python3.7/site-packages/certifi/cacert.pem'

Or from the command line::

    $ python -m certifi
    /usr/local/lib/python3.7/site-packages/certifi/cacert.pem

Enjoy!

.. _`Requests`: https://requests.readthedocs.io/en/master/

Addition/Removal of Certificates
--------------------------------

Certifi does not support any addition/removal or other modification of the
CA trust store content. This project is intended to provide a reliable and
highly portable root of trust to python deployments. Look to upstream projects
for methods to use alternate trust.
"""^^xsd:string ;
    schema1:name "certifi"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2024.7.4"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/chardet> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/6> ;
    schema1:description """Chardet: The Universal Character Encoding Detector
--------------------------------------------------

.. image:: https://img.shields.io/travis/chardet/chardet/stable.svg
   :alt: Build status
   :target: https://travis-ci.org/chardet/chardet

.. image:: https://img.shields.io/coveralls/chardet/chardet/stable.svg
   :target: https://coveralls.io/r/chardet/chardet

.. image:: https://img.shields.io/pypi/v/chardet.svg
   :target: https://warehouse.python.org/project/chardet/
   :alt: Latest version on PyPI

.. image:: https://img.shields.io/pypi/l/chardet.svg
   :alt: License


Detects
 - ASCII, UTF-8, UTF-16 (2 variants), UTF-32 (4 variants)
 - Big5, GB2312, EUC-TW, HZ-GB-2312, ISO-2022-CN (Traditional and Simplified Chinese)
 - EUC-JP, SHIFT_JIS, CP932, ISO-2022-JP (Japanese)
 - EUC-KR, ISO-2022-KR, Johab (Korean)
 - KOI8-R, MacCyrillic, IBM855, IBM866, ISO-8859-5, windows-1251 (Cyrillic)
 - ISO-8859-5, windows-1251 (Bulgarian)
 - ISO-8859-1, windows-1252, MacRoman (Western European languages)
 - ISO-8859-7, windows-1253 (Greek)
 - ISO-8859-8, windows-1255 (Visual and Logical Hebrew)
 - TIS-620 (Thai)

.. note::
   Our ISO-8859-2 and windows-1250 (Hungarian) probers have been temporarily
   disabled until we can retrain the models.

Requires Python 3.7+.

Installation
------------

Install from `PyPI <https://pypi.org/project/chardet/>`_::

    pip install chardet

Documentation
-------------

For users, docs are now available at https://chardet.readthedocs.io/.

Command-line Tool
-----------------

chardet comes with a command-line script which reports on the encodings of one
or more files::

    % chardetect somefile someotherfile
    somefile: windows-1252 with confidence 0.5
    someotherfile: ascii with confidence 1.0

About
-----

This is a continuation of Mark Pilgrim's excellent original chardet port from C, and `Ian Cordasco <https://github.com/sigmavirus24>`_'s
`charade <https://github.com/sigmavirus24/charade>`_ Python 3-compatible fork.

:maintainer: Dan Blanchard
"""^^xsd:string ;
    schema1:name "chardet"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "5.2.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/charset-normalizer> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/4> ;
    schema1:description """<h1 align="center">Charset Detection, for Everyone ðŸ‘‹</h1>

<p align="center">
  <sup>The Real First Universal Charset Detector</sup><br>
  <a href="https://pypi.org/project/charset-normalizer">
    <img src="https://img.shields.io/pypi/pyversions/charset_normalizer.svg?orange=blue" />
  </a>
  <a href="https://pepy.tech/project/charset-normalizer/">
    <img alt="Download Count Total" src="https://static.pepy.tech/badge/charset-normalizer/month" />
  </a>
  <a href="https://bestpractices.coreinfrastructure.org/projects/7297">
    <img src="https://bestpractices.coreinfrastructure.org/projects/7297/badge">
  </a>
</p>
<p align="center">
  <sup><i>Featured Packages</i></sup><br>
  <a href="https://github.com/jawah/niquests">
   <img alt="Static Badge" src="https://img.shields.io/badge/Niquests-HTTP_1.1%2C%202%2C_and_3_Client-cyan">
  </a>
  <a href="https://github.com/jawah/wassima">
   <img alt="Static Badge" src="https://img.shields.io/badge/Wassima-Certifi_Killer-cyan">
  </a>
</p>
<p align="center">
  <sup><i>In other language (unofficial port - by the community)</i></sup><br>
  <a href="https://github.com/nickspring/charset-normalizer-rs">
   <img alt="Static Badge" src="https://img.shields.io/badge/Rust-red">
  </a>
</p>

> A library that helps you read text from an unknown charset encoding.<br /> Motivated by `chardet`,
> I'm trying to resolve the issue by taking a new approach.
> All IANA character set names for which the Python core library provides codecs are supported.

<p align="center">
  >>>>> <a href="https://charsetnormalizerweb.ousret.now.sh" target="_blank">ðŸ‘‰ Try Me Online Now, Then Adopt Me ðŸ‘ˆ </a> <<<<<
</p>

This project offers you an alternative to **Universal Charset Encoding Detector**, also known as **Chardet**.

| Feature                                          | [Chardet](https://github.com/chardet/chardet) |                                         Charset Normalizer                                         | [cChardet](https://github.com/PyYoshi/cChardet) |
|--------------------------------------------------|:---------------------------------------------:|:--------------------------------------------------------------------------------------------------:|:-----------------------------------------------:|
| `Fast`                                           |                       âŒ                       |                                                 âœ…                                                  |                        âœ…                        |
| `Universal**`                                    |                       âŒ                       |                                                 âœ…                                                  |                        âŒ                        |
| `Reliable` **without** distinguishable standards |                       âŒ                       |                                                 âœ…                                                  |                        âœ…                        |
| `Reliable` **with** distinguishable standards    |                       âœ…                       |                                                 âœ…                                                  |                        âœ…                        |
| `License`                                        |           LGPL-2.1<br>_restrictive_           |                                                MIT                                                 |            MPL-1.1<br>_restrictive_             |
| `Native Python`                                  |                       âœ…                       |                                                 âœ…                                                  |                        âŒ                        |
| `Detect spoken language`                         |                       âŒ                       |                                                 âœ…                                                  |                       N/A                       |
| `UnicodeDecodeError Safety`                      |                       âŒ                       |                                                 âœ…                                                  |                        âŒ                        |
| `Whl Size (min)`                                 |                   193.6 kB                    |                                               42 kB                                                |                     ~200 kB                     |
| `Supported Encoding`                             |                      33                       | ðŸŽ‰ [99](https://charset-normalizer.readthedocs.io/en/latest/user/support.html#supported-encodings) |                       40                        |

<p align="center">
<img src="https://i.imgflip.com/373iay.gif" alt="Reading Normalized Text" width="226"/><img src="https://media.tenor.com/images/c0180f70732a18b4965448d33adba3d0/tenor.gif" alt="Cat Reading Text" width="200"/>
</p>

*\\*\\* : They are clearly using specific code for a specific encoding even if covering most of used one*<br> 
Did you got there because of the logs? See [https://charset-normalizer.readthedocs.io/en/latest/user/miscellaneous.html](https://charset-normalizer.readthedocs.io/en/latest/user/miscellaneous.html)

## âš¡ Performance

This package offer better performance than its counterpart Chardet. Here are some numbers.

| Package                                       | Accuracy | Mean per file (ms) | File per sec (est) |
|-----------------------------------------------|:--------:|:------------------:|:------------------:|
| [chardet](https://github.com/chardet/chardet) |   86 %   |       200 ms       |     5 file/sec     |
| charset-normalizer                            | **98 %** |     **10 ms**      |    100 file/sec    |

| Package                                       | 99th percentile | 95th percentile | 50th percentile |
|-----------------------------------------------|:---------------:|:---------------:|:---------------:|
| [chardet](https://github.com/chardet/chardet) |     1200 ms     |     287 ms      |      23 ms      |
| charset-normalizer                            |     100 ms      |      50 ms      |      5 ms       |

Chardet's performance on larger file (1MB+) are very poor. Expect huge difference on large payload.

> Stats are generated using 400+ files using default parameters. More details on used files, see GHA workflows.
> And yes, these results might change at any time. The dataset can be updated to include more files.
> The actual delays heavily depends on your CPU capabilities. The factors should remain the same.
> Keep in mind that the stats are generous and that Chardet accuracy vs our is measured using Chardet initial capability
> (eg. Supported Encoding) Challenge-them if you want.

## âœ¨ Installation

Using pip:

```sh
pip install charset-normalizer -U
```

## ðŸš€ Basic Usage

### CLI
This package comes with a CLI.

```
usage: normalizer [-h] [-v] [-a] [-n] [-m] [-r] [-f] [-t THRESHOLD]
                  file [file ...]

The Real First Universal Charset Detector. Discover originating encoding used
on text file. Normalize text to unicode.

positional arguments:
  files                 File(s) to be analysed

optional arguments:
  -h, --help            show this help message and exit
  -v, --verbose         Display complementary information about file if any.
                        Stdout will contain logs about the detection process.
  -a, --with-alternative
                        Output complementary possibilities if any. Top-level
                        JSON WILL be a list.
  -n, --normalize       Permit to normalize input file. If not set, program
                        does not write anything.
  -m, --minimal         Only output the charset detected to STDOUT. Disabling
                        JSON output.
  -r, --replace         Replace file when trying to normalize it instead of
                        creating a new one.
  -f, --force           Replace file without asking if you are sure, use this
                        flag with caution.
  -t THRESHOLD, --threshold THRESHOLD
                        Define a custom maximum amount of chaos allowed in
                        decoded content. 0. <= chaos <= 1.
  --version             Show version information and exit.
```

```bash
normalizer ./data/sample.1.fr.srt
```

or

```bash
python -m charset_normalizer ./data/sample.1.fr.srt
```

ðŸŽ‰ Since version 1.4.0 the CLI produce easily usable stdout result in JSON format.

```json
{
    "path": "/home/default/projects/charset_normalizer/data/sample.1.fr.srt",
    "encoding": "cp1252",
    "encoding_aliases": [
        "1252",
        "windows_1252"
    ],
    "alternative_encodings": [
        "cp1254",
        "cp1256",
        "cp1258",
        "iso8859_14",
        "iso8859_15",
        "iso8859_16",
        "iso8859_3",
        "iso8859_9",
        "latin_1",
        "mbcs"
    ],
    "language": "French",
    "alphabets": [
        "Basic Latin",
        "Latin-1 Supplement"
    ],
    "has_sig_or_bom": false,
    "chaos": 0.149,
    "coherence": 97.152,
    "unicode_path": null,
    "is_preferred": true
}
```

### Python
*Just print out normalized text*
```python
from charset_normalizer import from_path

results = from_path('./my_subtitle.srt')

print(str(results.best()))
```

*Upgrade your code without effort*
```python
from charset_normalizer import detect
```

The above code will behave the same as **chardet**. We ensure that we offer the best (reasonable) BC result possible.

See the docs for advanced usage : [readthedocs.io](https://charset-normalizer.readthedocs.io/en/latest/)

## ðŸ˜‡ Why

When I started using Chardet, I noticed that it was not suited to my expectations, and I wanted to propose a
reliable alternative using a completely different method. Also! I never back down on a good challenge!

I **don't care** about the **originating charset** encoding, because **two different tables** can
produce **two identical rendered string.**
What I want is to get readable text, the best I can. 

In a way, **I'm brute forcing text decoding.** How cool is that ? ðŸ˜Ž

Don't confuse package **ftfy** with charset-normalizer or chardet. ftfy goal is to repair unicode string whereas charset-normalizer to convert raw file in unknown encoding to unicode.

## ðŸ° How

  - Discard all charset encoding table that could not fit the binary content.
  - Measure noise, or the mess once opened (by chunks) with a corresponding charset encoding.
  - Extract matches with the lowest mess detected.
  - Additionally, we measure coherence / probe for a language.

**Wait a minute**, what is noise/mess and coherence according to **YOU ?**

*Noise :* I opened hundred of text files, **written by humans**, with the wrong encoding table. **I observed**, then
**I established** some ground rules about **what is obvious** when **it seems like** a mess.
 I know that my interpretation of what is noise is probably incomplete, feel free to contribute in order to
 improve or rewrite it.

*Coherence :* For each language there is on earth, we have computed ranked letter appearance occurrences (the best we can). So I thought
that intel is worth something here. So I use those records against decoded text to check if I can detect intelligent design.

## âš¡ Known limitations

  - Language detection is unreliable when text contains two or more languages sharing identical letters. (eg. HTML (english tags) + Turkish content (Sharing Latin characters))
  - Every charset detector heavily depends on sufficient content. In common cases, do not bother run detection on very tiny content.

## âš ï¸ About Python EOLs

**If you are running:**

- Python >=2.7,<3.5: Unsupported
- Python 3.5: charset-normalizer < 2.1
- Python 3.6: charset-normalizer < 3.1
- Python 3.7: charset-normalizer < 4.0

Upgrade your Python interpreter as soon as possible.

## ðŸ‘¤ Contributing

Contributions, issues and feature requests are very much welcome.<br />
Feel free to check [issues page](https://github.com/ousret/charset_normalizer/issues) if you want to contribute.

## ðŸ“ License

Copyright Â© [Ahmed TAHRI @Ousret](https://github.com/Ousret).<br />
This project is [MIT](https://github.com/Ousret/charset_normalizer/blob/master/LICENSE) licensed.

Characters frequencies used in this project Â© 2012 [Denny VrandeÄiÄ‡](http://simia.net/letters/)

## ðŸ’¼ For Enterprise

Professional support for charset-normalizer is available as part of the [Tidelift
Subscription][1]. Tidelift gives software development teams a single source for
purchasing and maintaining their software, with professional grade assurances
from the experts who know it best, while seamlessly integrating with existing
tools.

[1]: https://tidelift.com/subscription/pkg/pypi-charset-normalizer?utm_source=pypi-charset-normalizer&utm_medium=readme

# Changelog
All notable changes to charset-normalizer will be documented in this file. This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).
The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/).

## [3.3.2](https://github.com/Ousret/charset_normalizer/compare/3.3.1...3.3.2) (2023-10-31)

### Fixed
- Unintentional memory usage regression when using large payload that match several encoding (#376)
- Regression on some detection case showcased in the documentation (#371)

### Added
- Noise (md) probe that identify malformed arabic representation due to the presence of letters in isolated form (credit to my wife)

## [3.3.1](https://github.com/Ousret/charset_normalizer/compare/3.3.0...3.3.1) (2023-10-22)

### Changed
- Optional mypyc compilation upgraded to version 1.6.1 for Python >= 3.8
- Improved the general detection reliability based on reports from the community

## [3.3.0](https://github.com/Ousret/charset_normalizer/compare/3.2.0...3.3.0) (2023-09-30)

### Added
- Allow to execute the CLI (e.g. normalizer) through `python -m charset_normalizer.cli` or `python -m charset_normalizer`
- Support for 9 forgotten encoding that are supported by Python but unlisted in `encoding.aliases` as they have no alias (#323)

### Removed
- (internal) Redundant utils.is_ascii function and unused function is_private_use_only
- (internal) charset_normalizer.assets is moved inside charset_normalizer.constant

### Changed
- (internal) Unicode code blocks in constants are updated using the latest v15.0.0 definition to improve detection
- Optional mypyc compilation upgraded to version 1.5.1 for Python >= 3.8

### Fixed
- Unable to properly sort CharsetMatch when both chaos/noise and coherence were close due to an unreachable condition in \\_\\_lt\\_\\_ (#350)

## [3.2.0](https://github.com/Ousret/charset_normalizer/compare/3.1.0...3.2.0) (2023-06-07)

### Changed
- Typehint for function `from_path` no longer enforce `PathLike` as its first argument
- Minor improvement over the global detection reliability

### Added
- Introduce function `is_binary` that relies on main capabilities, and optimized to detect binaries
- Propagate `enable_fallback` argument throughout `from_bytes`, `from_path`, and `from_fp` that allow a deeper control over the detection (default True)
- Explicit support for Python 3.12

### Fixed
- Edge case detection failure where a file would contain 'very-long' camel cased word (Issue #289)

## [3.1.0](https://github.com/Ousret/charset_normalizer/compare/3.0.1...3.1.0) (2023-03-06)

### Added
- Argument `should_rename_legacy` for legacy function `detect` and disregard any new arguments without errors (PR #262)

### Removed
- Support for Python 3.6 (PR #260)

### Changed
- Optional speedup provided by mypy/c 1.0.1

## [3.0.1](https://github.com/Ousret/charset_normalizer/compare/3.0.0...3.0.1) (2022-11-18)

### Fixed
- Multi-bytes cutter/chunk generator did not always cut correctly (PR #233)

### Changed
- Speedup provided by mypy/c 0.990 on Python >= 3.7

## [3.0.0](https://github.com/Ousret/charset_normalizer/compare/2.1.1...3.0.0) (2022-10-20)

### Added
- Extend the capability of explain=True when cp_isolation contains at most two entries (min one), will log in details of the Mess-detector results
- Support for alternative language frequency set in charset_normalizer.assets.FREQUENCIES
- Add parameter `language_threshold` in `from_bytes`, `from_path` and `from_fp` to adjust the minimum expected coherence ratio
- `normalizer --version` now specify if current version provide extra speedup (meaning mypyc compilation whl)

### Changed
- Build with static metadata using 'build' frontend
- Make the language detection stricter
- Optional: Module `md.py` can be compiled using Mypyc to provide an extra speedup up to 4x faster than v2.1

### Fixed
- CLI with opt --normalize fail when using full path for files
- TooManyAccentuatedPlugin induce false positive on the mess detection when too few alpha character have been fed to it
- Sphinx warnings when generating the documentation

### Removed
- Coherence detector no longer return 'Simple English' instead return 'English'
- Coherence detector no longer return 'Classical Chinese' instead return 'Chinese'
- Breaking: Method `first()` and `best()` from CharsetMatch
- UTF-7 will no longer appear as "detected" without a recognized SIG/mark (is unreliable/conflict with ASCII)
- Breaking: Class aliases CharsetDetector, CharsetDoctor, CharsetNormalizerMatch and CharsetNormalizerMatches
- Breaking: Top-level function `normalize`
- Breaking: Properties `chaos_secondary_pass`, `coherence_non_latin` and `w_counter` from CharsetMatch
- Support for the backport `unicodedata2`

## [3.0.0rc1](https://github.com/Ousret/charset_normalizer/compare/3.0.0b2...3.0.0rc1) (2022-10-18)

### Added
- Extend the capability of explain=True when cp_isolation contains at most two entries (min one), will log in details of the Mess-detector results
- Support for alternative language frequency set in charset_normalizer.assets.FREQUENCIES
- Add parameter `language_threshold` in `from_bytes`, `from_path` and `from_fp` to adjust the minimum expected coherence ratio

### Changed
- Build with static metadata using 'build' frontend
- Make the language detection stricter

### Fixed
- CLI with opt --normalize fail when using full path for files
- TooManyAccentuatedPlugin induce false positive on the mess detection when too few alpha character have been fed to it

### Removed
- Coherence detector no longer return 'Simple English' instead return 'English'
- Coherence detector no longer return 'Classical Chinese' instead return 'Chinese'

## [3.0.0b2](https://github.com/Ousret/charset_normalizer/compare/3.0.0b1...3.0.0b2) (2022-08-21)

### Added
- `normalizer --version` now specify if current version provide extra speedup (meaning mypyc compilation whl)

### Removed
- Breaking: Method `first()` and `best()` from CharsetMatch
- UTF-7 will no longer appear as "detected" without a recognized SIG/mark (is unreliable/conflict with ASCII)

### Fixed
- Sphinx warnings when generating the documentation

## [3.0.0b1](https://github.com/Ousret/charset_normalizer/compare/2.1.0...3.0.0b1) (2022-08-15)

### Changed
- Optional: Module `md.py` can be compiled using Mypyc to provide an extra speedup up to 4x faster than v2.1

### Removed
- Breaking: Class aliases CharsetDetector, CharsetDoctor, CharsetNormalizerMatch and CharsetNormalizerMatches
- Breaking: Top-level function `normalize`
- Breaking: Properties `chaos_secondary_pass`, `coherence_non_latin` and `w_counter` from CharsetMatch
- Support for the backport `unicodedata2`

## [2.1.1](https://github.com/Ousret/charset_normalizer/compare/2.1.0...2.1.1) (2022-08-19)

### Deprecated
- Function `normalize` scheduled for removal in 3.0

### Changed
- Removed useless call to decode in fn is_unprintable (#206)

### Fixed
- Third-party library (i18n xgettext) crashing not recognizing utf_8 (PEP 263) with underscore from [@aleksandernovikov](https://github.com/aleksandernovikov) (#204)

## [2.1.0](https://github.com/Ousret/charset_normalizer/compare/2.0.12...2.1.0) (2022-06-19)

### Added
- Output the Unicode table version when running the CLI with `--version` (PR #194)

### Changed
- Re-use decoded buffer for single byte character sets from [@nijel](https://github.com/nijel) (PR #175)
- Fixing some performance bottlenecks from [@deedy5](https://github.com/deedy5) (PR #183)

### Fixed
- Workaround potential bug in cpython with Zero Width No-Break Space located in Arabic Presentation Forms-B, Unicode 1.1 not acknowledged as space (PR #175)
- CLI default threshold aligned with the API threshold from [@oleksandr-kuzmenko](https://github.com/oleksandr-kuzmenko) (PR #181)

### Removed
- Support for Python 3.5 (PR #192)

### Deprecated
- Use of backport unicodedata from `unicodedata2` as Python is quickly catching up, scheduled for removal in 3.0 (PR #194)

## [2.0.12](https://github.com/Ousret/charset_normalizer/compare/2.0.11...2.0.12) (2022-02-12)

### Fixed
- ASCII miss-detection on rare cases (PR #170) 

## [2.0.11](https://github.com/Ousret/charset_normalizer/compare/2.0.10...2.0.11) (2022-01-30)

### Added
- Explicit support for Python 3.11 (PR #164)

### Changed
- The logging behavior have been completely reviewed, now using only TRACE and DEBUG levels (PR #163 #165)

## [2.0.10](https://github.com/Ousret/charset_normalizer/compare/2.0.9...2.0.10) (2022-01-04)

### Fixed
- Fallback match entries might lead to UnicodeDecodeError for large bytes sequence (PR #154)

### Changed
- Skipping the language-detection (CD) on ASCII (PR #155)

## [2.0.9](https://github.com/Ousret/charset_normalizer/compare/2.0.8...2.0.9) (2021-12-03)

### Changed
- Moderating the logging impact (since 2.0.8) for specific environments (PR #147)

### Fixed
- Wrong logging level applied when setting kwarg `explain` to True (PR #146)

## [2.0.8](https://github.com/Ousret/charset_normalizer/compare/2.0.7...2.0.8) (2021-11-24)
### Changed
- Improvement over Vietnamese detection (PR #126)
- MD improvement on trailing data and long foreign (non-pure latin) data (PR #124)
- Efficiency improvements in cd/alphabet_languages from [@adbar](https://github.com/adbar) (PR #122)
- call sum() without an intermediary list following PEP 289 recommendations from [@adbar](https://github.com/adbar) (PR #129)
- Code style as refactored by Sourcery-AI (PR #131) 
- Minor adjustment on the MD around european words (PR #133)
- Remove and replace SRTs from assets / tests (PR #139)
- Initialize the library logger with a `NullHandler` by default from [@nmaynes](https://github.com/nmaynes) (PR #135)
- Setting kwarg `explain` to True will add provisionally (bounded to function lifespan) a specific stream handler (PR #135)

### Fixed
- Fix large (misleading) sequence giving UnicodeDecodeError (PR #137)
- Avoid using too insignificant chunk (PR #137)

### Added
- Add and expose function `set_logging_handler` to configure a specific StreamHandler from [@nmaynes](https://github.com/nmaynes) (PR #135)
- Add `CHANGELOG.md` entries, format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/) (PR #141)

## [2.0.7](https://github.com/Ousret/charset_normalizer/compare/2.0.6...2.0.7) (2021-10-11)
### Added
- Add support for Kazakh (Cyrillic) language detection (PR #109)

### Changed
- Further, improve inferring the language from a given single-byte code page (PR #112)
- Vainly trying to leverage PEP263 when PEP3120 is not supported (PR #116)
- Refactoring for potential performance improvements in loops from [@adbar](https://github.com/adbar) (PR #113)
- Various detection improvement (MD+CD) (PR #117)

### Removed
- Remove redundant logging entry about detected language(s) (PR #115)

### Fixed
- Fix a minor inconsistency between Python 3.5 and other versions regarding language detection (PR #117 #102)

## [2.0.6](https://github.com/Ousret/charset_normalizer/compare/2.0.5...2.0.6) (2021-09-18)
### Fixed
- Unforeseen regression with the loss of the backward-compatibility with some older minor of Python 3.5.x (PR #100)
- Fix CLI crash when using --minimal output in certain cases (PR #103)

### Changed
- Minor improvement to the detection efficiency (less than 1%) (PR #106 #101)

## [2.0.5](https://github.com/Ousret/charset_normalizer/compare/2.0.4...2.0.5) (2021-09-14)
### Changed
- The project now comply with: flake8, mypy, isort and black to ensure a better overall quality (PR #81)
- The BC-support with v1.x was improved, the old staticmethods are restored (PR #82)
- The Unicode detection is slightly improved (PR #93)
- Add syntax sugar \\_\\_bool\\_\\_ for results CharsetMatches list-container (PR #91)

### Removed
- The project no longer raise warning on tiny content given for detection, will be simply logged as warning instead (PR #92)

### Fixed
- In some rare case, the chunks extractor could cut in the middle of a multi-byte character and could mislead the mess detection (PR #95)
- Some rare 'space' characters could trip up the UnprintablePlugin/Mess detection (PR #96)
- The MANIFEST.in was not exhaustive (PR #78)

## [2.0.4](https://github.com/Ousret/charset_normalizer/compare/2.0.3...2.0.4) (2021-07-30)
### Fixed
- The CLI no longer raise an unexpected exception when no encoding has been found (PR #70)
- Fix accessing the 'alphabets' property when the payload contains surrogate characters (PR #68)
- The logger could mislead (explain=True) on detected languages and the impact of one MBCS match (PR #72)
- Submatch factoring could be wrong in rare edge cases (PR #72)
- Multiple files given to the CLI were ignored when publishing results to STDOUT. (After the first path) (PR #72)
- Fix line endings from CRLF to LF for certain project files (PR #67)

### Changed
- Adjust the MD to lower the sensitivity, thus improving the global detection reliability (PR #69 #76)
- Allow fallback on specified encoding if any (PR #71)

## [2.0.3](https://github.com/Ousret/charset_normalizer/compare/2.0.2...2.0.3) (2021-07-16)
### Changed
- Part of the detection mechanism has been improved to be less sensitive, resulting in more accurate detection results. Especially ASCII. (PR #63)
- According to the community wishes, the detection will fall back on ASCII or UTF-8 in a last-resort case. (PR #64)

## [2.0.2](https://github.com/Ousret/charset_normalizer/compare/2.0.1...2.0.2) (2021-07-15)
### Fixed
- Empty/Too small JSON payload miss-detection fixed. Report from [@tseaver](https://github.com/tseaver) (PR #59) 

### Changed
- Don't inject unicodedata2 into sys.modules from [@akx](https://github.com/akx) (PR #57)

## [2.0.1](https://github.com/Ousret/charset_normalizer/compare/2.0.0...2.0.1) (2021-07-13)
### Fixed
- Make it work where there isn't a filesystem available, dropping assets frequencies.json. Report from [@sethmlarson](https://github.com/sethmlarson). (PR #55)
- Using explain=False permanently disable the verbose output in the current runtime (PR #47)
- One log entry (language target preemptive) was not show in logs when using explain=True (PR #47)
- Fix undesired exception (ValueError) on getitem of instance CharsetMatches (PR #52)

### Changed
- Public function normalize default args values were not aligned with from_bytes (PR #53)

### Added
- You may now use charset aliases in cp_isolation and cp_exclusion arguments (PR #47)

## [2.0.0](https://github.com/Ousret/charset_normalizer/compare/1.4.1...2.0.0) (2021-07-02)
### Changed
- 4x to 5 times faster than the previous 1.4.0 release. At least 2x faster than Chardet.
- Accent has been made on UTF-8 detection, should perform rather instantaneous.
- The backward compatibility with Chardet has been greatly improved. The legacy detect function returns an identical charset name whenever possible.
- The detection mechanism has been slightly improved, now Turkish content is detected correctly (most of the time)
- The program has been rewritten to ease the readability and maintainability. (+Using static typing)+
- utf_7 detection has been reinstated.

### Removed
- This package no longer require anything when used with Python 3.5 (Dropped cached_property)
- Removed support for these languages: Catalan, Esperanto, Kazakh, Baque, VolapÃ¼k, Azeri, Galician, Nynorsk, Macedonian, and Serbocroatian.
- The exception hook on UnicodeDecodeError has been removed.

### Deprecated
- Methods coherence_non_latin, w_counter, chaos_secondary_pass of the class CharsetMatch are now deprecated and scheduled for removal in v3.0

### Fixed
- The CLI output used the relative path of the file(s). Should be absolute.

## [1.4.1](https://github.com/Ousret/charset_normalizer/compare/1.4.0...1.4.1) (2021-05-28)
### Fixed
- Logger configuration/usage no longer conflict with others (PR #44)

## [1.4.0](https://github.com/Ousret/charset_normalizer/compare/1.3.9...1.4.0) (2021-05-21)
### Removed
- Using standard logging instead of using the package loguru.
- Dropping nose test framework in favor of the maintained pytest.
- Choose to not use dragonmapper package to help with gibberish Chinese/CJK text.
- Require cached_property only for Python 3.5 due to constraint. Dropping for every other interpreter version.
- Stop support for UTF-7 that does not contain a SIG.
- Dropping PrettyTable, replaced with pure JSON output in CLI.

### Fixed
- BOM marker in a CharsetNormalizerMatch instance could be False in rare cases even if obviously present. Due to the sub-match factoring process.
- Not searching properly for the BOM when trying utf32/16 parent codec.

### Changed
- Improving the package final size by compressing frequencies.json.
- Huge improvement over the larges payload.

### Added
- CLI now produces JSON consumable output.
- Return ASCII if given sequences fit. Given reasonable confidence.

## [1.3.9](https://github.com/Ousret/charset_normalizer/compare/1.3.8...1.3.9) (2021-05-13)

### Fixed
- In some very rare cases, you may end up getting encode/decode errors due to a bad bytes payload (PR #40)

## [1.3.8](https://github.com/Ousret/charset_normalizer/compare/1.3.7...1.3.8) (2021-05-12)

### Fixed
- Empty given payload for detection may cause an exception if trying to access the `alphabets` property. (PR #39)

## [1.3.7](https://github.com/Ousret/charset_normalizer/compare/1.3.6...1.3.7) (2021-05-12)

### Fixed
- The legacy detect function should return UTF-8-SIG if sig is present in the payload. (PR #38)

## [1.3.6](https://github.com/Ousret/charset_normalizer/compare/1.3.5...1.3.6) (2021-02-09)

### Changed
- Amend the previous release to allow prettytable 2.0 (PR #35)

## [1.3.5](https://github.com/Ousret/charset_normalizer/compare/1.3.4...1.3.5) (2021-02-08)

### Fixed
- Fix error while using the package with a python pre-release interpreter (PR #33)

### Changed
- Dependencies refactoring, constraints revised.

### Added
- Add python 3.9 and 3.10 to the supported interpreters

MIT License

Copyright (c) 2019 TAHRI Ahmed R.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
"""^^xsd:string ;
    schema1:name "charset-normalizer"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.3.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/contourpy> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/mypy>,
        <https://data.siemens-energy.com/cybersec/package/numpy>,
        <https://data.siemens-energy.com/cybersec/package/sphinx> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.0.1> ;
    schema1:description """<img alt="ContourPy" src="https://raw.githubusercontent.com/contourpy/contourpy/main/docs/_static/contourpy_logo_horiz.svg" height="90">

ContourPy is a Python library for calculating contours of 2D quadrilateral grids.  It is written in C++11 and wrapped using pybind11.

It contains the 2005 and 2014 algorithms used in Matplotlib as well as a newer algorithm that includes more features and is available in both serial and multithreaded versions.  It provides an easy way for Python libraries to use contouring algorithms without having to include Matplotlib as a dependency.

  * **Documentation**: https://contourpy.readthedocs.io
  * **Source code**: https://github.com/contourpy/contourpy

| | |
| --- | --- |
| Latest release | [![PyPI version](https://img.shields.io/pypi/v/contourpy.svg?label=pypi&color=fdae61)](https://pypi.python.org/pypi/contourpy) [![conda-forge version](https://img.shields.io/conda/v/conda-forge/contourpy.svg?label=conda-forge&color=a6d96a)](https://anaconda.org/conda-forge/contourpy) |
| Downloads | [![PyPi downloads](https://img.shields.io/pypi/dm/contourpy?label=pypi&style=flat&color=fdae61)](https://pepy.tech/project/contourpy) |
| Python version | [![Platforms](https://img.shields.io/pypi/pyversions/contourpy?color=fdae61)](https://pypi.org/project/contourpy/) |
| Coverage | [![Codecov](https://img.shields.io/codecov/c/gh/contourpy/contourpy?color=fdae61&label=codecov)](https://app.codecov.io/gh/contourpy/contourpy) |
"""^^xsd:string ;
    schema1:name "contourpy"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.3.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/coverage-enable-subprocess> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1> .

<https://data.siemens-energy.com/cybersec/package/cssselect> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.7> .

<https://data.siemens-energy.com/cybersec/package/cycler> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.10> ;
    schema1:description """|PyPi|_ |Conda|_ |Supported Python versions|_ |GitHub Actions|_ |Codecov|_

.. |PyPi| image:: https://img.shields.io/pypi/v/cycler.svg?style=flat
.. _PyPi: https://pypi.python.org/pypi/cycler

.. |Conda| image:: https://img.shields.io/conda/v/conda-forge/cycler
.. _Conda:  https://anaconda.org/conda-forge/cycler

.. |Supported Python versions| image:: https://img.shields.io/pypi/pyversions/cycler.svg
.. _Supported Python versions: https://pypi.python.org/pypi/cycler

.. |GitHub Actions| image:: https://github.com/matplotlib/cycler/actions/workflows/tests.yml/badge.svg
.. _GitHub Actions: https://github.com/matplotlib/cycler/actions

.. |Codecov| image:: https://codecov.io/github/matplotlib/cycler/badge.svg?branch=main&service=github
.. _Codecov: https://codecov.io/github/matplotlib/cycler?branch=main

cycler: composable cycles
=========================

Docs: https://matplotlib.org/cycler/
"""^^xsd:string ;
    schema1:name "cycler"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.12.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/dataframe-api-compat> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.1.7> .

<https://data.siemens-energy.com/cybersec/package/dateparser> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/regex> ;
    schema1:description """==========================
Introduction to dateparser
==========================


Features
========

* Generic parsing of dates in over 200 language locales plus numerous formats in a language agnostic fashion.
* Generic parsing of relative dates like: ``'1 min ago'``, ``'2 weeks ago'``, ``'3 months, 1 week and 1 day ago'``, ``'in 2 days'``, ``'tomorrow'``.
* Generic parsing of dates with time zones abbreviations or UTC offsets like: ``'August 14, 2015 EST'``, ``'July 4, 2013 PST'``, ``'21 July 2013 10:15 pm +0500'``.
* Date lookup in longer texts.
* Support for non-Gregorian calendar systems. See `Supported Calendars`_.
* Extensive test coverage.


Basic Usage
===========

The most straightforward way is to use the `dateparser.parse <#dateparser.parse>`_ function,
that wraps around most of the functionality in the module.


   
   :noindex:


Popular Formats
---------------

    >>> import dateparser
    >>> dateparser.parse('12/12/12')
    datetime.datetime(2012, 12, 12, 0, 0)
    >>> dateparser.parse('Fri, 12 Dec 2014 10:55:50')
    datetime.datetime(2014, 12, 12, 10, 55, 50)
    >>> dateparser.parse('Martes 21 de Octubre de 2014')  # Spanish (Tuesday 21 October 2014)
    datetime.datetime(2014, 10, 21, 0, 0)
    >>> dateparser.parse('Le 11 DÃ©cembre 2014 Ã  09:00')  # French (11 December 2014 at 09:00)
    datetime.datetime(2014, 12, 11, 9, 0)
    >>> dateparser.parse('13 ÑÐ½Ð²Ð°Ñ€Ñ 2015 Ð³. Ð² 13:34')  # Russian (13 January 2015 at 13:34)
    datetime.datetime(2015, 1, 13, 13, 34)
    >>> dateparser.parse('1 à¹€à¸”à¸·à¸­à¸™à¸•à¸¸à¸¥à¸²à¸„à¸¡ 2005, 1:00 AM')  # Thai (1 October 2005, 1:00 AM)
    datetime.datetime(2005, 10, 1, 1, 0)

This will try to parse a date from the given string, attempting to
detect the language each time.

You can specify the language(s), if known, using ``languages`` argument. In this case, given languages are used and language detection is skipped:

    >>> dateparser.parse('2015, Ago 15, 1:08 pm', languages=['pt', 'es'])
    datetime.datetime(2015, 8, 15, 13, 8)

If you know the possible formats of the dates, you can
use the ``date_formats`` argument:

    >>> dateparser.parse('22 DÃ©cembre 2010', date_formats=['%d %B %Y'])
    datetime.datetime(2010, 12, 22, 0, 0)


Relative Dates
--------------

    >>> parse('1 hour ago')
    datetime.datetime(2015, 5, 31, 23, 0)
    >>> parse('Il ya 2 heures')  # French (2 hours ago)
    datetime.datetime(2015, 5, 31, 22, 0)
    >>> parse('1 anno 2 mesi')  # Italian (1 year 2 months)
    datetime.datetime(2014, 4, 1, 0, 0)
    >>> parse('yaklaÅŸÄ±k 23 saat Ã¶nce')  # Turkish (23 hours ago)
    datetime.datetime(2015, 5, 31, 1, 0)
    >>> parse('Hace una semana')  # Spanish (a week ago)
    datetime.datetime(2015, 5, 25, 0, 0)
    >>> parse('2å°æ—¶å‰')  # Chinese (2 hours ago)
    datetime.datetime(2015, 5, 31, 22, 0)

.. note:: Testing above code might return different values for you depending on your environment's current date and time.

.. note:: For `Finnish` language, please specify ``settings={'SKIP_TOKENS': []}`` to correctly parse relative dates.

OOTB Language Based Date Order Preference
-----------------------------------------

   >>> # parsing ambiguous date
   >>> parse('02-03-2016')  # assumes english language, uses MDY date order
   datetime.datetime(2016, 2, 3, 0, 0)
   >>> parse('le 02-03-2016')  # detects french, uses DMY date order
   datetime.datetime(2016, 3, 2, 0, 0)

.. note:: Ordering is not locale based, that's why do not expect `DMY` order for UK/Australia English. You can specify date order in that case as follows using `settings`:

    >>> parse('18-12-15 06:00', settings={'DATE_ORDER': 'DMY'})
    datetime.datetime(2015, 12, 18, 6, 0)

For more on date order, please look at `settings`.


Timezone and UTC Offset
-----------------------

By default, `dateparser` returns tzaware `datetime` if timezone is present in date string. Otherwise, it returns a naive `datetime` object.

    >>> parse('January 12, 2012 10:00 PM EST')
    datetime.datetime(2012, 1, 12, 22, 0, tzinfo=<StaticTzInfo 'EST'>)

    >>> parse('January 12, 2012 10:00 PM -0500')
    datetime.datetime(2012, 1, 12, 22, 0, tzinfo=<StaticTzInfo 'UTC\\-05:00'>)

    >>> parse('2 hours ago EST')
    datetime.datetime(2017, 3, 10, 15, 55, 39, 579667, tzinfo=<StaticTzInfo 'EST'>)

    >>> parse('2 hours ago -0500')
    datetime.datetime(2017, 3, 10, 15, 59, 30, 193431, tzinfo=<StaticTzInfo 'UTC\\-05:00'>)

 If date has no timezone name/abbreviation or offset, you can specify it using `TIMEZONE` setting.

    >>> parse('January 12, 2012 10:00 PM', settings={'TIMEZONE': 'US/Eastern'})
    datetime.datetime(2012, 1, 12, 22, 0)

    >>> parse('January 12, 2012 10:00 PM', settings={'TIMEZONE': '+0500'})
    datetime.datetime(2012, 1, 12, 22, 0)

``TIMEZONE`` option may not be useful alone as it only attaches given timezone to
resultant ``datetime`` object. But can be useful in cases where you want conversions from and to different
timezones or when simply want a tzaware date with given timezone info attached.

    >>> parse('January 12, 2012 10:00 PM', settings={'TIMEZONE': 'US/Eastern', 'RETURN_AS_TIMEZONE_AWARE': True})
    datetime.datetime(2012, 1, 12, 22, 0, tzinfo=<DstTzInfo 'US/Eastern' EST-1 day, 19:00:00 STD>)


    >>> parse('10:00 am', settings={'TIMEZONE': 'EST', 'TO_TIMEZONE': 'EDT'})
    datetime.datetime(2016, 9, 25, 11, 0)

Some more use cases for conversion of timezones.

    >>> parse('10:00 am EST', settings={'TO_TIMEZONE': 'EDT'})  # date string has timezone info
    datetime.datetime(2017, 3, 12, 11, 0, tzinfo=<StaticTzInfo 'EDT'>)

    >>> parse('now EST', settings={'TO_TIMEZONE': 'UTC'})  # relative dates
    datetime.datetime(2017, 3, 10, 23, 24, 47, 371823, tzinfo=<StaticTzInfo 'UTC'>)

In case, no timezone is present in date string or defined in `settings`. You can still
return tzaware ``datetime``. It is especially useful in case of relative dates when uncertain
what timezone is relative base.

    >>> parse('2 minutes ago', settings={'RETURN_AS_TIMEZONE_AWARE': True})
    datetime.datetime(2017, 3, 11, 4, 25, 24, 152670, tzinfo=<DstTzInfo 'Asia/Karachi' PKT+5:00:00 STD>)

In case, you want to compute relative dates in UTC instead of default system's local timezone, you can use `TIMEZONE` setting.

    >>> parse('4 minutes ago', settings={'TIMEZONE': 'UTC'})
    datetime.datetime(2017, 3, 10, 23, 27, 59, 647248, tzinfo=<StaticTzInfo 'UTC'>)

.. note:: In case, when timezone is present both in string and also specified using `settings`, string is parsed into tzaware representation and then converted to timezone specified in `settings`.

   >>> parse('10:40 pm PKT', settings={'TIMEZONE': 'UTC'})
   datetime.datetime(2017, 3, 12, 17, 40, tzinfo=<StaticTzInfo 'UTC'>)

   >>> parse('20 mins ago EST', settings={'TIMEZONE': 'UTC'})
   datetime.datetime(2017, 3, 12, 21, 16, 0, 885091, tzinfo=<StaticTzInfo 'UTC'>)

For more on timezones, please look at `settings`.


Incomplete Dates
----------------

    >>> from dateparser import parse
    >>> parse('December 2015')  # default behavior
    datetime.datetime(2015, 12, 16, 0, 0)
    >>> parse('December 2015', settings={'PREFER_DAY_OF_MONTH': 'last'})
    datetime.datetime(2015, 12, 31, 0, 0)
    >>> parse('December 2015', settings={'PREFER_DAY_OF_MONTH': 'first'})
    datetime.datetime(2015, 12, 1, 0, 0)

    >>> parse('March')
    datetime.datetime(2015, 3, 16, 0, 0)
    >>> parse('March', settings={'PREFER_DATES_FROM': 'future'})
    datetime.datetime(2016, 3, 16, 0, 0)
    >>> # parsing with preference set for 'past'
    >>> parse('August', settings={'PREFER_DATES_FROM': 'past'})
    datetime.datetime(2015, 8, 15, 0, 0)

    >>> import dateparser
    >>> dateparser.parse("2015") # default behavior
    datetime.datetime(2015, 3, 27, 0, 0)
    >>> dateparser.parse("2015", settings={"PREFER_MONTH_OF_YEAR": "last"})
    datetime.datetime(2015, 12, 27, 0, 0)
    >>> dateparser.parse("2015", settings={"PREFER_MONTH_OF_YEAR": "first"})
    datetime.datetime(2015, 1, 27, 0, 0)
    >>> dateparser.parse("2015", settings={"PREFER_MONTH_OF_YEAR": "current"})
    datetime.datetime(2015, 3, 27, 0, 0)

You can also ignore parsing incomplete dates altogether by setting `STRICT_PARSING` flag as follows:

    >>> parse('December 2015', settings={'STRICT_PARSING': True})
    None

For more on handling incomplete dates, please look at `settings`.


Search for Dates in Longer Chunks of Text
-----------------------------------------

.. warning:: Support for searching dates is really limited and needs a lot of improvement, we look forward to community's contribution to get better on that part. See "`contributing`".


You can extract dates from longer strings of text. They are returned as list of tuples with text chunk containing the date and parsed datetime object.



   
   :noindex:

Advanced Usage
==============
If you need more control over what is being parser check the `settings` section as well as the `using-datedataparser` section.


Dependencies
============

`dateparser` relies on following libraries in some ways:

  * dateutil_'s module ``relativedelta`` for its freshness parser.
  * convertdate_ to convert *Jalali* dates to *Gregorian*.
  * hijri-converter_ to convert *Hijri* dates to *Gregorian*.
  * tzlocal_ to reliably get local timezone.
  * ruamel.yaml_ (optional) for operations on language files.

.. _dateutil: https://pypi.python.org/pypi/python-dateutil
.. _convertdate: https://pypi.python.org/pypi/convertdate
.. _hijri-converter: https://pypi.python.org/pypi/hijri-converter
.. _tzlocal: https://pypi.python.org/pypi/tzlocal
.. _ruamel.yaml: https://pypi.python.org/pypi/ruamel.yaml

Supported languages and locales
===============================
You can check the supported locales by visiting the "`supported-locales`" section.


Supported Calendars
===================

Apart from the Georgian calendar, `dateparser` supports the `Persian Jalali calendar` and the `Hijri/Islami calendar`

To be able to use them you need to install the `calendar` extra by typing:

    pip install dateparser[calendars]


* Example using the `Persian Jalali calendar`. For more information, refer to `Persian Jalali Calendar <https://en.wikipedia.org/wiki/Iranian_calendars#Zoroastrian_calendar>`_.

    >>> from dateparser.calendars.jalali import JalaliCalendar
    >>> JalaliCalendar('Ø¬Ù…Ø¹Ù‡ Ø³ÛŒ Ø§Ù… Ø§Ø³ÙÙ†Ø¯ Û±Û³Û¸Û·').get_date()
    DateData(date_obj=datetime.datetime(2009, 3, 20, 0, 0), period='day', locale=None)


* Example using the `Hijri/Islamic Calendar`. For more information, refer to `Hijri Calendar <https://en.wikipedia.org/wiki/Islamic_calendar>`_.

    >>> from dateparser.calendars.hijri import HijriCalendar
    >>> HijriCalendar('17-01-1437 Ù‡Ù€ 08:30 Ù…Ø³Ø§Ø¡Ù‹').get_date()
    DateData(date_obj=datetime.datetime(2015, 10, 30, 20, 30), period='day', locale=None)

.. note:: `HijriCalendar` only works with Python â‰¥ 3.6.


.. :changelog:

History
=======

1.2.0 (2023-11-17)
------------------

New features:

- New ``PREFER_MONTH_OF_YEAR`` setting (#1146)

Fixes:

- Absolute years in Russian are no longer being treated as a number of years in
  the past (#1129)

Cleanups and internal improvements:

- Removed the use of ``datetime.utcnow``, deprecated on Python 3.12 (#1179)
- Applied Black formatting to the code base (#1158)
- Initial integration with OSSFuzz (#1198)
- Extended test cases (#1191)


1.1.8 (2023-03-22)
------------------

Improvements:

- Improved date parsing for Chinese (#1148)
- Improved date parsing for Czech (#1151)
- Reorder language by popularity (#1152)
- Fix leak of memory in cache (#1140)
- Add support for "\\d units later" (#1154)
- Move modification in CLDR data to yaml (#1153)
- Add support to use timezone via settings to get PREFER_DATES_FROM result (#1155)


1.1.7 (2023-02-02)
------------------

Improvements:

- Add an â€œagoâ€ synonym for Arabic (#1128)
- Improved date parsing for Czech (#1131)
- Improved date parsing for Indonesian (#1134)


1.1.6 (2023-01-12)
------------------

Improvements:

- Fix the bug where Monday is parsed as a month (#1121)
- Prevent ReDoS in Spanish sentence splitting regex (#1084)


1.1.5 (2022-12-29)
------------------

Improvements:

- Parse short versions of day, month, and year (#1103)
- Add a test for â€œin 1dâ€ (#1104)
- Update languages_info (#1107)
- Add a workaround for zipimporter not having exec_module before Python 3.10 (#1069)
- Stabilize tests at midnight (#1111)
- Add a test case for French (#1110)

Cleanups:

- Remove the requirements-build file (#1113)


1.1.4 (2022-11-21)
------------------

Improvements:

- Improved support for languages such as Slovak, Indonesian, Hindi, German and Japanese (#1064, #1094, #986, #1071, #1068)
- Recursively create a model home (#996)
- Replace regex sub with simple string replace (#1095)
- Add Python 3.10, 3.11 support (#1096)
- Drop support for Python 3.5, 3.6 versions (#1097)


1.1.3 (2022-11-03)
------------------

New features:

- Add support for fractional units (#876)

Improvements:

- Fix the returned datetime skipping a day with time+timezone input and PREFER_DATES_FROM = 'future' (#1002)
- Fix input translatation breaking keep_formatting (#720)
- English: support "till date" (#1005)
- English: support â€œafterâ€ and â€œbeforeâ€ in relative dates (#1008)

Cleanups:

- Reorganize internal data (#1090)
- CI updates (#1088)


1.1.2 (2022-10-20)
------------------

Improvements:

- Added support for negative timestamp (#1060)
- Fixed PytzUsageWarning for Python versions >= 3.6 (#1062)
- Added support for dates with dots and spaces (#1028)
- Improved support for Ukrainian, Croatian and Russian (#1072, #1074, #1079, #1082, #1073, #1083)
- Added support for parsing Unix timestamps consistently regardless of timezones (#954)
- Improved tests (#1086)


1.1.1 (2022-03-17)
------------------

Improvements:

- Fixed issue with regex library by pinning dependencies to an earlier version (< 2022.3.15, #1046).
- Extended support for Russian language dates starting with lowercase (#999).
- Allowed to use_given_order for languages too (#997).
- Fixed link to settings section (#1018).
- Defined UTF-8 encoding for Windows (#998).
- Fixed directories creation error in CLI utils (#1022).


1.1.0 (2021-10-04)
------------------

New features:

* Support language detection based on ``langdetect``, ``fastText``, or a
  custom implementation (see #932)
* Add support for 'by <time>' (see #839)
* Sort default language list by internet usage (see #805)

Improvements:

* Improved support of Chinese (#910), Czech (#977)
* Improvements in ``search_dates`` (see #953)
* Make order of previous locales deterministic (see #851)
* Fix parsing with trailing space (see #841)
* Consider ``RETURN_TIME_AS_PERIOD`` for timestamp times (see #922)
* Exclude failing regex version (see #974)
* Ongoing work multithreading support (see #881, #885)
* Add demo URL (see #883)

QA:

* Migrate pipelines from Travis CI to Github Actions (see #859, #879, #884,
  #886, #911, #966)
* Use versioned CLDR data (see #825)
* Add a script to update table of supported languages and locales (see #601)
* Sort 'skip' keys in yaml files (see #844)
* Improve test coverage (see #827)
* Code cleanup (see #888, #907, #951, #958, #957)


1.0.0 (2020-10-29)
------------------

Breaking changes:

* Drop support for Python 2.7 and pypy (see #727, #744, #748, #749, #754, #755, #758, #761, #763, #764, #777 and #783)
* Now ``DateDataParser.get_date_data()`` returns a ``DateData`` object instead of a ``dict`` (see #778).
* From now wrong ``settings`` are not silenced and raise ``SettingValidationError`` (see #797)
* Now ``dateparser.parse()`` is deterministic and doesn't try previous locales. Also, ``DateDataParser.get_date_data()`` doesn't try the previous locales by default (see #781)
* Remove the ``'base-formats'`` parser (see #721)
* Extract the ``'no-spaces-time'`` parser from the ``'absolute-time'`` parser and make it an optional parser (see #786)
* Remove ``numeral_translation_data`` (see #782)
* Remove the undocumented ``SKIP_TOKENS_PARSER`` and ``FUZZY`` settings (see #728, #794)
* Remove support for using strings in ``date_formats`` (see #726)
* The undocumented ``ExactLanguageSearch`` class has been moved to the private scope and some internal methods have changed (see #778)
* Changes in ``dateparser.utils``: ``normalize_unicode()`` doesn't accept ``bytes`` as input and ``convert_to_unicode`` has been deprecated (see #749)

New features:

* Add Python 3.9 support (see #732, #823)
* Detect hours separated with a period/dot (see #741)
* Add support for "decade" (see #762)
* Add support for the hijri calendar in Python â‰¥ 3.6 (see #718)

Improvements:

* New logo! (see #719)
* Improve the README and docs (see #779, #722)
* Fix the "calendars" extra (see #740)
* Fix leap years when ``PREFER_DATES_FROM`` is set (see #738)
* Fix ``STRICT_PARSING`` setting in ``no-spaces-time`` parser (see #715)
* Consider ``RETURN_AS_TIME_PERIOD`` setting for ``relative-time`` parser (see #807)
* Parse the 24hr time format with meridian info (see #634)
* Other small improvements (see #698, #709, #710, #712, #730, #731, #735, #739, #784, #788, #795 and #801)


0.7.6 (2020-06-12)
------------------

Improvements:

* Rename ``scripts`` to ``dateparser_scripts`` to avoid name collisions with modules from other packages or projects (see #707)


0.7.5 (2020-06-10)
------------------

New features:

* Add Python 3.8 support (see #664)
* Implement a ``REQUIRE_PARTS`` setting (see #703)
* Add support for subscript and superscript numbers (see #684)
* Extended French support (see #672)
* Extended German support (see #673)


Improvements:

* Migrate test suite to Pytest (see #662)
* Add test to check the `yaml` and `json` files content (see #663 and #692)
* Add flake8 pipeline with pytest-flake8 (see #665)
* Add partial support for 8-digit dates without separators (see #639)
* Fix possible ``OverflowError`` errors and explicitly avoid to raise ``ValueError`` when parsing relative dates (see #686)
* Fix double-digit GMT and UTC parsing (see #632)
* Fix bug when using ``DATE_ORDER`` (see #628)
* Fix bug when parsing relative time with timezone (see #503)
* Fix milliseconds parsing (see #572 and #661)
* Fix wrong values to be interpreted as ``'future'`` in ``PREFER_DATES_FROM`` (see #629)
* Other small improvements (see #667, #675, #511, #626, #512, #509, #696, #702 and #699)


0.7.4 (2020-03-06)
------------------
New features:

* Extended Norwegian support (see #598)
* Implement a ``PARSERS`` setting (see #603)

Improvements:

* Add support for ``PREFER_DATES_FROM`` in relative/freshness parser (see #414)
* Add support for ``PREFER_DAY_OF_MONTH`` in base-formats parser (see #611)
* Added UTC -00:00 as a valid offset (see #574)
* Fix support for â€œoneâ€ (see #593)
* Fix TypeError when parsing some invalid dates (see #536)
* Fix tokenizer for non recognized characters (see #622)
* Prevent installing regex 2019.02.19 (see #600)
* Resolve DeprecationWarning related to raw string escape sequences (see #596)
* Implement a tox environment to build the documentation (see #604)
* Improve tests stability (see #591, #605)
* Documentation improvements (see #510, #578, #619, #614, #620)
* Performance improvements (see #570, #569, #625)


0.7.3 (2020-03-06)
------------------
* Broken version


0.7.2 (2019-09-17)
------------------

Features:

* Extended Czech support
* Added ``time`` to valid periods
* Added timezone information to dates found with ``search_dates()``
* Support strings as date formats


Improvements:

* Fixed Collections ABCs depreciation warning
* Fixed dates with trailing colons not being parsed
* Fixed date format override on any settings change
* Fixed parsing current weekday as past date, regardless of settings
* Added UTC -2:30 as a valid offset
* Added Python 3.7 to supported versions, dropped support for Python 3.3 and 3.4
* Moved to importlib from imp where possible
* Improved support for Catalan
* Documentation improvements


0.7.1 (2019-02-12)
------------------

Features/news:

* Added detected language to return value of ``search_dates()``
* Performance improvements
* Refreshed versions of dependencies

Improvements:

* Fixed unpickleable ``DateTime`` objects with timezones
* Fixed regex pattern to avoid new behaviour of re.split in Python 3.7
* Fixed an exception thrown when parsing colons
* Fixed tests failing on days with number greater than 30
* Fixed ``ZeroDivisionError`` exceptions



0.7.0 (2018-02-08)
------------------

Features added during Google Summer of Code 2017:

* Harvesting language data from Unicode CLDR database (https://github.com/unicode-cldr/cldr-json), which includes over 200 locales (#321) - authored by Sarthak Maddan.
  See full currently supported locale list in README.
* Extracting dates from longer strings of text (#324) - authored by Elena Zakharova.
  Special thanks for their awesome contributions!


New features:

* Added (independently from CLDR) Georgian (#308) and Swedish (#305)

Improvements:

* Improved support of Chinese (#359), Thai (#345), French (#301, #304), Russian (#302)
* Removed ruamel.yaml from dependencies (#374). This should reduce the number of installation issues and improve performance as the result of moving away from YAML as basic data storage format.
  Note that YAML is still used as format for support language files.
* Improved performance through using pre-compiling frequent regexes and lazy loading of data (#293, #294, #295, #315)
* Extended tests (#316, #317, #318, #323)
* Updated nose_parameterized to its current package, parameterized (#381)


Planned for next release:

* Full language and locale names
* Performance and stability improvements
* Documentation improvements


0.6.0 (2017-03-13)
------------------

New features:

* Consistent parsing in terms of true python representation of date string. See #281
* Added support for Bangla, Bulgarian and Hindi languages.

Improvements:

* Major bug fixes related to parser and system's locale. See #277, #282
* Type check for timezone arguments in settings. see #267
* Pinned dependencies' versions in requirements. See #265
* Improved support for cn, es, dutch languages. See #274, #272, #285

Packaging:

* Make calendars extras to be used at the time of installation if need to use calendars feature.


0.5.1 (2016-12-18)
------------------

New features:

* Added support for Hebrew

Improvements:

* Safer loading of YAML. See #251
* Better timezone parsing for freshness dates. See #256
* Pinned dependencies' versions in requirements. See #265
* Improved support for zh, fi languages. See #249, #250, #248, #244


0.5.0 (2016-09-26)
------------------

New features:

* ``DateDataParser`` now also returns detected language in the result dictionary.
* Explicit and lucid timezone conversion for a given datestring using ``TIMEZONE``, ``TO_TIMEZONE`` settings.
* Added Hungarian language.
* Added setting, ``STRICT_PARSING`` to ignore incomplete dates.

Improvements:

* Fixed quite a few parser bugs reported in issues #219, #222, #207, #224.
* Improved support for chinese language.
* Consistent interface for both Jalali and Hijri parsers.


0.4.0 (2016-06-17)
------------------

New features:

* Support for Language based date order preference while parsing ambiguous dates.
* Support for parsing dates with no spaces in between components.
* Support for custom date order preference using ``settings``.
* Support for parsing generic relative dates in future.e.g. "tomorrow", "in two weeks", etc.
* Added ``RELATIVE_BASE`` settings to set date context to any datetime in past or future.
* Replaced ``dateutil.parser.parse`` with dateparser's own parser.

Improvements:

* Added simplifications for "12 noon" and "12 midnight".
* Fixed several bugs
* Replaced PyYAML library by its active fork `ruamel.yaml` which also fixed the issues with installation on windows using python35.
* More predictable ``date_formats`` handling.


0.3.5 (2016-04-27)
------------------

New features:

* Danish language support.
* Japanese language support.
* Support for parsing date strings with accents.

Improvements:

* Transformed languages.yaml into base file and separate files for each language.
* Fixed vietnamese language simplifications.
* No more version restrictions for python-dateutil.
* Timezone parsing improvements.
* Fixed test environments.
* Cleaned language codes. Now we strictly follow codes as in ISO 639-1.
* Improved chinese dates parsing.


0.3.4 (2016-03-03)
------------------

Improvements:

* Fixed broken version 0.3.3 by excluding latest python-dateutil version.

0.3.3 (2016-02-29)
------------------

New features:

* Finnish language support.

Improvements:

* Faster parsing with switching to regex module.
* ``RETURN_AS_TIMEZONE_AWARE`` setting to return tz aware date object.
* Fixed conflicts with month/weekday names similarity across languages.

0.3.2 (2016-01-25)
------------------

New features:

* Added Hijri Calendar support.
* Added settings for better control over parsing dates.
* Support to convert parsed time to the given timezone for both complete and relative dates.

Improvements:

* Fixed problem with caching `datetime.now` in `FreshnessDateDataParser`.
* Added month names and week day names abbreviations to several languages.
* More simplifications for Russian and Ukrainian languages.
* Fixed problem with parsing time component of date strings with several kinds of apostrophes.


0.3.1 (2015-10-28)
------------------

New features:

* Support for Jalali Calendar.
* Belarusian language support.
* Indonesian language support.


Improvements:

* Extended support for Russian and Polish.
* Fixed bug with time zone recognition.
* Fixed bug with incorrect translation of "second" for Portuguese.


0.3.0 (2015-07-29)
------------------

New features:

* Compatibility with Python 3 and PyPy.

Improvements:

* `languages.yaml` data cleaned up to make it human-readable.
* Improved Spanish date parsing.


0.2.1 (2015-07-13)
------------------

* Support for generic parsing of dates with UTC offset.
* Support for Tagalog/Filipino dates.
* Improved support for French and Spanish dates.


0.2.0 (2015-06-17)
------------------

* Easy to use ``parse`` function
* Languages definitions using YAML.
* Using translation based approach for parsing non-english languages. Previously, `dateutil.parserinfo` was used for language definitions.
* Better period extraction.
* Improved tests.
* Added a number of new simplifications for more comprehensive generic parsing.
* Improved validation for dates.
* Support for Polish, Thai and Arabic dates.
* Support for `pytz` timezones.
* Fixed building and packaging issues.


0.1.0 (2014-11-24)
------------------

* First release on PyPI.
"""^^xsd:string ;
    schema1:name "dateparser"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.2.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/diff-cover> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/8.0.1> .

<https://data.siemens-energy.com/cybersec/package/dill> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/gprof2dot>,
        <https://data.siemens-energy.com/cybersec/package/objgraph> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.2>,
        <https://data.siemens-energy.com/cybersec/version/0.3.6>,
        <https://data.siemens-energy.com/cybersec/version/0.3.7> ;
    schema1:description """-----------------------------
dill: serialize all of Python
-----------------------------

About Dill
==========

``dill`` extends Python's ``pickle`` module for serializing and de-serializing
Python objects to the majority of the built-in Python types. Serialization
is the process of converting an object to a byte stream, and the inverse
of which is converting a byte stream back to a Python object hierarchy.

``dill`` provides the user the same interface as the ``pickle`` module, and
also includes some additional features. In addition to pickling Python
objects, ``dill`` provides the ability to save the state of an interpreter
session in a single command.  Hence, it would be feasible to save an
interpreter session, close the interpreter, ship the pickled file to
another computer, open a new interpreter, unpickle the session and
thus continue from the 'saved' state of the original interpreter
session.

``dill`` can be used to store Python objects to a file, but the primary
usage is to send Python objects across the network as a byte stream.
``dill`` is quite flexible, and allows arbitrary user defined classes
and functions to be serialized.  Thus ``dill`` is not intended to be
secure against erroneously or maliciously constructed data. It is
left to the user to decide whether the data they unpickle is from
a trustworthy source.

``dill`` is part of ``pathos``, a Python framework for heterogeneous computing.
``dill`` is in active development, so any user feedback, bug reports, comments,
or suggestions are highly appreciated.  A list of issues is located at
https://github.com/uqfoundation/dill/issues, with a legacy list maintained at
https://uqfoundation.github.io/project/pathos/query.


Major Features
==============

``dill`` can pickle the following standard types:

    - none, type, bool, int, float, complex, bytes, str,
    - tuple, list, dict, file, buffer, builtin,
    - Python classes, namedtuples, dataclasses, metaclasses,
    - instances of classes,
    - set, frozenset, array, functions, exceptions

``dill`` can also pickle more 'exotic' standard types:

    - functions with yields, nested functions, lambdas,
    - cell, method, unboundmethod, module, code, methodwrapper,
    - methoddescriptor, getsetdescriptor, memberdescriptor, wrapperdescriptor,
    - dictproxy, slice, notimplemented, ellipsis, quit

``dill`` cannot yet pickle these standard types:

    - frame, generator, traceback

``dill`` also provides the capability to:

    - save and load Python interpreter sessions
    - save and extract the source code from functions and classes
    - interactively diagnose pickling errors


Current Release
===============

The latest released version of ``dill`` is available from:

    https://pypi.org/project/dill

``dill`` is distributed under a 3-clause BSD license.


Development Version
===================

You can get the latest development version with all the shiny new features at:

    https://github.com/uqfoundation

If you have a new contribution, please submit a pull request.


Installation
============

``dill`` can be installed with ``pip``::

    $ pip install dill

To optionally include the ``objgraph`` diagnostic tool in the install::

    $ pip install dill[graph]

To optionally include the ``gprof2dot`` diagnostic tool in the install::

    $ pip install dill[profile]

For windows users, to optionally install session history tools::

    $ pip install dill[readline]


Requirements
============

``dill`` requires:

    - ``python`` (or ``pypy``), **>=3.8**
    - ``setuptools``, **>=42**

Optional requirements:

    - ``objgraph``, **>=1.7.2**
    - ``gprof2dot``, **>=2022.7.29**
    - ``pyreadline``, **>=1.7.1** (on windows)


Basic Usage
===========

``dill`` is a drop-in replacement for ``pickle``. Existing code can be
updated to allow complete pickling using::

    >>> import dill as pickle

or::

    >>> from dill import dumps, loads

``dumps`` converts the object to a unique byte string, and ``loads`` performs
the inverse operation::

    >>> squared = lambda x: x**2
    >>> loads(dumps(squared))(3)
    9

There are a number of options to control serialization which are provided
as keyword arguments to several ``dill`` functions:

* with *protocol*, the pickle protocol level can be set. This uses the
  same value as the ``pickle`` module, *DEFAULT_PROTOCOL*.
* with *byref=True*, ``dill`` to behave a lot more like pickle with
  certain objects (like modules) pickled by reference as opposed to
  attempting to pickle the object itself.
* with *recurse=True*, objects referred to in the global dictionary are
  recursively traced and pickled, instead of the default behavior of
  attempting to store the entire global dictionary.
* with *fmode*, the contents of the file can be pickled along with the file
  handle, which is useful if the object is being sent over the wire to a
  remote system which does not have the original file on disk. Options are
  *HANDLE_FMODE* for just the handle, *CONTENTS_FMODE* for the file content
  and *FILE_FMODE* for content and handle.
* with *ignore=False*, objects reconstructed with types defined in the
  top-level script environment use the existing type in the environment
  rather than a possibly different reconstructed type.

The default serialization can also be set globally in *dill.settings*.
Thus, we can modify how ``dill`` handles references to the global dictionary
locally or globally::

    >>> import dill.settings
    >>> dumps(absolute) == dumps(absolute, recurse=True)
    False
    >>> dill.settings['recurse'] = True
    >>> dumps(absolute) == dumps(absolute, recurse=True)
    True

``dill`` also includes source code inspection, as an alternate to pickling::

    >>> import dill.source
    >>> print(dill.source.getsource(squared))
    squared = lambda x:x**2

To aid in debugging pickling issues, use *dill.detect* which provides
tools like pickle tracing::

    >>> import dill.detect
    >>> with dill.detect.trace():
    >>>     dumps(squared)
    â”¬ F1: <function <lambda> at 0x7fe074f8c280>
    â”œâ”¬ F2: <function _create_function at 0x7fe074c49c10>
    â”‚â”” # F2 [34 B]
    â”œâ”¬ Co: <code object <lambda> at 0x7fe07501eb30, file "<stdin>", line 1>
    â”‚â”œâ”¬ F2: <function _create_code at 0x7fe074c49ca0>
    â”‚â”‚â”” # F2 [19 B]
    â”‚â”” # Co [87 B]
    â”œâ”¬ D1: <dict object at 0x7fe0750d4680>
    â”‚â”” # D1 [22 B]
    â”œâ”¬ D2: <dict object at 0x7fe074c5a1c0>
    â”‚â”” # D2 [2 B]
    â”œâ”¬ D2: <dict object at 0x7fe074f903c0>
    â”‚â”œâ”¬ D2: <dict object at 0x7fe074f8ebc0>
    â”‚â”‚â”” # D2 [2 B]
    â”‚â”” # D2 [23 B]
    â”” # F1 [180 B]

With trace, we see how ``dill`` stored the lambda (``F1``) by first storing
``_create_function``, the underlying code object (``Co``) and ``_create_code``
(which is used to handle code objects), then we handle the reference to
the global dict (``D2``) plus other dictionaries (``D1`` and ``D2``) that
save the lambda object's state. A ``#`` marks when the object is actually stored.


More Information
================

Probably the best way to get started is to look at the documentation at
http://dill.rtfd.io. Also see ``dill.tests`` for a set of scripts that
demonstrate how ``dill`` can serialize different Python objects. You can
run the test suite with ``python -m dill.tests``. The contents of any
pickle file can be examined with ``undill``.  As ``dill`` conforms to
the ``pickle`` interface, the examples and documentation found at
http://docs.python.org/library/pickle.html also apply to ``dill``
if one will ``import dill as pickle``. The source code is also generally
well documented, so further questions may be resolved by inspecting the
code itself. Please feel free to submit a ticket on github, or ask a
question on stackoverflow (**@Mike McKerns**).
If you would like to share how you use ``dill`` in your work, please send
an email (to **mmckerns at uqfoundation dot org**).


Citation
========

If you use ``dill`` to do research that leads to publication, we ask that you
acknowledge use of ``dill`` by citing the following in your publication::

    M.M. McKerns, L. Strand, T. Sullivan, A. Fang, M.A.G. Aivazis,
    "Building a framework for predictive science", Proceedings of
    the 10th Python in Science Conference, 2011;
    http://arxiv.org/pdf/1202.1056

    Michael McKerns and Michael Aivazis,
    "pathos: a framework for heterogeneous computing", 2010- ;
    https://uqfoundation.github.io/project/pathos

Please see https://uqfoundation.github.io/project/pathos or
http://arxiv.org/pdf/1202.1056 for further information.
"""^^xsd:string ;
    schema1:name "dill"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.3.9"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/distlib> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1> ;
    schema1:description """|badge1| |badge2|

.. |badge1| image:: https://img.shields.io/github/actions/workflow/status/pypa/distlib/package-tests.yml
   :alt: GitHub Workflow Status (with event)

.. |badge2| image:: https://img.shields.io/codecov/c/github/pypa/distlib
   :target: https://app.codecov.io/gh/pypa/distlib
   :alt: GitHub coverage status

What is it?
-----------

Distlib is a library which implements low-level functions that relate to
packaging and distribution of Python software. It is intended to be used as the
basis for third-party packaging tools. The documentation is available at

https://distlib.readthedocs.io/

Main features
-------------

Distlib currently offers the following features:

* The package ``distlib.database``, which implements a database of installed
  distributions, as defined by :pep:`376`, and distribution dependency graph
  logic. Support is also provided for non-installed distributions (i.e.
  distributions registered with metadata on an index like PyPI), including
  the ability to scan for dependencies and building dependency graphs.
* The package ``distlib.index``, which implements an interface to perform
  operations on an index, such as registering a project, uploading a
  distribution or uploading documentation. Support is included for verifying
  SSL connections (with domain matching) and signing/verifying packages using
  GnuPG.
* The package ``distlib.metadata``, which implements distribution metadata as
  defined by :pep:`643`, :pep:`566`, :pep:`345`, :pep:`314` and :pep:`241`.
* The package ``distlib.markers``, which implements environment markers as
  defined by :pep:`508`.
* The package ``distlib.manifest``, which implements lists of files used
  in packaging source distributions.
* The package ``distlib.locators``, which allows finding distributions, whether
  on PyPI (XML-RPC or via the "simple" interface), local directories or some
  other source.
* The package ``distlib.resources``, which allows access to data files stored
  in Python packages, both in the file system and in .zip files.
* The package ``distlib.scripts``, which allows installing of scripts with
  adjustment of shebang lines and support for native Windows executable
  launchers.
* The package ``distlib.version``, which implements version specifiers as
  defined by :pep:`440`, but also support for working with "legacy" versions and
  semantic versions.
* The package ``distlib.wheel``, which provides support for building and
  installing from the Wheel format for binary distributions (see :pep:`427`).
* The package ``distlib.util``, which contains miscellaneous functions and
  classes which are useful in packaging, but which do not fit neatly into
  one of the other packages in ``distlib``.* The package implements enhanced
  globbing functionality such as the ability to use ``**`` in patterns to
  specify recursing into subdirectories.


Python version and platform compatibility
-----------------------------------------

Distlib is intended to be used on and is tested on Python versions 2.7 and 3.6 or later,
pypy-2.7 and pypy3 on Linux, Windows, and macOS.

Project status
--------------

The project has reached a mature status in its development: there is a comprehensive
test suite and it has been exercised on Windows, Ubuntu and macOS. The project is used
by well-known projects such as `pip <https://pypi.org/pypi/pip>`_ and `caniusepython3
<https://pypi.org/pypi/caniusepython3>`_.

This project was migrated from Mercurial to Git and from BitBucket to GitHub, and
although all information of importance has been retained across the migration, some
commit references in issues and issue comments may have become invalid.

Code of Conduct
---------------

Everyone interacting in the distlib project's codebases, issue trackers, chat
rooms, and mailing lists is expected to follow the `PyPA Code of Conduct`_.

.. _PyPA Code of Conduct: https://www.pypa.io/en/latest/code-of-conduct/


"""^^xsd:string ;
    schema1:name "distlib"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.3.8"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/doc8> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.11.2> .

<https://data.siemens-energy.com/cybersec/package/email-validator> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.0.0> .

<https://data.siemens-energy.com/cybersec/package/executing> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/asttokens> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.2.0> ;
    schema1:description """# executing

[![Build Status](https://github.com/alexmojaki/executing/workflows/Tests/badge.svg?branch=master)](https://github.com/alexmojaki/executing/actions) [![Coverage Status](https://coveralls.io/repos/github/alexmojaki/executing/badge.svg?branch=master)](https://coveralls.io/github/alexmojaki/executing?branch=master) [![Supports Python versions 3.5+, including PyPy](https://img.shields.io/pypi/pyversions/executing.svg)](https://pypi.python.org/pypi/executing)

This mini-package lets you get information about what a frame is currently doing, particularly the AST node being executed.

* [Usage](#usage)
    * [Getting the AST node](#getting-the-ast-node)
    * [Getting the source code of the node](#getting-the-source-code-of-the-node)
    * [Getting the `__qualname__` of the current function](#getting-the-__qualname__-of-the-current-function)
    * [The Source class](#the-source-class)
* [Installation](#installation)
* [How does it work?](#how-does-it-work)
* [Is it reliable?](#is-it-reliable)
* [Which nodes can it identify?](#which-nodes-can-it-identify)
* [Libraries that use this](#libraries-that-use-this)

## Usage

### Getting the AST node

```python
import executing

node = executing.Source.executing(frame).node
```

Then `node` will be an AST node (from the `ast` standard library module) or None if the node couldn't be identified (which may happen often and should always be checked).

`node` will always be the same instance for multiple calls with frames at the same point of execution.

If you have a traceback object, pass it directly to `Source.executing()` rather than the `tb_frame` attribute to get the correct node.

### Getting the source code of the node

For this you will need to separately install the [`asttokens`](https://github.com/gristlabs/asttokens) library, then obtain an `ASTTokens` object:

```python
executing.Source.executing(frame).source.asttokens()
```

or:

```python
executing.Source.for_frame(frame).asttokens()
```

or use one of the convenience methods:

```python
executing.Source.executing(frame).text()
executing.Source.executing(frame).text_range()
```

### Getting the `__qualname__` of the current function

```python
executing.Source.executing(frame).code_qualname()
```

or:

```python
executing.Source.for_frame(frame).code_qualname(frame.f_code)
```

### The `Source` class

Everything goes through the `Source` class. Only one instance of the class is created for each filename. Subclassing it to add more attributes on creation or methods is recommended. The classmethods such as `executing` will respect this. See the source code and docstrings for more detail.

## Installation

    pip install executing

If you don't like that you can just copy the file `executing.py`, there are no dependencies (but of course you won't get updates).

## How does it work?

Suppose the frame is executing this line:

```python
self.foo(bar.x)
```

and in particular it's currently obtaining the attribute `self.foo`. Looking at the bytecode, specifically `frame.f_code.co_code[frame.f_lasti]`, we can tell that it's loading an attribute, but it's not obvious which one. We can narrow down the statement being executed using `frame.f_lineno` and find the two `ast.Attribute` nodes representing `self.foo` and `bar.x`. How do we find out which one it is, without recreating the entire compiler in Python?

The trick is to modify the AST slightly for each candidate expression and observe the changes in the bytecode instructions. We change the AST to this:

```python
(self.foo ** 'longuniqueconstant')(bar.x)
```
    
and compile it, and the bytecode will be almost the same but there will be two new instructions:

    LOAD_CONST 'longuniqueconstant'
    BINARY_POWER

and just before that will be a `LOAD_ATTR` instruction corresponding to `self.foo`. Seeing that it's in the same position as the original instruction lets us know we've found our match.

## Is it reliable?

Yes - if it identifies a node, you can trust that it's identified the correct one. The tests are very thorough - in addition to unit tests which check various situations directly, there are property tests against a large number of files (see the filenames printed in [this build](https://travis-ci.org/alexmojaki/executing/jobs/557970457)) with real code. Specifically, for each file, the tests:
 
 1. Identify as many nodes as possible from all the bytecode instructions in the file, and assert that they are all distinct
 2. Find all the nodes that should be identifiable, and assert that they were indeed identified somewhere

In other words, it shows that there is a one-to-one mapping between the nodes and the instructions that can be handled. This leaves very little room for a bug to creep in.

Furthermore, `executing` checks that the instructions compiled from the modified AST exactly match the original code save for a few small known exceptions. This accounts for all the quirks and optimisations in the interpreter. 

## Which nodes can it identify?

Currently it works in almost all cases for the following `ast` nodes:
 
 - `Call`, e.g. `self.foo(bar)`
 - `Attribute`, e.g. `point.x`
 - `Subscript`, e.g. `lst[1]`
 - `BinOp`, e.g. `x + y` (doesn't include `and` and `or`)
 - `UnaryOp`, e.g. `-n` (includes `not` but only works sometimes)
 - `Compare` e.g. `a < b` (not for chains such as `0 < p < 1`)

The plan is to extend to more operations in the future.

## Projects that use this

### My Projects

- **[`stack_data`](https://github.com/alexmojaki/stack_data)**: Extracts data from stack frames and tracebacks, particularly to display more useful tracebacks than the default. Also uses another related library of mine: **[`pure_eval`](https://github.com/alexmojaki/pure_eval)**.
- **[`futurecoder`](https://futurecoder.io/)**: Highlights the executing node in tracebacks using `executing` via `stack_data`, and provides debugging with `snoop`.
- **[`snoop`](https://github.com/alexmojaki/snoop)**: A feature-rich and convenient debugging library. Uses `executing` to show the operation which caused an exception and to allow the `pp` function to display the source of its arguments.
- **[`heartrate`](https://github.com/alexmojaki/heartrate)**: A simple real time visualisation of the execution of a Python program. Uses `executing` to highlight currently executing operations, particularly in each frame of the stack trace.
- **[`sorcery`](https://github.com/alexmojaki/sorcery)**: Dark magic delights in Python. Uses `executing` to let special callables called spells know where they're being called from.

### Projects I've contributed to

- **[`IPython`](https://github.com/ipython/ipython/pull/12150)**: Highlights the executing node in tracebacks using `executing` via [`stack_data`](https://github.com/alexmojaki/stack_data).
- **[`icecream`](https://github.com/gruns/icecream)**: ðŸ¦ Sweet and creamy print debugging. Uses `executing` to identify where `ic` is called and print its arguments.
- **[`friendly_traceback`](https://github.com/friendly-traceback/friendly-traceback)**: Uses `stack_data` and `executing` to pinpoint the cause of errors and provide helpful explanations.
- **[`python-devtools`](https://github.com/samuelcolvin/python-devtools)**: Uses `executing` for print debugging similar to `icecream`.
- **[`sentry_sdk`](https://github.com/getsentry/sentry-python)**: Add the integration `sentry_sdk.integrations.executingExecutingIntegration()` to show the function `__qualname__` in each frame in sentry events.
- **[`varname`](https://github.com/pwwang/python-varname)**: Dark magics about variable names in python. Uses `executing` to find where its various magical functions like `varname` and `nameof` are called from.
"""^^xsd:string ;
    schema1:name "executing"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.0.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/fastjsonschema> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.15> ;
    schema1:description """===========================
Fast JSON schema for Python
===========================

|PyPI| |Pythons|

.. |PyPI| image:: https://img.shields.io/pypi/v/fastjsonschema.svg
   :alt: PyPI version
   :target: https://pypi.python.org/pypi/fastjsonschema

.. |Pythons| image:: https://img.shields.io/pypi/pyversions/fastjsonschema.svg
   :alt: Supported Python versions
   :target: https://pypi.python.org/pypi/fastjsonschema

See `documentation <https://horejsek.github.io/python-fastjsonschema/>`_.
"""^^xsd:string ;
    schema1:name "fastjsonschema"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.20.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/fastparquet> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2022.12.0> .

<https://data.siemens-energy.com/cybersec/package/flaky> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.7> .

<https://data.siemens-energy.com/cybersec/package/flit-core> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/4> .

<https://data.siemens-energy.com/cybersec/package/fonttools> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/brotli>,
        <https://data.siemens-energy.com/cybersec/package/brotlicffi>,
        <https://data.siemens-energy.com/cybersec/package/fs>,
        <https://data.siemens-energy.com/cybersec/package/lxml>,
        <https://data.siemens-energy.com/cybersec/package/lz4>,
        <https://data.siemens-energy.com/cybersec/package/skia-pathops>,
        <https://data.siemens-energy.com/cybersec/package/uharfbuzz>,
        <https://data.siemens-energy.com/cybersec/package/unicodedata2>,
        <https://data.siemens-energy.com/cybersec/package/zopfli> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/4.22.0> ;
    schema1:description """|CI Build Status| |Coverage Status| |PyPI| |Gitter Chat|

What is this?
~~~~~~~~~~~~~

| fontTools is a library for manipulating fonts, written in Python. The
  project includes the TTX tool, that can convert TrueType and OpenType
  fonts to and from an XML text format, which is also called TTX. It
  supports TrueType, OpenType, AFM and to an extent Type 1 and some
  Mac-specific formats. The project has an `MIT open-source
  licence <LICENSE>`__.
| Among other things this means you can use it free of charge.

`User documentation <https://fonttools.readthedocs.io/en/latest/>`_ and
`developer documentation <https://fonttools.readthedocs.io/en/latest/developer.html>`_
are available at `Read the Docs <https://fonttools.readthedocs.io/>`_.

Installation
~~~~~~~~~~~~

FontTools requires `Python <http://www.python.org/download/>`__ 3.8
or later. We try to follow the same schedule of minimum Python version support as
NumPy (see `NEP 29 <https://numpy.org/neps/nep-0029-deprecation_policy.html>`__).

The package is listed in the Python Package Index (PyPI), so you can
install it with `pip <https://pip.pypa.io>`__:

.. code:: sh

    pip install fonttools

If you would like to contribute to its development, you can clone the
repository from GitHub, install the package in 'editable' mode and
modify the source code in place. We recommend creating a virtual
environment, using `virtualenv <https://virtualenv.pypa.io>`__ or
Python 3 `venv <https://docs.python.org/3/library/venv.html>`__ module.

.. code:: sh

    # download the source code to 'fonttools' folder
    git clone https://github.com/fonttools/fonttools.git
    cd fonttools

    # create new virtual environment called e.g. 'fonttools-venv', or anything you like
    python -m virtualenv fonttools-venv

    # source the `activate` shell script to enter the environment (Unix-like); to exit, just type `deactivate`
    . fonttools-venv/bin/activate

    # to activate the virtual environment in Windows `cmd.exe`, do
    fonttools-venv\\Scripts\\activate.bat

    # install in 'editable' mode
    pip install -e .

Optional Requirements
---------------------

The ``fontTools`` package currently has no (required) external dependencies
besides the modules included in the Python Standard Library.
However, a few extra dependencies are required by some of its modules, which
are needed to unlock optional features.
The ``fonttools`` PyPI distribution also supports so-called "extras", i.e. a
set of keywords that describe a group of additional dependencies, which can be
used when installing via pip, or when specifying a requirement.
For example:

.. code:: sh

    pip install fonttools[ufo,lxml,woff,unicode]

This command will install fonttools, as well as the optional dependencies that
are required to unlock the extra features named "ufo", etc.

- ``Lib/fontTools/misc/etree.py``

  The module exports a ElementTree-like API for reading/writing XML files, and
  allows to use as the backend either the built-in ``xml.etree`` module or
  `lxml <https://lxml.de>`__. The latter is preferred whenever present,
  as it is generally faster and more secure.

  *Extra:* ``lxml``

- ``Lib/fontTools/ufoLib``

  Package for reading and writing UFO source files; it requires:

  * `fs <https://pypi.org/pypi/fs>`__: (aka ``pyfilesystem2``) filesystem
    abstraction layer.

  * `enum34 <https://pypi.org/pypi/enum34>`__: backport for the built-in ``enum``
    module (only required on Python < 3.4).

  *Extra:* ``ufo``

- ``Lib/fontTools/ttLib/woff2.py``

  Module to compress/decompress WOFF 2.0 web fonts; it requires:

  * `brotli <https://pypi.python.org/pypi/Brotli>`__: Python bindings of
    the Brotli compression library.

  *Extra:* ``woff``

- ``Lib/fontTools/ttLib/sfnt.py``

  To better compress WOFF 1.0 web fonts, the following module can be used
  instead of the built-in ``zlib`` library:

  * `zopfli <https://pypi.python.org/pypi/zopfli>`__: Python bindings of
    the Zopfli compression library.

  *Extra:* ``woff``

- ``Lib/fontTools/unicode.py``

  To display the Unicode character names when dumping the ``cmap`` table
  with ``ttx`` we use the ``unicodedata`` module in the Standard Library.
  TheÂ version included in there varies between different Python versions.
  To use the latest available data, you can install:

  * `unicodedata2 <https://pypi.python.org/pypi/unicodedata2>`__:
    ``unicodedata`` backport for Python 3.x updated to the latest Unicode
    version 15.0.

  *Extra:* ``unicode``

- ``Lib/fontTools/varLib/interpolatable.py``

  Module for finding wrongÂ contour/component order between different masters.
  It requires one of the following packages in order to solve the so-called
  "minimum weight perfect matching problem in bipartite graphs", or
  the Assignment problem:

  * `scipy <https://pypi.python.org/pypi/scipy>`__: the Scientific Library
    for Python, which internally uses `NumPy <https://pypi.python.org/pypi/numpy>`__
    arrays and hence is very fast;
  * `munkres <https://pypi.python.org/pypi/munkres>`__: a pure-Python
    module that implements the Hungarian or Kuhn-Munkres algorithm.

  To plot the results to a PDF or HTML format, you also need to install:

  * `pycairo <https://pypi.org/project/pycairo/>`__: Python bindings for the
    Cairo graphics library. Note that wheels are currently only available for
    Windows, for other platforms see pycairo's `installation instructions
    <https://pycairo.readthedocs.io/en/latest/getting_started.html>`__.

  *Extra:* ``interpolatable``

- ``Lib/fontTools/varLib/plot.py``

  Module for visualizing DesignSpaceDocument and resulting VariationModel.

  * `matplotlib <https://pypi.org/pypi/matplotlib>`__: 2D plotting library.

  *Extra:* ``plot``

- ``Lib/fontTools/misc/symfont.py``

  Advanced module for symbolic font statistics analysis; it requires:

  * `sympy <https://pypi.python.org/pypi/sympy>`__: the Python library for
    symbolic mathematics.

  *Extra:* ``symfont``

- ``Lib/fontTools/t1Lib.py``

  To get the file creatorÂ and type of Macintosh PostScript Type 1 fonts
  on Python 3 you need to install the following module, as the old ``MacOS``
  module is no longer included in Mac Python:

  * `xattr <https://pypi.python.org/pypi/xattr>`__: Python wrapper for
    extended filesystem attributes (macOS platform only).

  *Extra:* ``type1``

- ``Lib/fontTools/ttLib/removeOverlaps.py``

  Simplify TrueType glyphs by merging overlapping contours and components.

  * `skia-pathops <https://pypi.python.org/pypy/skia-pathops>`__: Python
    bindings for the Skia library's PathOps module, performing boolean
    operations on paths (union, intersection, etc.).

  *Extra:* ``pathops``

- ``Lib/fontTools/pens/cocoaPen.py`` and ``Lib/fontTools/pens/quartzPen.py``

  Pens for drawing glyphs with Cocoa ``NSBezierPath`` or ``CGPath`` require:

  * `PyObjC <https://pypi.python.org/pypi/pyobjc>`__: the bridge between
    Python and the Objective-C runtime (macOS platform only).

- ``Lib/fontTools/pens/qtPen.py``

  Pen for drawing glyphs with Qt's ``QPainterPath``, requires:

  * `PyQt5 <https://pypi.python.org/pypi/PyQt5>`__: Python bindings for
    the QtÂ cross platform UI and application toolkit.

- ``Lib/fontTools/pens/reportLabPen.py``

  Pen to drawing glyphs as PNG images, requires:

  * `reportlab <https://pypi.python.org/pypi/reportlab>`__: Python toolkit
    for generating PDFs and graphics.

- ``Lib/fontTools/pens/freetypePen.py``

  Pen to drawing glyphs with FreeType as raster images, requires:

  * `freetype-py <https://pypi.python.org/pypi/freetype-py>`__: Python binding
    for the FreeType library.
    
- ``Lib/fontTools/ttLib/tables/otBase.py``

  Use the Harfbuzz library to serialize GPOS/GSUB using ``hb_repack`` method, requires:
  
  * `uharfbuzz <https://pypi.python.org/pypi/uharfbuzz>`__: Streamlined Cython
    bindings for the harfbuzz shaping engine
    
  *Extra:* ``repacker``

How to make a new release
~~~~~~~~~~~~~~~~~~~~~~~~~

1) Update ``NEWS.rst`` with all the changes since the last release. Write a
   changelog entry for each PR, with one or two short sentences summarizing it,
   as well as links to the PR and relevant issues addressed by the PR. Do not
   put a new title, the next command will do it for you.
2) Use semantic versioning to decide whether the new release will be a 'major',
   'minor' or 'patch' release. It's usually one of the latter two, depending on
   whether new backward compatible APIs were added, or simply some bugs were fixed.
3) From inside a venv, first do ``pip install -r dev-requirements.txt``, then run
   the ``python setup.py release`` command from the tip of the ``main`` branch.
   By default this bumps the third or 'patch' digit only, unless you pass ``--major``
   or ``--minor`` to bump respectively the first or second digit.
   This bumps the package version string, extracts the changes since the latest
   version from ``NEWS.rst``, and uses that text to create an annotated git tag
   (or a signed git tag if you pass the ``--sign`` option and your git and Github
   account are configured for `signing commits <https://docs.github.com/en/github/authenticating-to-github/managing-commit-signature-verification/signing-commits>`__
   using a GPG key).
   It also commits an additional version bump which opens the main branch for
   the subsequent developmental cycle
4) Push both the tag and commit to the upstream repository, by running the command
   ``git push --follow-tags``. Note: it may push other local tags as well, be
   careful.
5) Let the CI build the wheel and source distribution packages and verify both
   get uploaded to the Python Package Index (PyPI).
6) [Optional] Go to fonttools `Github Releases <https://github.com/fonttools/fonttools/releases>`__
   page and create a new release, copy-pasting the content of the git tag
   message. This way, the release notes are nicely formatted as markdown, and
   users watching the repo will get an email notification. One day we shall
   automate that too.


Acknowledgements
~~~~~~~~~~~~~~~~

In alphabetical order:

aschmitz, Olivier Berten, Samyak Bhuta, Erik van Blokland, Petr van Blokland,
Jelle Bosma, Sascha Brawer, Tom Byrer, Antonio Cavedoni, FrÃ©dÃ©ric Coiffier,
Vincent Connare, David Corbett, Simon Cozens, Dave Crossland, Simon Daniels,
Peter Dekkers, Behdad Esfahbod, Behnam Esfahbod, Hannes Famira, Sam Fishman,
Matt Fontaine, Takaaki Fuji, Rob Hagemans, Yannis Haralambous, Greg Hitchcock,
Jeremie Hornus, Khaled Hosny, John Hudson, Denis Moyogo Jacquerye, Jack Jansen,
Tom Kacvinsky, Jens Kutilek, Antoine Leca, Werner Lemberg, Tal Leming, Peter
Lofting, Cosimo Lupo, Olli Meier, Masaya Nakamura, Dave Opstad, Laurence Penney,
Roozbeh Pournader, Garret Rieger, Read Roberts, Colin Rofls, Guido van Rossum,
Just van Rossum, Andreas Seidel, Georg Seifert, Chris Simpkins, Miguel Sousa,
Adam Twardoch, Adrien TÃ©tar, Vitaly Volkov, Paul Wise.

Copyrights
~~~~~~~~~~

| Copyright (c) 1999-2004 Just van Rossum, LettError
  (just@letterror.com)
| See `LICENSE <LICENSE>`__ for the full license.

Copyright (c) 2000 BeOpen.com. All Rights Reserved.

Copyright (c) 1995-2001 Corporation for National Research Initiatives.
All Rights Reserved.

Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam. All
Rights Reserved.

Have fun!

.. |CI Build Status| image:: https://github.com/fonttools/fonttools/workflows/Test/badge.svg
   :target: https://github.com/fonttools/fonttools/actions?query=workflow%3ATest
.. |Coverage Status| image:: https://codecov.io/gh/fonttools/fonttools/branch/main/graph/badge.svg
   :target: https://codecov.io/gh/fonttools/fonttools
.. |PyPI| image:: https://img.shields.io/pypi/v/fonttools.svg
   :target: https://pypi.org/project/FontTools
.. |Gitter Chat| image:: https://badges.gitter.im/fonttools-dev/Lobby.svg
   :alt: Join the chat at https://gitter.im/fonttools-dev/Lobby
   :target: https://gitter.im/fonttools-dev/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

Changelog
~~~~~~~~~

4.55.4 (released 2025-01-21)
----------------------------

- [bezierTools] Fixed ``splitCubicAtT`` sometimes not returning identical start/end points as result of numerical precision (#3742, #3743).
- [feaLib/ast] Fixed docstring of ``AlternateSubstStatement`` (#3735).
- [transform] Typing fixes (#3734).

4.55.3 (released 2024-12-10)
----------------------------

- [Docs] fill out ttLib table section [#3716]
- [feaLib] More efficient inline format 4 lookups [#3726]

4.55.2 (released 2024-12-05)
----------------------------

- [Docs] update Sphinx config (#3712)
- [designspaceLib] Allow axisOrdering to be set to zero (#3715)
- [feaLib] Donâ€™t modify variable anchors in place (#3717)

4.55.1 (released 2024-12-02)
----------------------------

- [ttGlyphSet] Support VARC CFF2 fonts (#3683)
- [DecomposedTransform] Document and implement always skewY == 0 (#3697)
- [varLib] "Fix" cython iup issue? (#3704)
- Cython minor refactor (#3705)


4.55.0 (released 2024-11-14)
----------------------------

- [cffLib.specializer] Adjust stack use calculation (#3689)
- [varLib] Lets not add mac names if the rest of name doesn't have them (#3688)
- [ttLib.reorderGlyphs] Update CFF table charstrings and charset (#3682)
- [cffLib.specializer] Add cmdline to specialize a CFF2 font (#3675, #3679)
- [CFF2] Lift uint16 VariationStore.length limitation (#3674)
- [subset] consider variation selectors subsetting cmap14 (#3672)
- [varLib.interpolatable] Support CFF2 fonts (#3670)
- Set isfinal to true in XML parser for proper resource cleanup (#3669)
- [removeOverlaps] Fix CFF CharString width (#3659)
- [glyf] Add optimizeSize option (#3657)
- Python 3.13 support (#3656)
- [TupleVariation] Optimize for loading speed, not size (#3650, #3653)


4.54.1 (released 2024-09-24)
----------------------------

- [unicodedata] Update to Unicode 16
- [subset] Escape ``\\\\`` in doc string

4.54.0 (released 2024-09-23)
----------------------------

- [Docs] Small docs cleanups by @n8willis (#3611)
- [Docs] cleanup code blocks by @n8willis (#3627)
- [Docs] fix Sphinx builds by @n8willis (#3625)
- [merge] Minor fixes to documentation for merge by @drj11 (#3588)
- [subset] Small tweaks to pyftsubset documentation by @RoelN (#3633)
- [Tests] Do not require fonttools command to be available by @behdad (#3612)
- [Tests] subset_test: add failing test to reproduce issue #3616 by @anthrotype (#3622)
- [ttLib] NameRecordVisitor: include whole sequence of character variants' UI labels, not just the first by @anthrotype (#3617)
- [varLib.avar] Reconstruct mappings from binary by @behdad (#3598)
- [varLib.instancer] Fix visual artefacts with partial L2 instancing by @Hoolean (#3635)
- [varLib.interpolatable] Support discrete axes in .designspace by @behdad (#3599)
- [varLib.models] By default, assume OpenType-like normalized space by @behdad (#3601)

4.53.1 (released 2024-07-05)
----------------------------

- [feaLib] Improve the sharing of inline chained lookups (#3559)
- [otlLib] Correct the calculation of OS/2.usMaxContext with reversed chaining contextual single substitutions (#3569)
- [misc.visitor] Visitors search the inheritance chain of objects they are visiting (#3581)

4.53.0 (released 2024-05-31)
----------------------------

- [ttLib.removeOverlaps] Support CFF table to aid in downconverting CFF2 fonts (#3528)
- [avar] Fix crash when accessing not-yet-existing attribute (#3550)
- [docs] Add buildMathTable to otlLib.builder documentation (#3540)
- [feaLib] Allow UTF-8 with BOM when reading features (#3495)
- [SVGPathPen] Revert rounding coordinates to two decimal places by default (#3543)
- [varLib.instancer] Refix output filename decision-making  (#3545, #3544, #3548)

4.52.4 (released 2024-05-27)
----------------------------

- [varLib.cff] Restore and deprecate convertCFFtoCFF2 that was removed in 4.52.0
  release as it is used by downstream projects (#3535).

4.52.3 (released 2024-05-27)
----------------------------

- Fixed a small syntax error in the reStructuredText-formatted NEWS.rst file
  which caused the upload to PyPI to fail for 4.52.2. No other code changes.

4.52.2 (released 2024-05-27)
----------------------------

- [varLib.interpolatable] Ensure that scipy/numpy output is JSON-serializable
  (#3522, #3526).
- [housekeeping] Regenerate table lists, to fix pyinstaller packaging of the new
  ``VARC`` table (#3531, #3529).
- [cffLib] Make CFFToCFF2 and CFF2ToCFF more robust (#3521, #3525).

4.52.1 (released 2024-05-24)
----------------------------

- Fixed a small syntax error in the reStructuredText-formatted NEWS.rst file
  which caused the upload to PyPI to fail for 4.52.0. No other code changes.

4.52.0 (released 2024-05-24)
----------------------------

- Added support for the new ``VARC`` (Variable Composite) table that is being
  proposed to OpenType spec (#3395). For more info:
  https://github.com/harfbuzz/boring-expansion-spec/blob/main/VARC.md
- [ttLib.__main__] Fixed decompiling all tables (90fed08).
- [feaLib] Don't reference the same lookup index multiple times within the same
  feature record, it is only applied once anyway (#3520).
- [cffLib] Moved methods to desubroutinize, remove hints and unused subroutines
  from subset module to cffLib (#3517).
- [varLib.instancer] Added support for partial-instancing CFF2 tables! Also, added
  method to down-convert from CFF2 to CFF 1.0, and CLI entry points to convert
  CFF<->CFF2 (#3506).
- [subset] Prune unused user name IDs even with --name-IDs='*' (#3410).
- [ttx] use GNU-style getopt to intermix options and positional arguments (#3509).
- [feaLib.variableScalar] Fixed ``value_at_location()`` method (#3491)
- [psCharStrings] Shorten output of ``encodeFloat`` (#3492).
- [bezierTools] Fix infinite-recursion in ``calcCubicArcLength`` (#3502).
- [avar2] Implement ``avar2`` support in ``TTFont.getGlyphSet()`` (#3473).

4.51.0 (released 2024-04-05)
----------------------------

- [ttLib] Optimization on loading aux fields (#3464).
- [ttFont] Add reorderGlyphs (#3468).

4.50.0 (released 2024-03-15)
----------------------------

- [pens] Added decomposing filter pens that draw components as regular contours (#3460).
- [instancer] Drop explicit no-op axes from TupleVariations (#3457).
- [cu2qu/ufo] Return set of modified glyph names from fonts_to_quadratic (#3456).

4.49.0 (released 2024-02-15)
----------------------------

- [otlLib] Add API for building ``MATH`` table (#3446)

4.48.1 (released 2024-02-06)
----------------------------

- Fixed uploading wheels to PyPI, no code changes since v4.48.0.

4.48.0 (released 2024-02-06)
----------------------------

- [varLib] Do not log when there are no OTL tables to be merged.
- [setup.py] Do not restrict lxml<5 any more, tests pass just fine with lxml>=5.
- [feaLib] Remove glyph and class names length restrictions in FEA (#3424).
- [roundingPens] Added ``transformRoundFunc`` parameter to the rounding pens to allow
  for custom rounding of the components' transforms (#3426).
- [feaLib] Keep declaration order of ligature components within a ligature set, instead
  of sorting by glyph name (#3429).
- [feaLib] Fixed ordering of alternates in ``aalt`` lookups, following the declaration
  order of feature references within the ``aalt`` feature block (#3430).
- [varLib.instancer] Fixed a bug in the instancer's IUP optimization (#3432).
- [sbix] Support sbix glyphs with new graphicType "flip" (#3433).
- [svgPathPen] Added ``--glyphs`` option to dump the SVG paths for the named glyphs
  in the font (0572f78).
- [designspaceLib] Added "description" attribute to ``<mappings>`` and ``<mapping>``
  elements, and allow multiple ``<mappings>`` elements to group ``<mapping>`` elements
  that are logically related (#3435, #3437).
- [otlLib] Correctly choose the most compact GSUB contextual lookup format (#3439).

4.47.2 (released 2024-01-11)
----------------------------

Minor release to fix uploading wheels to PyPI.

4.47.1 (released 2024-01-11)
----------------------------

- [merge] Improve help message and add standard command line options (#3408)
- [otlLib] Pass ``ttFont`` to ``name.addName`` in ``buildStatTable`` (#3406)
- [featureVars] Re-use ``FeatureVariationRecord``'s when possible (#3413)

4.47.0 (released 2023-12-18)
----------------------------

- [varLib.models] New API for VariationModel: ``getMasterScalars`` and
  ``interpolateFromValuesAndScalars``.
- [varLib.interpolatable] Various bugfixes and rendering improvements. In particular,
  add a Summary page in the front, and an Index and Table-of-Contents in the back.
  Change the page size to Letter.
- [Docs/designspaceLib] Defined a new ``public.fontInfo`` lib key, not used anywhere yet (#3358).

4.46.0 (released 2023-12-02)
----------------------------

- [featureVars] Allow to register the same set of substitution rules to multiple features.
  The ``addFeatureVariations`` function can now take a list of featureTags; similarly, the
  lib key 'com.github.fonttools.varLib.featureVarsFeatureTag' can now take a
  comma-separateed string of feature tags (e.g. "salt,ss01") instead of a single tag (#3360).
- [featureVars] Don't overwrite GSUB FeatureVariations, but append new records to it
  for features which are not already there. But raise ``VarLibError`` if the feature tag
  already has feature variations associated with it (#3363).
- [varLib] Added ``addGSUBFeatureVariations`` function to add GSUB Feature Variations
  to an existing variable font from rules defined in a DesignSpace document (#3362).
- [varLib.interpolatable] Various bugfixes and rendering improvements. In particular,
  a new test for "underweight" glyphs. The new test reports quite a few false-positives
  though. Please send feedback.

4.45.1 (released 2023-11-23)
----------------------------

- [varLib.interpolatable] Various bugfixes and improvements, better reporting, reduced
  false positives.
- [ttGlyphSet] Added option to not recalculate glyf bounds (#3348).

4.45.0 (released 2023-11-20)
----------------------------

- [varLib.interpolatable] Vastly improved algorithms. Also available now is ``--pdf``
  and ``--html`` options to generate a PDF or HTML report of the interpolation issues.
  The PDF/HTML report showcases the problematic masters, the interpolated broken
  glyph, as well as the proposed fixed version.

4.44.3 (released 2023-11-15)
----------------------------

- [subset] Only prune codepage ranges for OS/2.version >= 1, ignore otherwise (#3334).
- [instancer] Ensure hhea vertical metrics stay in sync with OS/2 ones after instancing
  MVAR table containing 'hasc', 'hdsc' or 'hlgp' tags (#3297).

4.44.2 (released 2023-11-14)
----------------------------

- [glyf] Have ``Glyph.recalcBounds`` skip empty components (base glyph with no contours)
  when computing the bounding box of composite glyphs. This simply restores the existing
  behavior before some changes were introduced in fonttools 4.44.0 (#3333).

4.44.1 (released 2023-11-14)
----------------------------

- [feaLib] Ensure variable mark anchors are deep-copied while building since they
  get modified in-place and later reused (#3330).
- [OS/2|subset] Added method to ``recalcCodePageRanges`` to OS/2 table class; added
  ``--prune-codepage-ranges`` to `fonttools subset` command (#3328, #2607).

4.44.0 (released 2023-11-03)
----------------------------

- [instancer] Recalc OS/2 AvgCharWidth after instancing if default changes (#3317).
- [otlLib] Make ClassDefBuilder class order match varLib.merger's, i.e. large
  classes first, then glyph lexicographic order (#3321, #3324).
- [instancer] Allow not specifying any of min:default:max values and let be filled
  up with fvar's values (#3322, #3323).
- [instancer] When running --update-name-table ignore axes that have no STAT axis
  values (#3318, #3319).
- [Debg] When dumping to ttx, write the embedded JSON as multi-line string with
  indentation (92cbfee0d).
- [varStore] Handle > 65535 items per encoding by splitting VarData subtable (#3310).
- [subset] Handle null-offsets in MarkLigPos subtables.
- [subset] Keep East Asian spacing fatures vhal, halt, chws, vchw by default (#3305).
- [instancer.solver] Fixed case where axisDef < lower and upper < axisMax (#3304).
- [glyf] Speed up compilation, mostly around ``recalcBounds`` (#3301).
- [varLib.interpolatable] Speed it up when working on variable fonts, plus various
  micro-optimizations (#3300).
- Require unicodedata2 >= 15.1.0 when installed with 'unicode' extra, contains UCD 15.1.

4.43.1 (released 2023-10-06)
----------------------------

- [EBDT] Fixed TypeError exception in `_reverseBytes` method triggered when dumping
  some bitmap fonts with `ttx -z bitwise` option (#3162).
- [v/hhea] Fixed UnboundLocalError exception in ``recalc`` method when no vmtx or hmtx
  tables are present (#3290).
- [bezierTools] Fixed incorrectly typed cython local variable leading to TypeError when
  calling ``calcQuadraticArcLength`` (#3288).
- [feaLib/otlLib] Better error message when building Coverage table with missing glyph (#3286).

4.43.0 (released 2023-09-29)
----------------------------

- [subset] Set up lxml ``XMLParser(resolve_entities=False)`` when parsing OT-SVG documents
  to prevent XML External Entity (XXE) attacks (9f61271dc):
  https://codeql.github.com/codeql-query-help/python/py-xxe/
- [varLib.iup] Added workaround for a Cython bug in ``iup_delta_optimize`` that was
  leading to IUP tolerance being incorrectly initialised, resulting in sub-optimal deltas
  (60126435d, cython/cython#5732).
- [varLib] Added new command-line entry point ``fonttools varLib.avar`` to add an
  ``avar`` table to an existing VF from axes mappings in a .designspace file (0a3360e52).
- [instancer] Fixed bug whereby no longer used variation regions were not correctly pruned
  after VarData optimization (#3268).
- Added support for Python 3.12 (#3283).

4.42.1 (released 2023-08-20)
----------------------------

- [t1Lib] Fixed several Type 1 issues (#3238, #3240).
- [otBase/packer] Allow sharing tables reached by different offset sizes (#3241, #3236).
- [varLib/merger] Fix Cursive attachment merging error when all anchors are NULL (#3248, #3247).
- [ttLib] Fixed warning when calling ``addMultilingualName`` and ``ttFont`` parameter was not
  passed on to ``findMultilingualName`` (#3253).

4.42.0 (released 2023-08-02)
----------------------------

- [varLib] Use sentinel value 0xFFFF to mark a glyph advance in hmtx/vmtx as non
  participating, allowing sparse masters to contain glyphs for variation purposes other
  than {H,V}VAR (#3235).
- [varLib/cff] Treat empty glyphs in non-default masters as missing, thus not participating
  in CFF2 delta computation, similarly to how varLib already treats them for gvar (#3234).
- Added varLib.avarPlanner script to deduce 'correct' avar v1 axis mappings based on
  glyph average weights (#3223).

4.41.1 (released 2023-07-21)
----------------------------

- [subset] Fixed perf regression in v4.41.0 by making ``NameRecordVisitor`` only visit
  tables that do contain nameID references (#3213, #3214).
- [varLib.instancer] Support instancing fonts containing null ConditionSet offsets in
  FeatureVariationRecords (#3211, #3212).
- [statisticsPen] Report font glyph-average weight/width and font-wide slant.
- [fontBuilder] Fixed head.created date incorrectly set to 0 instead of the current
  timestamp, regression introduced in v4.40.0 (#3210).
- [varLib.merger] Support sparse ``CursivePos`` masters (#3209).

4.41.0 (released 2023-07-12)
----------------------------

- [fontBuilder] Fixed bug in setupOS2 with default panose attribute incorrectly being
  set to a dict instead of a Panose object (#3201).
- [name] Added method to ``removeUnusedNameRecords`` in the user range (#3185).
- [varLib.instancer] Fixed issue with L4 instancing (moving default) (#3179).
- [cffLib] Use latin1 so we can roundtrip non-ASCII in {Full,Font,Family}Name (#3202).
- [designspaceLib] Mark <source name="..."> as optional in docs (as it is in the code).
- [glyf-1] Fixed drawPoints() bug whereby last cubic segment becomes quadratic (#3189, #3190).
- [fontBuilder] Propagate the 'hidden' flag to the fvar Axis instance (#3184).
- [fontBuilder] Update setupAvar() to also support avar 2, fixing ``_add_avar()`` call
  site (#3183).
- Added new ``voltLib.voltToFea`` submodule (originally Tiro Typeworks' "Volto") for
  converting VOLT OpenType Layout sources to FEA format (#3164).

4.40.0 (released 2023-06-12)
----------------------------

- Published native binary wheels to PyPI for all the python minor versions and platform
  and architectures currently supported that would benefit from this. They will include
  precompiled Cython-accelerated modules (e.g. cu2qu) without requiring to compile them
  from source. The pure-python wheel and source distribution will continue to be
  published as always (pip will automatically chose them when no binary wheel is
  available for the given platform, e.g. pypy). Use ``pip install --no-binary=fonttools fonttools``
  to expliclity request pip to install from the pure-python source.
- [designspaceLib|varLib] Add initial support for specifying axis mappings and build
  ``avar2`` table from those (#3123).
- [feaLib] Support variable ligature caret position (#3130).
- [varLib|glyf] Added option to --drop-implied-oncurves; test for impliable oncurve
  points either before or after rounding (#3146, #3147, #3155, #3156).
- [TTGlyphPointPen] Don't error with empty contours, simply ignore them (#3145).
- [sfnt] Fixed str vs bytes remnant of py3 transition in code dealing with de/compiling
  WOFF metadata (#3129).
- [instancer-solver] Fixed bug when moving default instance with sparse masters (#3139, #3140).
- [feaLib] Simplify variable scalars that donâ€™t vary (#3132).
- [pens] Added filter pen that explicitly emits closing line when lastPt != movePt (#3100).
- [varStore] Improve optimize algorithm and better document the algorithm (#3124, #3127).
  Added ``quantization`` option (#3126).
- Added CI workflow config file for building native binary wheels (#3121).
- [fontBuilder] Added glyphDataFormat=0 option; raise error when glyphs contain cubic
  outlines but glyphDataFormat was not explicitly set to 1 (#3113, #3119).
- [subset] Prune emptied GDEF.MarkGlyphSetsDef and remap indices; ensure GDEF is
  subsetted before GSUB and GPOS (#3114, #3118).
- [xmlReader] Fixed issue whereby DSIG table data was incorrectly parsed (#3115, #2614).
- [varLib/merger] Fixed merging of SinglePos with pos=0 (#3111, #3112).
- [feaLib] Demote "Feature has not been defined" error to a warning when building aalt
  and referenced feature is empty (#3110).
- [feaLib] Dedupe multiple substitutions with classes (#3105).

4.39.4 (released 2023-05-10)
----------------------------

- [varLib.interpolatable] Allow for sparse masters (#3075)
- [merge] Handle differing default/nominalWidthX in CFF (#3070)
- [ttLib] Add missing main.py file to ttLib package (#3088)
- [ttx] Fix missing composite instructions in XML (#3092)
- [ttx] Fix split tables option to work on filenames containing '%' (#3096)
- [featureVars] Process lookups for features other than rvrn last (#3099)
- [feaLib] support multiple substitution with classes (#3103)

4.39.3 (released 2023-03-28)
----------------------------

- [sbix] Fixed TypeError when compiling empty glyphs whose imageData is None, regression
  was introduced in v4.39 (#3059).
- [ttFont] Fixed AttributeError on python <= 3.10 when opening a TTFont from a tempfile
  SpooledTemporaryFile, seekable method only added on python 3.11 (#3052).

4.39.2 (released 2023-03-16)
----------------------------

- [varLib] Fixed regression introduced in 4.39.1 whereby an incomplete 'STAT' table
  would be built even though a DesignSpace v5 did contain 'STAT' definitions (#3045, #3046).

4.39.1 (released 2023-03-16)
----------------------------

- [avar2] Added experimental support for reading/writing avar version 2 as specified in
  this draft proposal: https://github.com/harfbuzz/boring-expansion-spec/blob/main/avar2.md
- [glifLib] Wrap underlying XML library exceptions with GlifLibError when parsing GLIFs,
  and also print the name and path of the glyph that fails to be parsed (#3042).
- [feaLib] Consult avar for normalizing user-space values in ConditionSets and in
  VariableScalars (#3042, #3043).
- [ttProgram] Handle string input to Program.fromAssembly() (#3038).
- [otlLib] Added a config option to emit GPOS 7 lookups, currently disabled by default
  because of a macOS bug (#3034).
- [COLRv1] Added method to automatically compute ClipBoxes (#3027).
- [ttFont] Fixed getGlyphID to raise KeyError on missing glyphs instead of returning
  None. The regression was introduced in v4.27.0 (#3032).
- [sbix] Fixed UnboundLocalError: cannot access local variable 'rawdata' (#3031).
- [varLib] When building VF, do not overwrite a pre-existing ``STAT`` table that was built
  with feaLib from FEA feature file. Also, added support for building multiple VFs
  defined in Designspace v5 from ``fonttools varLib`` script (#3024).
- [mtiLib] Only add ``Debg`` table with lookup names when ``FONTTOOLS_LOOKUP_DEBUGGING``
  env variable is set (#3023).

4.39.0 (released 2023-03-06)
----------------------------

- [mtiLib] Optionally add `Debg` debug info for MTI feature builds (#3018).
- [ttx] Support reading input file from standard input using special `-` character,
  similar to existing `-o -` option to write output to standard output (#3020).
- [cython] Prevent ``cython.compiled`` raise AttributeError if cython not installed
  properly (#3017).
- [OS/2] Guard against ZeroDivisionError when calculating xAvgCharWidth in the unlikely
  scenario no glyph has non-zero advance (#3015).
- [subset] Recompute xAvgCharWidth independently of --no-prune-unicode-ranges,
  previously the two options were involuntarily bundled together (#3012).
- [fontBuilder] Add ``debug`` parameter to addOpenTypeFeatures method to add source
  debugging information to the font in the ``Debg`` private table (#3008).
- [name] Make NameRecord `__lt__` comparison not fail on Unicode encoding errors (#3006).
- [featureVars] Fixed bug in ``overlayBox`` (#3003, #3005).
- [glyf] Added experimental support for cubic bezier curves in TrueType glyf table, as
  outlined in glyf v1 proposal (#2988):
  https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-cubicOutlines.md
- Added new qu2cu module and related qu2cuPen, the reverse of cu2qu for converting
  TrueType quadratic splines to cubic bezier curves (#2993).
- [glyf] Added experimental support for reading and writing Variable Composites/Components
  as defined in glyf v1 spec proposal (#2958):
  https://github.com/harfbuzz/boring-expansion-spec/blob/main/glyf1-varComposites.md.
- [pens]: Added `addVarComponent` method to pen protocols' base classes, which pens can implement
  to handle varcomponents (by default they get decomposed) (#2958).
- [misc.transform] Added DecomposedTransform class which implements an affine transformation
  with separate translate, rotation, scale, skew, and transformation-center components (#2598)
- [sbix] Ensure Glyph.referenceGlyphName is set; fixes error after dumping and
  re-compiling sbix table with 'dupe' glyphs (#2984).
- [feaLib] Be cleverer when merging chained single substitutions into same lookup
  when they are specified using the inline notation (#2150, #2974).
- [instancer] Clamp user-inputted axis ranges to those of fvar (#2959).
- [otBase/subset] Define ``__getstate__`` for BaseTable so that a copied/pickled 'lazy'
  object gets its own OTTableReader to read from; incidentally fixes a bug while
  subsetting COLRv1 table containing ClipBoxes on python 3.11 (#2965, #2968).
- [sbix] Handle glyphs with "dupe" graphic type on compile correctly (#2963).
- [glyf] ``endPointsOfContours`` field should be unsigned! Kudos to behdad for
  spotting one of the oldest bugs in FT. Probably nobody has ever dared to make
  glyphs with more than 32767 points... (#2957).
- [feaLib] Fixed handling of ``ignore`` statements with unmarked glyphs to match
  makeotf behavior, which assumes the first glyph is marked (#2950).
- Reformatted code with ``black`` and enforce new code style via CI check (#2925).
- [feaLib] Sort name table entries following OT spec prescribed order in the builder (#2927).
- [cu2quPen] Add Cu2QuMultiPen that converts multiple outlines at a time in
  interpolation compatible way; its methods take a list of tuples arguments
  that would normally be passed to individual segment pens, and at the end it
  dispatches the converted outlines to each pen (#2912).
- [reverseContourPen/ttGlyphPen] Add outputImpliedClosingLine option (#2913, #2914,
  #2921, #2922, #2995).
- [gvar] Avoid expanding all glyphs unnecessarily upon compile (#2918).
- [scaleUpem] Fixed bug whereby CFF2 vsindex was scaled; it should not (#2893, #2894).
- [designspaceLib] Add DS.getAxisByTag and refactor getAxis (#2891).
- [unicodedata] map Zmth<->math in ot_tag_{to,from}_script (#1737, #2889).
- [woff2] Support encoding/decoding OVERLAP_SIMPLE glyf flags (#2576, #2884).
- [instancer] Update OS/2 class and post.italicAngle when default moved (L4)
- Dropped support for Python 3.7 which reached EOL, fontTools requires 3.8+.
- [instancer] Fixed instantiateFeatureVariations logic when a rule range becomes
  default-applicable (#2737, #2880).
- [ttLib] Add main to ttFont and ttCollection that just decompile and re-compile the
  input font (#2869).
- [featureVars] Insert 'rvrn' lookup at the beginning of LookupList, to work around bug
  in Apple implementation of 'rvrn' feature which the spec says it should be processed
  early whereas on macOS 10.15 it follows lookup order (#2140, #2867).
- [instancer/mutator] Remove 'DSIG' table if present.
- [svgPathPen] Don't close path in endPath(), assume open unless closePath() (#2089, #2865).

4.38.0 (released 2022-10-21)
----------------------------

- [varLib.instancer] Added support for L4 instancing, i.e. moving the default value of
  an axis while keeping it variable. Thanks Behdad! (#2728, #2861).
  It's now also possible to restrict an axis min/max values beyond the current default
  value, e.g. a font wght has min=100, def=400, max=900 and you want a partial VF that
  only varies between 500 and 700, you can now do that.
  You can either specify two min/max values (wght=500:700), and the new default will be
  set to either the minimum or maximum, depending on which one is closer to the current
  default (e.g. 500 in this case). Or you can specify three values (e.g. wght=500:600:700)
  to specify the new default value explicitly.
- [otlLib/featureVars] Set a few Count values so one doesn't need to compile the font
  to update them (#2860).
- [varLib.models] Make extrapolation work for 2-master models as well where one master
  is at the default location (#2843, #2846).
  Add optional extrapolate=False to normalizeLocation() (#2847, #2849).
- [varLib.cff] Fixed sub-optimal packing of CFF2 deltas by no longer rounding them to
  integer (#2838).
- [scaleUpem] Calculate numShorts in VarData after scale; handle CFF hintmasks (#2840).

4.37.4 (released 2022-09-30)
----------------------------

- [subset] Keep nameIDs used by CPAL palette entry labels (#2837).
- [varLib] Avoid negative hmtx values when creating font from variable CFF2 font (#2827).
- [instancer] Don't prune stat.ElidedFallbackNameID (#2828).
- [unicodedata] Update Scripts/Blocks to Unicode 15.0 (#2833).

4.37.3 (released 2022-09-20)
----------------------------

- Fix arguments in calls to (glyf) glyph.draw() and drawPoints(), whereby offset wasn't
  correctly passed down; this fix also exposed a second bug, where lsb and tsb were not
  set (#2824, #2825, adobe-type-tools/afdko#1560).

4.37.2 (released 2022-09-15)
----------------------------

- [subset] Keep CPAL table and don't attempt to prune unused color indices if OT-SVG
  table is present even if COLR table was subsetted away; OT-SVG may be referencing the
  CPAL table; for now we assume that's the case (#2814, #2815).
- [varLib.instancer] Downgrade GPOS/GSUB version if there are no more FeatureVariations
  after instancing (#2812).
- [subset] Added ``--no-lazy`` to optionally load fonts eagerly (mostly to ease
  debugging of table lazy loading, no practical effects) (#2807).
- [varLib] Avoid building empty COLR.DeltaSetIndexMap with only identity mappings (#2803).
- [feaLib] Allow multiple value record types (by promoting to the most general format)
  within the same PairPos subtable; e.g. this allows variable and non variable kerning
  rules to share the same subtable. This also fixes a bug whereby some kerning pairs
  would become unreachable while shapiong because of premature subtable splitting (#2772, #2776).
- [feaLib] Speed up ``VarScalar`` by caching models for recurring master locations (#2798).
- [feaLib] Optionally cythonize ``feaLib.lexer``, speeds up parsing FEA a bit (#2799).
- [designspaceLib] Avoid crash when handling unbounded rule conditions (#2797).
- [post] Don't crash if ``post`` legacy format 1 is malformed/improperly used (#2786)
- [gvar] Don't be "lazy" (load all glyph variations up front) when TTFont.lazy=False (#2771).
- [TTFont] Added ``normalizeLocation`` method to normalize a location dict from the
  font's defined axes space (also known as "user space") into the normalized (-1..+1)
  space. It applies ``avar`` mapping if the font contains an ``avar`` table (#2789).
- [TTVarGlyphSet] Support drawing glyph instances from CFF2 variable glyph set (#2784).
- [fontBuilder] Do not error when building cmap if there are zero code points (#2785).
- [varLib.plot] Added ability to plot a variation model and set of accompaning master
  values corresponding to the model's master locations into a pyplot figure (#2767).
- [Snippets] Added ``statShape.py`` script to draw statistical shape of a glyph as an
  ellips (requires pycairo) (baecd88).
- [TTVarGlyphSet] implement drawPoints natively, avoiding going through
  SegmentToPointPen (#2778).
- [TTVarGlyphSet] Fixed bug whereby drawing a composite glyph multiple times, its
  components would shif; needed an extra copy (#2774).

4.37.1 (released 2022-08-24)
----------------------------

- [subset] Fixed regression introduced with v4.37.0 while subsetting the VarStore of
  ``HVAR`` and ``VVAR`` tables, whereby an ``AttributeError: subset_varidxes`` was
  thrown because an apparently unused import statement (with the side-effect of
  dynamically binding that ``subset_varidxes`` method to the VarStore class) had been
  accidentally deleted in an unrelated PR (#2679, #2773).
- [pens] Added ``cairoPen`` (#2678).
- [gvar] Read ``gvar`` more lazily by not parsing all of the ``glyf`` table (#2771).
- [ttGlyphSet] Make ``drawPoints(pointPen)`` method work for CFF fonts as well via
  adapter pen (#2770).

4.37.0 (released 2022-08-23)
----------------------------

- [varLib.models] Reverted PR #2717 which added support for "narrow tents" in v4.36.0,
  as it introduced a regression (#2764, #2765). It will be restored in upcoming release
  once we found a solution to the bug.
- [cff.specializer] Fixed issue in charstring generalizer with the ``blend`` operator
  (#2750, #1975).
- [varLib.models] Added support for extrapolation (#2757).
- [ttGlyphSet] Ensure the newly added ``_TTVarGlyphSet`` inherits from ``_TTGlyphSet``
  to keep backward compatibility with existing API (#2762).
- [kern] Allow compiling legacy kern tables with more than 64k entries (d21cfdede).
- [visitor] Added new visitor API to traverse tree of objects and dispatch based
  on the attribute type: cf. ``fontTools.misc.visitor`` and ``fontTools.ttLib.ttVisitor``. Added ``fontTools.ttLib.scaleUpem`` module that uses the latter to
  change a font's units-per-em and scale all the related fields accordingly (#2718,
  #2755).

4.36.0 (released 2022-08-17)
----------------------------

- [varLib.models] Use a simpler model that generates narrower "tents" (regions, master
  supports) whenever possible: specifically when any two axes that actively "cooperate"
  (have masters at non-zero positions for both axes) have a complete set of intermediates.
  The simpler algorithm produces fewer overlapping regions and behaves better with
  respect to rounding at the peak positions than the generic solver, always matching
  intermediate masters exactly, instead of maximally 0.5 units off. This may be useful
  when 100% metrics compatibility is desired (#2218, #2717).
- [feaLib] Remove warning when about ``GDEF`` not being built when explicitly not
  requested; don't build one unconditonally even when not requested (#2744, also works
  around #2747).
- [ttFont] ``TTFont.getGlyphSet`` method now supports selecting a location that
  represents an instance of a variable font (supports both user-scale and normalized
  axes coordinates via the ``normalized=False`` parameter). Currently this only works
  for TrueType-flavored variable fonts (#2738).

4.35.0 (released 2022-08-15)
----------------------------

- [otData/otConverters] Added support for 'biased' PaintSweepGradient start/end angles
  to match latest COLRv1 spec (#2743).
- [varLib.instancer] Fixed bug in ``_instantiateFeatureVariations`` when at the same
  time pinning one axis and restricting the range of a subsequent axis; the wrong axis
  tag was being used in the latter step (as the records' axisIdx was updated in the
  preceding step but looked up using the old axes order in the following step) (#2733,
  #2734).
- [mtiLib] Pad script tags with space when less than 4 char long (#1727).
- [merge] Use ``'.'`` instead of ``'#'`` in duplicate glyph names (#2742).
- [gvar] Added support for lazily loading glyph variations (#2741).
- [varLib] In ``build_many``, we forgot to pass on ``colr_layer_reuse`` parameter to
  the ``build`` method (#2730).
- [svgPathPen] Add a main that prints SVG for input text (6df779fd).
- [cffLib.width] Fixed off-by-one in optimized values; previous code didn't match the
  code block above it (2963fa50).
- [varLib.interpolatable] Support reading .designspace and .glyphs files (via optional
  ``glyphsLib``).
- Compile some modules with Cython when available and building/installing fonttools
  from source: ``varLib.iup`` (35% faster), ``pens.momentsPen`` (makes
  ``varLib.interpolatable`` 3x faster).
- [feaLib] Allow features to be built for VF without also building a GDEF table (e.g.
  only build GSUB); warn when GDEF would be needed but isn't requested (#2705, 2694).
- [otBase] Fixed ``AttributeError`` when uharfbuzz < 0.23.0 and 'repack' method is
  missing (32aa8eaf). Use new ``uharfbuzz.repack_with_tag`` when available (since
  uharfbuzz>=0.30.0), enables table-specific optimizations to be performed during
  repacking (#2724).
- [statisticsPen] By default report all glyphs (4139d891). Avoid division-by-zero
  (52b28f90).
- [feaLib] Added missing required argument to FeatureLibError exception (#2693)
- [varLib.merge] Fixed error during error reporting (#2689). Fixed undefined
  ``NotANone`` variable (#2714).

4.34.4 (released 2022-07-07)
----------------------------

- Fixed typo in varLib/merger.py that causes NameError merging COLR glyphs
  containing more than 255 layers (#2685).

4.34.3 (released 2022-07-07)
----------------------------

- [designspaceLib] Don't make up bad PS names when no STAT data (#2684)

4.34.2 (released 2022-07-06)
----------------------------

- [varStore/subset] fixed KeyError exception to do with NO_VARIATION_INDEX while
  subsetting varidxes in GPOS/GDEF (a08140d).

4.34.1 (released 2022-07-06)
----------------------------

- [instancer] When optimizing HVAR/VVAR VarStore, use_NO_VARIATION_INDEX=False to avoid
  including NO_VARIATION_INDEX in AdvWidthMap, RsbMap, LsbMap mappings, which would
  push the VarIdx width to maximum (4bytes), which is not desirable. This also fixes
  a hard crash when attempting to subset a varfont after it had been partially instanced
  with use_NO_VARIATION_INDEX=True.

4.34.0 (released 2022-07-06)
----------------------------

- [instancer] Set RIBBI bits in head and OS/2 table when cutting instances and the
  subfamily nameID=2 contains strings like 'Italic' or 'Bold' (#2673).
- [otTraverse] Addded module containing methods for traversing trees of otData tables
  (#2660).
- [otTables] Made DeltaSetIndexMap TTX dump less verbose by omitting no-op entries
  (#2660).
- [colorLib.builder] Added option to disable PaintColrLayers's reuse of layers from
  LayerList (#2660).
- [varLib] Added support for merging multiple master COLRv1 tables into a variable
  COLR table (#2660, #2328). Base color glyphs of same name in different masters must have
  identical paint graph structure (incl. number of layers, palette indices, number
  of color line stops, corresponding paint formats at each level of the graph),
  but can differ in the variable fields (e.g. PaintSolid.Alpha). PaintVar* tables
  are produced when this happens and a VarStore/DeltaSetIndexMap is added to the
  variable COLR table. It is possible for non-default masters to be 'sparse', i.e.
  omit some of the color glyphs present in the default master.
- [feaLib] Let the Parser set nameIDs 1 through 6 that were previously reserved (#2675).
- [varLib.varStore] Support NO_VARIATION_INDEX in optimizer and instancer.
- [feaLib] Show all missing glyphs at once at end of parsing (#2665).
- [varLib.iup] Rewrite force-set conditions and limit DP loopback length (#2651).
  For Noto Sans, IUP time drops from 23s down to 9s, with only a slight size increase
  in the final font. This basically turns the algorithm from O(n^3) into O(n).
- [featureVars] Report about missing glyphs in substitution rules (#2654).
- [mutator/instancer] Added CLI flag to --no-recalc-timestamp (#2649).
- [SVG] Allow individual SVG documents in SVG OT table to be compressed on uncompressed,
  and remember that when roundtripping to/from ttx. The SVG.docList is now a list
  of SVGDocument namedtuple-like dataclass containing an extra ``compressed`` field,
  and no longer a bare 3-tuple (#2645).
- [designspaceLib] Check for descriptor types with hasattr() to allow custom classes
  that don't inherit the default descriptors (#2634).
- [subset] Enable sharing across subtables of extension lookups for harfbuzz packing
  (#2626). Updated how table packing falls back to fontTools from harfbuzz (#2668).
- [subset] Updated default feature tags following current Harfbuzz (#2637).
- [svgLib] Fixed regex for real number to support e.g. 1e-4 in addition to 1.0e-4.
  Support parsing negative rx, ry on arc commands (#2596, #2611).
- [subset] Fixed subsetting SinglePosFormat2 when ValueFormat=0 (#2603).

4.33.3 (released 2022-04-26)
----------------------------

- [designspaceLib] Fixed typo in ``deepcopyExceptFonts`` method, preventing font
  references to be transferred (#2600). Fixed another typo in the name of ``Range``
  dataclass's ``__post_init__`` magic method (#2597).

4.33.2 (released 2022-04-22)
----------------------------

- [otBase] Make logging less verbose when harfbuzz fails to serialize. Do not exit
  at the first failure but continue attempting to fix offset overflow error using
  the pure-python serializer even when the ``USE_HARFBUZZ_REPACKER`` option was
  explicitly set to ``True``. This is normal with fonts with relatively large
  tables, at least until hb.repack implements proper table splitting.

4.33.1 (released 2022-04-22)
----------------------------

- [otlLib] Put back the ``FONTTOOLS_GPOS_COMPACT_MODE`` environment variable to fix
  regression in ufo2ft (and thus fontmake) introduced with v4.33.0 (#2592, #2593).
  This is deprecated and will be removed one ufo2ft gets updated to use the new
  config setup.

4.33.0 (released 2022-04-21)
----------------------------

- [OS/2 / merge] Automatically recalculate ``OS/2.xAvgCharWidth`` after merging
  fonts with ``fontTools.merge`` (#2591, #2538).
- [misc/config] Added ``fontTools.misc.configTools`` module, a generic configuration
  system (#2416, #2439).
  Added ``fontTools.config`` module, a fontTools-specific configuration
  system using ``configTools`` above.
  Attached a ``Config`` object to ``TTFont``.
- [otlLib] Replaced environment variable for GPOS compression level with an
  equivalent option using the new config system.
- [designspaceLib] Incremented format version to 5.0 (#2436).
  Added discrete axes, variable fonts, STAT information, either design- or
  user-space location on instances.
  Added ``fontTools.designspaceLib.split`` module to split a designspace
  into sub-spaces that interpolate and that represent the variable fonts
  listed in the document.
  Made instance names optional and allow computing them from STAT data instead.
  Added ``fontTools.designspaceLib.statNames`` module.
  Allow instances to have the same location as a previously defined STAT label.
  Deprecated some attributes:
  ``SourceDescriptor``: ``copyLib``, ``copyInfo``, ``copyGroups``, ``copyFeatures``.
  ``InstanceDescriptor``: ``kerning``, ``info``; ``glyphs``: use rules or sparse
  sources.
  For both, ``location``: use the more explicit designLocation.
  Note: all are soft deprecations and existing code should keep working.
  Updated documentation for Python methods and the XML format.
- [varLib] Added ``build_many`` to build several variable fonts from a single
  designspace document (#2436).
  Added ``fontTools.varLib.stat`` module to build STAT tables from a designspace
  document.
- [otBase] Try to use the Harfbuzz Repacker for packing GSUB/GPOS tables when
  ``uharfbuzz`` python bindings are available (#2552). Disable it by setting the
  "fontTools.ttLib.tables.otBase:USE_HARFBUZZ_REPACKER" config option to ``False``.
  If the option is set explicitly to ``True`` but ``uharfbuzz`` can't be imported
  or fails to serialize for any reasons, an error will be raised (ImportError or
  uharfbuzz errors).
- [CFF/T2] Ensure that ``pen.closePath()`` gets called for CFF2 charstrings (#2577).
  Handle implicit CFF2 closePath within ``T2OutlineExtractor`` (#2580).

4.32.0 (released 2022-04-08)
----------------------------

- [otlLib] Disable GPOS7 optimization to work around bug in Apple CoreText.
  Always force Chaining GPOS8 for now (#2540).
- [glifLib] Added ``outputImpliedClosingLine=False`` parameter to ``Glyph.draw()``,
  to control behaviour of ``PointToSegmentPen`` (6b4e2e7).
- [varLib.interpolatable] Check for wrong contour starting point (#2571).
- [cffLib] Remove leftover ``GlobalState`` class and fix calls to ``TopDictIndex()``
  (#2569, #2570).
- [instancer] Clear ``AxisValueArray`` if it is empty after instantiating (#2563).

4.31.2 (released 2022-03-22)
----------------------------

- [varLib] fix instantiation of GPOS SinglePos values (#2555).

4.31.1 (released 2022-03-18)
----------------------------

- [subset] fix subsetting OT-SVG when glyph id attribute is on the root ``<svg>``
  element (#2553).

4.31.0 (released 2022-03-18)
----------------------------

- [ttCollection] Fixed 'ResourceWarning: unclosed file' warning (#2549).
- [varLib.merger] Handle merging SinglePos with valueformat=0 (#2550).
- [ttFont] Update glyf's glyphOrder when calling TTFont.setGlyphOrder() (#2544).
- [ttFont] Added ``ensureDecompiled`` method to load all tables irrespective
  of the ``lazy`` attribute (#2551).
- [otBase] Added ``iterSubTable`` method to iterate over BaseTable's children of
  type BaseTable; useful for traversing a tree of otTables (#2551).

4.30.0 (released 2022-03-10)
----------------------------

- [varLib] Added debug logger showing the glyph name for which ``gvar`` is built (#2542).
- [varLib.errors] Fixed undefined names in ``FoundANone`` and ``UnsupportedFormat``
  exceptions (ac4d5611).
- [otlLib.builder] Added ``windowsNames`` and ``macNames`` (bool) parameters to the
  ``buildStatTabe`` function, so that one can select whether to only add one or both
  of the two sets (#2528).
- [t1Lib] Added the ability to recreate PostScript stream (#2504).
- [name] Added ``getFirstDebugName``, ``getBest{Family,SubFamily,Full}Name`` methods (#2526).

4.29.1 (released 2022-02-01)
----------------------------

- [colorLib] Fixed rounding issue with radial gradient's start/end circles inside
  one another (#2521).
- [freetypePen] Handle rotate/skew transform when auto-computing width/height of the
  buffer; raise PenError wen missing moveTo (#2517)

4.29.0 (released 2022-01-24)
----------------------------

- [ufoLib] Fixed illegal characters and expanded reserved filenames (#2506).
- [COLRv1] Don't emit useless PaintColrLayers of lenght=1 in LayerListBuilder (#2513).
- [ttx] Removed legacy ``waitForKeyPress`` method on Windows (#2509).
- [pens] Added FreeTypePen that uses ``freetype-py`` and the pen protocol for
  rasterizating outline paths (#2494).
- [unicodedata] Updated the script direction list to Unicode 14.0 (#2484).
  Bumped unicodedata2 dependency to 14.0 (#2499).
- [psLib] Fixed type of ``fontName`` in ``suckfont`` (#2496).

4.28.5 (released 2021-12-19)
----------------------------

- [svgPathPen] Continuation of #2471: make sure all occurrences of ``str()`` are now
  replaced with user-defined ``ntos`` callable.
- [merge] Refactored code into submodules, plus several bugfixes and improvements:
  fixed duplicate-glyph-resolution GSUB-lookup generation code; use tolerance in glyph
  comparison for empty glyph's width; ignore space of default ignorable glyphs;
  downgrade duplicates-resolution missing-GSUB from assert to warn; added --drop-tables
  option (#2473, #2475, #2476).

4.28.4 (released 2021-12-15)
----------------------------

- [merge] Merge GDEF marksets in Lookups properly (#2474).
- [feaLib] Have ``fontTools feaLib`` script exit with error code when build fails (#2459)
- [svgPathPen] Added ``ntos`` option to customize number formatting (e.g. rounding) (#2471).
- [subset] Speed up subsetting of large CFF fonts (#2467).
- [otTables] Speculatively promote lookups to extension to speed up compilation. If the
  offset to lookup N is too big to fit in a ushort, the offset to lookup N+1 is going to
  be too big as well, so we promote to extension all lookups from lookup N onwards (#2465).

4.28.3 (released 2021-12-03)
----------------------------

- [subset] Fixed bug while subsetting ``COLR`` table, whereby incomplete layer records
  pointing to missing glyphs were being retained leading to ``struct.error`` upon
  compiling. Make it so that ``glyf`` glyph closure, which follows the ``COLR`` glyph
  closure, does not influence the ``COLR`` table subsetting (#2461, #2462).
- [docs] Fully document the ``cmap`` and ``glyf`` tables (#2454, #2457).
- [colorLib.unbuilder] Fixed CLI by deleting no longer existing parameter (180bb1867).

4.28.2 (released 2021-11-22)
----------------------------

- [otlLib] Remove duplicates when building coverage (#2433).
- [docs] Add interrogate configuration (#2443).
- [docs] Remove comment about missing â€œstartâ€ optional argument to ``calcChecksum`` (#2448).
- [cu2qu/cli] Adapt to the latest ufoLib2.
- [subset] Support subsetting SVG table and remove it from the list of  drop by default tables (#534).
- [subset] add ``--pretty-svg`` option to pretty print SVG table contents (#2452).
- [merge] Support merging ``CFF`` tables (CID-keyed ``CFF`` is still not supported) (#2447).
- [merge] Support ``--output-file`` (#2447).
- [docs] Split table docs into individual pages (#2444).
- [feaLib] Forbid empty classes (#2446).
- [docs] Improve documentation for ``fontTools.ttLib.ttFont`` (#2442).

4.28.1 (released 2021-11-08)
----------------------------

- [subset] Fixed AttributeError while traversing a color glyph's Paint graph when there is no
  LayerList, which is optional (#2441).

4.28.0 (released 2021-11-05)
----------------------------

- Dropped support for EOL Python 3.6, require Python 3.7 (#2417).
- [ufoLib/glifLib] Make filename-clash checks faster by using a set instead of a list (#2422).
- [subset] Don't crash if optional ClipList and LayerList are ``None`` (empty) (#2424, 2439).
- [OT-SVG] Removed support for old deprecated version 1 and embedded color palettes,
  which were never officially part of the OpenType SVG spec. Upon compile, reuse offsets
  to SVG documents that are identical (#2430).
- [feaLib] Added support for Variable Feature File syntax. This is experimental and subject
  to change until it is finalized in the Adobe FEA spec (#2432).
- [unicodedata] Update Scripts/ScriptExtensions/Blocks to UnicodeData 14.0 (#2437).

4.27.1 (released 2021-09-23)
----------------------------

- [otlLib] Fixed error when chained contextual lookup builder overflows (#2404, #2411).
- [bezierTools] Fixed two floating-point bugs: one when computing `t` for a point
  lying on an almost horizontal/vertical line; another when computing the intersection
  point between a curve and a line (#2413).

4.27.0 (released 2021-09-14)
----------------------------

- [ttLib/otTables] Cleaned up virtual GID handling: allow virtual GIDs in ``Coverage``
  and ``ClassDef`` readers; removed unused ``allowVID`` argument from ``TTFont``
  constructor, and ``requireReal`` argument in ``TTFont.getGlyphID`` method.
  Make ``TTFont.setGlyphOrder`` clear reverse glyphOrder map, and assume ``glyphOrder``
  internal attribute is never modified outside setGlyphOrder; added ``TTFont.getGlyphNameMany``
  and ``getGlyphIDMany`` (#1536, #1654, #2334, #2398).
- [py23] Dropped internal use of ``fontTools.py23`` module to fix deprecation warnings
  in client code that imports from fontTools (#2234, #2399, #2400).
- [subset] Fix subsetting COLRv1 clip boxes when font is loaded lazily (#2408).

4.26.2 (released 2021-08-09)
----------------------------

- [otTables] Added missing ``CompositeMode.PLUS`` operator (#2390).

4.26.1 (released 2021-08-03)
----------------------------

- [transform] Added ``transformVector`` and ``transformVectors`` methods to the
  ``Transform`` class. Similar to ``transformPoint`` but ignore the translation
  part (#2386).

4.26.0 (released 2021-08-03)
----------------------------

- [xmlWriter] Default to ``"\\n"`` for ``newlinestr`` instead of platform-specific
  ``os.linesep`` (#2384).
- [otData] Define COLRv1 ClipList and ClipBox (#2379).
- [removeOverlaps/instancer] Added --ignore-overlap-errors option to work around
  Skia PathOps.Simplify bug (#2382, #2363, google/fonts#3365).
- NOTE: This will be the last version to support Python 3.6. FontTools will require
  Python 3.7 or above from the next release (#2350)

4.25.2 (released 2021-07-26)
----------------------------

- [COLRv1] Various changes to sync with the latest CORLv1 draft spec. In particular:
  define COLR.VarIndexMap, remove/inline ColorIndex struct, add VarIndexBase to ``PaintVar*`` tables (#2372);
  add reduced-precicion specialized transform Paints;
  define Angle as fraction of half circle encoded as F2Dot14;
  use FWORD (int16) for all Paint center coordinates;
  change PaintTransform to have an offset to Affine2x3;
- [ttLib] when importing XML, only set sfntVersion if the font has no reader and is empty (#2376)

4.25.1 (released 2021-07-16)
----------------------------

- [ttGlyphPen] Fixed bug in ``TTGlyphPointPen``, whereby open contours (i.e. starting
  with segmentType "move") would throw ``NotImplementedError``. They are now treated
  as if they are closed, like with the ``TTGlyphPen`` (#2364, #2366).

4.25.0 (released 2021-07-05)
----------------------------

- [tfmLib] Added new library for parsing TeX Font Metric (TFM) files (#2354).
- [TupleVariation] Make shared tuples order deterministic on python < 3.7 where
  Counter (subclass of dict) doesn't remember insertion order (#2351, #2353).
- [otData] Renamed COLRv1 structs to remove 'v1' suffix and match the updated draft
  spec: 'LayerV1List' -> 'LayerList', 'BaseGlyphV1List' -> 'BaseGlyphList',
  'BaseGlyphV1Record' -> 'BaseGlyphPaintRecord' (#2346).
  Added 8 new ``PaintScale*`` tables: with/without centers, uniform vs non-uniform.
  Added ``*AroundCenter`` variants to ``PaintRotate`` and ``PaintSkew``: the default
  versions no longer have centerX/Y, but default to origin.
  ``PaintRotate``, ``PaintSkew`` and ``PaintComposite`` formats were re-numbered.
  NOTE: these are breaking changes; clients using the experimental COLRv1 API will
  have to be updated (#2348).
- [pointPens] Allow ``GuessSmoothPointPen`` to accept a tolerance. Fixed call to
  ``math.atan2`` with x/y parameters inverted. Sync the code with fontPens (#2344).
- [post] Fixed parsing ``post`` table format 2.0 when it contains extra garbage
  at the end of the stringData array (#2314).
- [subset] drop empty features unless 'size' with FeatureParams table (#2324).
- [otlLib] Added ``otlLib.optimize`` module; added GPOS compaction algorithm.
  The compaction can be run on existing fonts with ``fonttools otlLib.optimize``
  or using the snippet ``compact_gpos.py``. There's experimental support for
  compacting fonts at compilation time using an environment variable, but that
  might be removed later (#2326).

4.24.4 (released 2021-05-25)
----------------------------

- [subset/instancer] Fixed ``AttributeError`` when instantiating a VF that
  contains GPOS ValueRecords with ``Device`` tables but without the respective
  non-Device values (e.g. ``XAdvDevice`` without ``XAdvance``). When not
  explicitly set, the latter are assumed to be 0 (#2323).

4.24.3 (released 2021-05-20)
----------------------------

- [otTables] Fixed ``AttributeError`` in methods that split LigatureSubst,
  MultipleSubst and AlternateSubst subtables when an offset overflow occurs.
  The ``Format`` attribute was removed in v4.22.0 (#2319).

4.24.2 (released 2021-05-20)
----------------------------

- [ttGlyphPen] Fixed typing annotation of TTGlyphPen glyphSet parameter (#2315).
- Fixed two instances of DeprecationWarning: invalid escape sequence (#2311).

4.24.1 (released 2021-05-20)
----------------------------

- [subset] Fixed AttributeError when SinglePos subtable has None Value (ValueFormat 0)
  (#2312, #2313).

4.24.0 (released 2021-05-17)
----------------------------

- [pens] Add ``ttGlyphPen.TTGlyphPointPen`` similar to ``TTGlyphPen`` (#2205).

4.23.1 (released 2021-05-14)
----------------------------

- [subset] Fix ``KeyError`` after subsetting ``COLR`` table that initially contains
  both v0 and v1 color glyphs when the subset only requested v1 glyphs; we were
  not pruning the v0 portion of the table (#2308).
- [colorLib] Set ``LayerV1List`` attribute to ``None`` when empty, it's optional
  in CORLv1 (#2308).

4.23.0 (released 2021-05-13)
----------------------------

- [designspaceLib] Allow to use ``\\\\UNC`` absolute paths on Windows (#2299, #2306).
- [varLib.merger] Fixed bug where ``VarLibMergeError`` was raised with incorrect
  parameters (#2300).
- [feaLib] Allow substituting a glyph class with ``NULL`` to delete multiple glyphs
  (#2303).
- [glyf] Fixed ``NameError`` exception in ``getPhantomPoints`` (#2295, #2305).
- [removeOverlaps] Retry pathops.simplify after rounding path coordinates to integers
  if it fails the first time using floats, to work around a rare and hard to debug
  Skia bug (#2288).
- [varLib] Added support for building, reading, writing and optimizing 32-bit
  ``ItemVariationStore`` as used in COLRv1 table (#2285).
- [otBase/otConverters] Add array readers/writers for int types (#2285).
- [feaLib] Allow more than one lookahead glyph/class in contextual positioning with
  "value at end" (#2293, #2294).
- [COLRv1] Default varIdx should be 0xFFFFFFFF (#2297, #2298).
- [pens] Make RecordingPointPen actually pass on identifiers; replace asserts with
  explicit ``PenError`` exception (#2284).
- [mutator] Round lsb for CF2 fonts as well (#2286).

4.22.1 (released 2021-04-26)
----------------------------

- [feaLib] Skip references to named lookups if the lookup block definition
  is empty, similarly to makeotf. This also fixes an ``AttributeError`` while
  generating ``aalt`` feature (#2276, #2277).
- [subset] Fixed bug with ``--no-hinting`` implementation for Device tables (#2272,
  #2275). The previous code was alwyas dropping Device tables if no-hinting was
  requested, but some Device tables (DeltaFormat=0x8000) are also used to encode
  variation indices and need to be retained.
- [otBase] Fixed bug in getting the ValueRecordSize when decompiling ``MVAR``
  table with ``lazy=True`` (#2273, #2274).
- [varLib/glyf/gvar] Optimized and simplified ``GlyphCoordinates`` and
  ``TupleVariation`` classes, use ``bytearray`` where possible, refactored
  phantom-points calculations. We measured about 30% speedup in total time
  of loading master ttfs, building gvar, and saving (#2261, #2266).
- [subset] Fixed ``AssertionError`` while pruning unused CPAL palettes when
  ``0xFFFF`` is present (#2257, #2259).

4.22.0 (released 2021-04-01)
----------------------------

- [ttLib] Remove .Format from Coverage, ClassDef, SingleSubst, LigatureSubst,
  AlternateSubst, MultipleSubst (#2238).
  ATTENTION: This will change your TTX dumps!
- [misc.arrayTools] move Vector to its own submodule, and rewrite as a tuple
  subclass (#2201).
- [docs] Added a terminology section for varLib (#2209).
- [varLib] Move rounding to VariationModel, to avoid error accumulation from
  multiple deltas (#2214)
- [varLib] Explain merge errors in more human-friendly terms (#2223, #2226)
- [otlLib] Correct some documentation (#2225)
- [varLib/otlLib] Allow merging into VariationFont without first saving GPOS
  PairPos2 (#2229)
- [subset] Improve PairPosFormat2 subsetting (#2221)
- [ttLib] TTFont.save: create file on disk as late as possible (#2253)
- [cffLib] Add missing CFF2 dict operators LanguageGroup and ExpansionFactor
  (#2249)
  ATTENTION: This will change your TTX dumps!

4.21.1 (released 2021-02-26)
----------------------------

- [pens] Reverted breaking change that turned ``AbstractPen`` and ``AbstractPointPen``
  into abstract base classes (#2164, #2198).

4.21.0 (released 2021-02-26)
----------------------------

- [feaLib] Indent anchor statements in ``asFea()`` to make them more legible and
  diff-able (#2193).
- [pens] Turn ``AbstractPen`` and ``AbstractPointPen`` into abstract base classes
  (#2164).
- [feaLib] Added support for parsing and building ``STAT`` table from AFDKO feature
  files (#2039).
- [instancer] Added option to update name table of generated instance using ``STAT``
  table's axis values (#2189).
- [bezierTools] Added functions to compute bezier point-at-time, as well as line-line,
  curve-line and curve-curve intersections (#2192).

4.20.0 (released 2021-02-15)
----------------------------

- [COLRv1] Added ``unbuildColrV1`` to deconstruct COLRv1 otTables to raw json-able
  data structure; it does the reverse of ``buildColrV1`` (#2171).
- [feaLib] Allow ``sub X by NULL`` sequence to delete a glyph (#2170).
- [arrayTools] Fixed ``Vector`` division (#2173).
- [COLRv1] Define new ``PaintSweepGradient`` (#2172).
- [otTables] Moved ``Paint.Format`` enum class outside of ``Paint`` class definition,
  now named ``PaintFormat``. It was clashing with paint instance ``Format`` attribute
  and thus was breaking lazy load of COLR table which relies on magic ``__getattr__``
  (#2175).
- [COLRv1] Replace hand-coded builder functions with otData-driven dynamic
  implementation (#2181).
- [COLRv1] Define additional static (non-variable) Paint formats (#2181).
- [subset] Added support for subsetting COLR v1 and CPAL tables (#2174, #2177).
- [fontBuilder] Allow ``setupFvar`` to optionally take ``designspaceLib.AxisDescriptor``
  objects. Added new ``setupAvar`` method. Support localised names for axes and
  named instances (#2185).

4.19.1 (released 2021-01-28)
----------------------------

- [woff2] An initial off-curve point with an overlap flag now stays an off-curve
  point after compression.

4.19.0 (released 2021-01-25)
----------------------------

- [codecs] Handle ``errors`` parameter different from 'strict' for the custom
  extended mac encodings (#2137, #2132).
- [featureVars] Raise better error message when a script is missing the required
  default language system (#2154).
- [COLRv1] Avoid abrupt change caused by rounding ``PaintRadialGradient.c0`` when
  the start circle almost touches the end circle's perimeter (#2148).
- [COLRv1] Support building unlimited lists of paints as 255-ary trees of
  ``PaintColrLayers`` tables (#2153).
- [subset] Prune redundant format-12 cmap subtables when all non-BMP characters
  are dropped (#2146).
- [basePen] Raise ``MissingComponentError`` instead of bare ``KeyError`` when a
  referenced component is missing (#2145).

4.18.2 (released 2020-12-16)
----------------------------

- [COLRv1] Implemented ``PaintTranslate`` paint format (#2129).
- [varLib.cff] Fixed unbound local variable error (#1787).
- [otlLib] Don't crash when creating OpenType class definitions if some glyphs
  occur more than once (#2125).

4.18.1 (released 2020-12-09)
----------------------------

- [colorLib] Speed optimization for ``LayerV1ListBuilder`` (#2119).
- [mutator] Fixed missing tab in ``interpolate_cff2_metrics`` (0957dc7a).

4.18.0 (released 2020-12-04)
----------------------------

- [COLRv1] Update to latest draft: added ``PaintRotate`` and ``PaintSkew`` (#2118).
- [woff2] Support new ``brotlicffi`` bindings for PyPy (#2117).
- [glifLib] Added ``expectContentsFile`` parameter to ``GlyphSet``, for use when
  reading existing UFOs, to comply with the specification stating that a
  ``contents.plist`` file must exist in a glyph set (#2114).
- [subset] Allow ``LangSys`` tags in ``--layout-scripts`` option (#2112). For example:
  ``--layout-scripts=arab.dflt,arab.URD,latn``; this will keep ``DefaultLangSys``
  and ``URD`` language for ``arab`` script, and all languages for ``latn`` script.
- [varLib.interpolatable] Allow UFOs to be checked; report open paths, non existant
  glyphs; add a ``--json`` option to produce a machine-readable list of
  incompatibilities
- [pens] Added ``QuartzPen`` to create ``CGPath`` from glyph outlines on macOS.
  Requires pyobjc (#2107).
- [feaLib] You can export ``FONTTOOLS_LOOKUP_DEBUGGING=1`` to enable feature file
  debugging info stored in ``Debg`` table (#2106).
- [otlLib] Build more efficient format 1 and format 2 contextual lookups whenever
  possible (#2101).

4.17.1 (released 2020-11-16)
----------------------------

- [colorLib] Fixed regression in 4.17.0 when building COLR v0 table; when color
  layers are stored in UFO lib plist, we can't distinguish tuples from lists so
  we need to accept either types (e5439eb9, googlefonts/ufo2ft/issues#426).

4.17.0 (released 2020-11-12)
----------------------------

- [colorLib/otData] Updated to latest draft ``COLR`` v1 spec (#2092).
- [svgLib] Fixed parsing error when arc commands' boolean flags are not separated
  by space or comma (#2094).
- [varLib] Interpret empty non-default glyphs as 'missing', if the default glyph is
  not empty (#2082).
- [feaLib.builder] Only stash lookup location for ``Debg`` if ``Builder.buildLookups_``
  has cooperated (#2065, #2067).
- [varLib] Fixed bug in VarStore optimizer (#2073, #2083).
- [varLib] Add designspace lib key for custom feavar feature tag (#2080).
- Add HashPointPen adapted from psautohint. With this pen, a hash value of a glyph
  can be computed, which can later be used to detect glyph changes (#2005).

4.16.1 (released 2020-10-05)
----------------------------

- [varLib.instancer] Fixed ``TypeError`` exception when instantiating a VF with
  a GSUB table 1.1 in which ``FeatureVariations`` attribute is present but set to
  ``None`` -- indicating that optional ``FeatureVariations`` is missing (#2077).
- [glifLib] Make ``x`` and ``y`` attributes of the ``point`` element required
  even when validation is turned off, and raise a meaningful ``GlifLibError``
  message when that happens (#2075).

4.16.0 (released 2020-09-30)
----------------------------

- [removeOverlaps] Added new module and ``removeOverlaps`` function that merges
  overlapping contours and components in TrueType glyphs. It requires the
  `skia-pathops <https://github.com/fonttools/skia-pathops>`__ module.
  Note that removing overlaps invalidates the TrueType hinting (#2068).
- [varLib.instancer] Added ``--remove-overlaps`` command-line option.
  The ``overlap`` option in ``instantiateVariableFont`` now takes an ``OverlapMode``
  enum: 0: KEEP_AND_DONT_SET_FLAGS, 1: KEEP_AND_SET_FLAGS (default), and 2: REMOVE.
  The latter is equivalent to calling ``removeOverlaps`` on the generated static
  instance. The option continues to accept ``bool`` value for backward compatibility.


4.15.0 (released 2020-09-21)
----------------------------

- [plistlib] Added typing annotations to plistlib module. Set up mypy static
  typechecker to run automatically on CI (#2061).
- [ttLib] Implement private ``Debg`` table, a reverse-DNS namespaced JSON dict.
- [feaLib] Optionally add an entry into the ``Debg`` table with the original
  lookup name (if any), feature name / script / language combination (if any),
  and original source filename and line location. Annotate the ttx output for
  a lookup with the information from the Debg table (#2052).
- [sfnt] Disabled checksum checking by default in ``SFNTReader`` (#2058).
- [Docs] Document ``mtiLib`` module (#2027).
- [varLib.interpolatable] Added checks for contour node count and operation type
  of each node (#2054).
- [ttLib] Added API to register custom table packer/unpacker classes (#2055).

4.14.0 (released 2020-08-19)
----------------------------

- [feaLib] Allow anonymous classes in LookupFlags definitions (#2037).
- [Docs] Better document DesignSpace rules processing order (#2041).
- [ttLib] Fixed 21-year old bug in ``maxp.maxComponentDepth`` calculation (#2044,
  #2045).
- [varLib.models] Fixed misspelled argument name in CLI entry point (81d0042a).
- [subset] When subsetting GSUB v1.1, fixed TypeError by checking whether the
  optional FeatureVariations table is present (e63ecc5b).
- [Snippets] Added snippet to show how to decompose glyphs in a TTF (#2030).
- [otlLib] Generate GSUB type 5 and GPOS type 7 contextual lookups where appropriate
  (#2016).

4.13.0 (released 2020-07-10)
----------------------------

- [feaLib/otlLib] Moved lookup subtable builders from feaLib to otlLib; refactored
  some common code (#2004, #2007).
- [docs] Document otlLib module (#2009).
- [glifLib] Fixed bug with some UFO .glif filenames clashing on case-insensitive
  filesystems (#2001, #2002).
- [colorLib] Updated COLRv1 implementation following changes in the draft spec:
  (#2008, googlefonts/colr-gradients-spec#24).

4.12.1 (released 2020-06-16)
----------------------------

- [_n_a_m_e] Fixed error in ``addMultilingualName`` with one-character names.
  Only attempt to recovered malformed UTF-16 data from a ``bytes`` string,
  not from unicode ``str`` (#1997, #1998).

4.12.0 (released 2020-06-09)
----------------------------

- [otlLib/varLib] Ensure that the ``AxisNameID`` in the ``STAT`` and ``fvar``
  tables is grater than 255 as per OpenType spec (#1985, #1986).
- [docs] Document more modules in ``fontTools.misc`` package: ``filenames``,
  ``fixedTools``, ``intTools``, ``loggingTools``, ``macCreatorType``, ``macRes``,
  ``plistlib`` (#1981).
- [OS/2] Don't calculate whole sets of unicode codepoints, use faster and more memory
  efficient ranges and bisect lookups (#1984).
- [voltLib] Support writing back abstract syntax tree as VOLT data (#1983).
- [voltLib] Accept DO_NOT_TOUCH_CMAP keyword (#1987).
- [subset/merge] Fixed a namespace clash involving a private helper class (#1955).

4.11.0 (released 2020-05-28)
----------------------------

- [feaLib] Introduced ``includeDir`` parameter on Parser and IncludingLexer to
  explicitly specify the directory to search when ``include()`` statements are
  encountered (#1973).
- [ufoLib] Silently delete duplicate glyphs within the same kerning group when reading
  groups (#1970).
- [ttLib] Set version of COLR table when decompiling COLRv1 (commit 9d8a7e2).

4.10.2 (released 2020-05-20)
----------------------------

- [sfnt] Fixed ``NameError: SimpleNamespace`` while reading TTC header. The regression
  was introduced with 4.10.1 after removing ``py23`` star import.

4.10.1 (released 2020-05-19)
----------------------------

- [sfnt] Make ``SFNTReader`` pickleable even when TTFont is loaded with lazy=True
  option and thus keeps a reference to an external file (#1962, #1967).
- [feaLib.ast] Restore backward compatibility (broken in 4.10 with #1905) for
  ``ChainContextPosStatement`` and ``ChainContextSubstStatement`` classes.
  Make them accept either list of lookups or list of lists of lookups (#1961).
- [docs] Document some modules in ``fontTools.misc`` package: ``arrayTools``,
  ``bezierTools`` ``cliTools`` and ``eexec`` (#1956).
- [ttLib._n_a_m_e] Fixed ``findMultilingualName()`` when name record's ``string`` is
  encoded as bytes sequence (#1963).

4.10.0 (released 2020-05-15)
----------------------------

- [varLib] Allow feature variations to be active across the entire space (#1957).
- [ufoLib] Added support for ``formatVersionMinor`` in UFO's ``fontinfo.plist`` and for
  ``formatMinor`` attribute in GLIF file as discussed in unified-font-object/ufo-spec#78.
  No changes in reading or writing UFOs until an upcoming (non-0) minor update of the
  UFO specification is published (#1786).
- [merge] Fixed merging fonts with different versions of ``OS/2`` table (#1865, #1952).
- [subset] Fixed ``AttributeError`` while subsetting ``ContextSubst`` and ``ContextPos``
  Format 3 subtable (#1879, #1944).
- [ttLib.table._m_e_t_a] if data happens to be ascii, emit comment in TTX (#1938).
- [feaLib] Support multiple lookups per glyph position (#1905).
- [psCharStrings] Use inheritance to avoid repeated code in initializer (#1932).
- [Doc] Improved documentation for the following modules: ``afmLib`` (#1933), ``agl``
  (#1934), ``cffLib`` (#1935), ``cu2qu`` (#1937), ``encodings`` (#1940), ``feaLib``
  (#1941), ``merge`` (#1949).
- [Doc] Split off developer-centric info to new page, making front page of docs more
  user-focused. List all utilities and sub-modules with brief descriptions.
  Make README more concise and focused (#1914).
- [otlLib] Add function to build STAT table from high-level description (#1926).
- [ttLib._n_a_m_e] Add ``findMultilingualName()`` method (#1921).
- [unicodedata] Update ``RTL_SCRIPTS`` for Unicode 13.0 (#1925).
- [gvar] Sort ``gvar`` XML output by glyph name, not glyph order (#1907, #1908).
- [Doc] Added help options to ``fonttools`` command line tool (#1913, #1920).
  Ensure all fonttools CLI tools have help documentation (#1948).
- [ufoLib] Only write fontinfo.plist when there actually is content (#1911).

4.9.0 (released 2020-04-29)
---------------------------

- [subset] Fixed subsetting of FeatureVariations table. The subsetter no longer drops
  FeatureVariationRecords that have empty substitutions as that will keep the search
  going and thus change the logic. It will only drop empty records that occur at the
  end of the FeatureVariationRecords array (#1881).
- [subset] Remove FeatureVariations table and downgrade GSUB/GPOS to version 0x10000
  when FeatureVariations contain no FeatureVariationRecords after subsetting (#1903).
- [agl] Add support for legacy Adobe Glyph List of glyph names in ``fontTools.agl``
  (#1895).
- [feaLib] Ignore superfluous script statements (#1883).
- [feaLib] Hide traceback by default on ``fonttools feaLib`` command line.
  Use ``--traceback`` option to show (#1898).
- [feaLib] Check lookup index in chaining sub/pos lookups and print better error
  message (#1896, #1897).
- [feaLib] Fix building chained alt substitutions (#1902).
- [Doc] Included all fontTools modules in the sphinx-generated documentation, and
  published it to ReadTheDocs for continuous documentation of the fontTools project
  (#1333). Check it out at https://fonttools.readthedocs.io/. Thanks to Chris Simpkins!
- [transform] The ``Transform`` class is now subclass of ``typing.NamedTuple``. No
  change in functionality (#1904).


4.8.1 (released 2020-04-17)
---------------------------

- [feaLib] Fixed ``AttributeError: 'NoneType' has no attribute 'getAlternateGlyphs'``
  when ``aalt`` feature references a chain contextual substitution lookup
  (googlefonts/fontmake#648, #1878).

4.8.0 (released 2020-04-16)
---------------------------

- [feaLib] If Parser is initialized without a ``glyphNames`` parameter, it cannot
  distinguish between a glyph name containing an hyphen, or a range of glyph names;
  instead of raising an error, it now interprets them as literal glyph names, while
  also outputting a logging warning to alert user about the ambiguity (#1768, #1870).
- [feaLib] When serializing AST to string, emit spaces around hyphens that denote
  ranges. Also, fixed an issue with CID ranges when round-tripping AST->string->AST
  (#1872).
- [Snippets/otf2ttf] In otf2ttf.py script update LSB in hmtx to match xMin (#1873).
- [colorLib] Added experimental support for building ``COLR`` v1 tables as per
  the `colr-gradients-spec <https://github.com/googlefonts/colr-gradients-spec/blob/main/colr-gradients-spec.md>`__
  draft proposal. **NOTE**: both the API and the XML dump of ``COLR`` v1 are
  susceptible to change while the proposal is being discussed and formalized (#1822).

4.7.0 (released 2020-04-03)
---------------------------

- [cu2qu] Added ``fontTools.cu2qu`` package, imported from the original
  `cu2qu <https://github.com/googlefonts/cu2qu>`__ project. The ``cu2qu.pens`` module
  was moved to ``fontTools.pens.cu2quPen``. The optional cu2qu extension module
  can be compiled by installing `Cython <https://cython.org/>`__ before installing
  fonttools from source (i.e. git repo or sdist tarball). The wheel package that
  is published on PyPI (i.e. the one ``pip`` downloads, unless ``--no-binary``
  option is used), will continue to be pure-Python for now (#1868).

4.6.0 (released 2020-03-24)
---------------------------

- [varLib] Added support for building variable ``BASE`` table version 1.1 (#1858).
- [CPAL] Added ``fromRGBA`` method to ``Color`` class (#1861).


4.5.0 (released 2020-03-20)
---------------------------

- [designspaceLib] Added ``add{Axis,Source,Instance,Rule}Descriptor`` methods to
  ``DesignSpaceDocument`` class, to initialize new descriptor objects using keyword
  arguments, and at the same time append them to the current document (#1860).
- [unicodedata] Update to Unicode 13.0 (#1859).

4.4.3 (released 2020-03-13)
---------------------------

- [varLib] Always build ``gvar`` table for TrueType-flavored Variable Fonts,
  even if it contains no variation data. The table is required according to
  the OpenType spec (#1855, #1857).

4.4.2 (released 2020-03-12)
---------------------------

- [ttx] Annotate ``LookupFlag`` in XML dump with comment explaining what bits
  are set and what they mean (#1850).
- [feaLib] Added more descriptive message to ``IncludedFeaNotFound`` error (#1842).

4.4.1 (released 2020-02-26)
---------------------------

- [woff2] Skip normalizing ``glyf`` and ``loca`` tables if these are missing from
  a font (e.g. in NotoColorEmoji using ``CBDT/CBLC`` tables).
- [timeTools] Use non-localized date parsing in ``timestampFromString``, to fix
  error when non-English ``LC_TIME`` locale is set (#1838, #1839).
- [fontBuilder] Make sure the CFF table generated by fontBuilder can be used by varLib
  without having to compile and decompile the table first. This was breaking in
  converting the CFF table to CFF2 due to some unset attributes (#1836).

4.4.0 (released 2020-02-18)
---------------------------

- [colorLib] Added ``fontTools.colorLib.builder`` module, initially with ``buildCOLR``
  and ``buildCPAL`` public functions. More color font formats will follow (#1827).
- [fontBuilder] Added ``setupCOLR`` and ``setupCPAL`` methods (#1826).
- [ttGlyphPen] Quantize ``GlyphComponent.transform`` floats to ``F2Dot14`` to fix
  round-trip issue when computing bounding boxes of transformed components (#1830).
- [glyf] If a component uses reference points (``firstPt`` and ``secondPt``) for
  alignment (instead of X and Y offsets), compute the effective translation offset
  *after* having applied any transform (#1831).
- [glyf] When all glyphs have zero contours, compile ``glyf`` table data as a single
  null byte in order to pass validation by OTS and Windows (#1829).
- [feaLib] Parsing feature code now ensures that referenced glyph names are part of
  the known glyph set, unless a glyph set was not provided.
- [varLib] When filling in the default axis value for a missing location of a source or
  instance, correctly map the value forward.
- [varLib] The avar table can now contain mapping output values that are greater than
  OR EQUAL to the preceeding value, as the avar specification allows this.
- [varLib] The errors of the module are now ordered hierarchically below VarLibError.
  See #1821.

4.3.0 (released 2020-02-03)
---------------------------

- [EBLC/CBLC] Fixed incorrect padding length calculation for Format 3 IndexSubTable
  (#1817, #1818).
- [varLib] Fixed error when merging OTL tables and TTFonts were loaded as ``lazy=True``
  (#1808, #1809).
- [varLib] Allow to use master fonts containing ``CFF2`` table when building VF (#1816).
- [ttLib] Make ``recalcBBoxes`` option work also with ``CFF2`` table (#1816).
- [feaLib] Don't reset ``lookupflag`` in lookups defined inside feature blocks.
  They will now inherit the current ``lookupflag`` of the feature. This is what
  Adobe ``makeotf`` also does in this case (#1815).
- [feaLib] Fixed bug with mixed single/multiple substitutions. If a single substitution
  involved a glyph class, we were incorrectly using only the first glyph in the class
  (#1814).

4.2.5 (released 2020-01-29)
---------------------------

- [feaLib] Do not fail on duplicate multiple substitutions, only warn (#1811).
- [subset] Optimize SinglePos subtables to Format 1 if all ValueRecords are the same
  (#1802).

4.2.4 (released 2020-01-09)
---------------------------

- [unicodedata] Update RTL_SCRIPTS for Unicode 11 and 12.

4.2.3 (released 2020-01-07)
---------------------------

- [otTables] Fixed bug when splitting `MarkBasePos` subtables as offsets overflow.
  The mark class values in the split subtable were not being updated, leading to
  invalid mark-base attachments (#1797, googlefonts/noto-source#145).
- [feaLib] Only log a warning instead of error when features contain duplicate
  substitutions (#1767).
- [glifLib] Strip XML comments when parsing with lxml (#1784, #1785).

4.2.2 (released 2019-12-12)
---------------------------

- [subset] Fixed issue with subsetting FeatureVariations table when the index
  of features changes as features get dropped. The feature index need to be
  remapped to point to index of the remaining features (#1777, #1782).
- [fontBuilder] Added `addFeatureVariations` method to `FontBuilder` class. This
  is a shorthand for calling `featureVars.addFeatureVariations` on the builder's
  TTFont object (#1781).
- [glyf] Fixed the flags bug in glyph.drawPoints() like we did for glyph.draw()
  (#1771, #1774).

4.2.1 (released 2019-12-06)
---------------------------

- [glyf] Use the ``flagOnCurve`` bit mask in ``glyph.draw()``, so that we ignore
  the ``overlap`` flag that may be set when instantiating variable fonts (#1771).

4.2.0 (released 2019-11-28)
---------------------------

- [pens] Added the following pens:

  * ``roundingPen.RoundingPen``: filter pen that rounds coordinates and components'
    offsets to integer;
  * ``roundingPen.RoundingPointPen``: like the above, but using PointPen protocol.
  * ``filterPen.FilterPointPen``: base class for filter point pens;
  * ``transformPen.TransformPointPen``: filter point pen to apply affine transform;
  * ``recordingPen.RecordingPointPen``: records and replays point-pen commands.

- [ttGlyphPen] Always round float coordinates and component offsets to integers
  (#1763).
- [ufoLib] When converting kerning groups from UFO2 to UFO3, avoid confusing
  groups with the same name as one of the glyphs (#1761, #1762,
  unified-font-object/ufo-spec#98).

4.1.0 (released 2019-11-18)
---------------------------

- [instancer] Implemented restricting axis ranges (level 3 partial instancing).
  You can now pass ``{axis_tag: (min, max)}`` tuples as input to the
  ``instantiateVariableFont`` function. Note that changing the default axis
  position is not supported yet. The command-line script also accepts axis ranges
  in the form of colon-separated float values, e.g. ``wght=400:700`` (#1753, #1537).
- [instancer] Never drop STAT ``DesignAxis`` records, but only prune out-of-range
  ``AxisValue`` records.
- [otBase/otTables] Enforce that VarStore.RegionAxisCount == fvar.axisCount, even
  when regions list is empty to appease OTS < v8.0 (#1752).
- [designspaceLib] Defined new ``processing`` attribute for ``<rules>`` element,
  with values "first" or "last", plus other editorial changes to DesignSpace
  specification. Bumped format version to 4.1 (#1750).
- [varLib] Improved error message when masters' glyph orders do not match (#1758,
  #1759).
- [featureVars] Allow to specify custom feature tag in ``addFeatureVariations``;
  allow said feature to already exist, in which case we append new lookup indices
  to existing features. Implemented ``<rules>`` attribute ``processing`` according to
  DesignSpace specification update in #1750. Depending on this flag, we generate
  either an 'rvrn' (always processed first) or a 'rclt' feature (follows lookup order,
  therefore last) (#1747, #1625, #1371).
- [ttCollection] Added support for context manager auto-closing via ``with`` statement
  like with ``TTFont`` (#1751).
- [unicodedata] Require unicodedata2 >= 12.1.0.
- [py2.py3] Removed yet more PY2 vestiges (#1743).
- [_n_a_m_e] Fixed issue when comparing NameRecords with different string types (#1742).
- [fixedTools] Changed ``fixedToFloat`` to not do any rounding but simply return
  ``value / (1 << precisionBits)``. Added ``floatToFixedToStr`` and
  ``strToFixedToFloat`` functions to be used when loading from or dumping to XML.
  Fixed values (e.g. fvar axes and instance coordinates, avar mappings, etc.) are
  are now stored as un-rounded decimal floats upon decompiling (#1740, #737).
- [feaLib] Fixed handling of multiple ``LigatureCaret`` statements for the same glyph.
  Only the first rule per glyph is used, additional ones are ignored (#1733).

4.0.2 (released 2019-09-26)
---------------------------

- [voltLib] Added support for ``ALL`` and ``NONE`` in ``PROCESS_MARKS`` (#1732).
- [Silf] Fixed issue in ``Silf`` table compilation and decompilation regarding str vs
  bytes in python3 (#1728).
- [merge] Handle duplicate glyph names better: instead of appending font index to
  all glyph names, use similar code like we use in ``post`` and ``CFF`` tables (#1729).

4.0.1 (released 2019-09-11)
---------------------------

- [otTables] Support fixing offset overflows in ``MultipleSubst`` lookup subtables
  (#1706).
- [subset] Prune empty strikes in ``EBDT`` and ``CBDT`` table data (#1698, #1633).
- [pens] Fixed issue in ``PointToSegmentPen`` when last point of closed contour has
  same coordinates as the starting point and was incorrectly dropped (#1720).
- [Graphite] Fixed ``Sill`` table output to pass OTS (#1705).
- [name] Added ``removeNames`` method to ``table__n_a_m_e`` class (#1719).
- [ttLib] Added aliases for renamed entries ``ascender`` and ``descender`` in
  ``hhea`` table (#1715).

4.0.0 (released 2019-08-22)
---------------------------

- NOTE: The v4.x version series only supports Python 3.6 or greater. You can keep
  using fonttools 3.x if you need support for Python 2.
- [py23] Removed all the python2-only code since it is no longer reachable, thus
  unused; only the Python3 symbols were kept, but these are no-op. The module is now
  DEPRECATED and will removed in the future.
- [ttLib] Fixed UnboundLocalError for empty loca/glyph tables (#1680). Also, allow
  the glyf table to be incomplete when dumping to XML (#1681).
- [varLib.models] Fixed KeyError while sorting masters and there are no on-axis for
  a given axis (38a8eb0e).
- [cffLib] Make sure glyph names are unique (#1699).
- [feaLib] Fix feature parser to correctly handle octal numbers (#1700).

3.44.0 (released 2019-08-02)
----------------------------

- NOTE: This is the last scheduled release to support Python 2.7. The upcoming fonttools
  v4.x series is going to require Python 3.6 or greater.
- [varLib] Added new ``varLib.instancer`` module for partially instantiating variable
  fonts. This extends (and will eventually replace) ``varLib.mutator`` module, as
  it allows to create not just full static instances from a variable font, but also
  "partial" or "less variable" fonts where some of the axes are dropped or
  instantiated at a particular value.
  Also available from the command-line as `fonttools varLib.instancer --help`
  (#1537, #1628).
- [cffLib] Added support for ``FDSelect`` format 4 (#1677).
- [subset] Added support for subsetting ``sbix`` (Apple bitmap color font) table.
- [t1Lib] Fixed issue parsing ``eexec`` section in Type1 fonts when whitespace
  characters are interspersed among the trailing zeros (#1676).
- [cffLib.specializer] Fixed bug in ``programToCommands`` with CFF2 charstrings (#1669).

3.43.2 (released 2019-07-10)
----------------------------

- [featureVars] Fixed region-merging code on python3 (#1659).
- [varLib.cff] Fixed merging of sparse PrivateDict items (#1653).

3.43.1 (released 2019-06-19)
----------------------------

- [subset] Fixed regression when passing ``--flavor=woff2`` option with an input font
  that was already compressed as WOFF 1.0 (#1650).

3.43.0 (released 2019-06-18)
----------------------------

- [woff2] Added support for compressing/decompressing WOFF2 fonts with non-transformed
  ``glyf`` and ``loca`` tables, as well as with transformed ``hmtx`` table.
  Removed ``Snippets/woff2_compress.py`` and ``Snippets/woff2_decompress.py`` scripts,
  and replaced them with a new console entry point ``fonttools ttLib.woff2``
  that provides two sub-commands ``compress`` and ``decompress``.
- [varLib.cff] Fixed bug when merging CFF2 ``PrivateDicts``. The ``PrivateDict``
  data from the first region font was incorrecty used for all subsequent fonts.
  The bug would only affect variable CFF2 fonts with hinting (#1643, #1644).
  Also, fixed a merging bug when VF masters have no blends or marking glyphs (#1632,
  #1642).
- [loggingTools] Removed unused backport of ``LastResortLogger`` class.
- [subset] Gracefully handle partial MATH table (#1635).
- [featureVars] Avoid duplicate references to ``rvrn`` feature record in
  ``DefaultLangSys`` tables when calling ``addFeatureVariations`` on a font that
  does not already have a ``GSUB`` table (aa8a5bc6).
- [varLib] Fixed merging of class-based kerning. Before, the process could introduce
  rogue kerning values and variations for random classes against class zero (everything
  not otherwise classed).
- [varLib] Fixed merging GPOS tables from master fonts with different number of
  ``SinglePos`` subtables (#1621, #1641).
- [unicodedata] Updated Blocks, Scripts and ScriptExtensions to Unicode 12.1.

3.42.0 (released 2019-05-28)
----------------------------

- [OS/2] Fixed sign of ``fsType``: it should be ``uint16``, not ``int16`` (#1619).
- [subset] Skip out-of-range class values in mark attachment (#1478).
- [fontBuilder] Add an empty ``DSIG`` table with ``setupDummyDSIG`` method (#1621).
- [varLib.merger] Fixed bug whereby ``GDEF.GlyphClassDef`` were being dropped
  when generating instance via ``varLib.mutator`` (#1614).
- [varLib] Added command-line options ``-v`` and ``-q`` to configure logging (#1613).
- [subset] Update font extents in head table (#1612).
- [subset] Make --retain-gids truncate empty glyphs after the last non-empty glyph
  (#1611).
- [requirements] Updated ``unicodedata2`` backport for Unicode 12.0.

3.41.2 (released 2019-05-13)
----------------------------

- [cffLib] Fixed issue when importing a ``CFF2`` variable font from XML, whereby
  the VarStore state was not propagated to PrivateDict (#1598).
- [varLib] Don't drop ``post`` glyph names when building CFF2 variable font (#1609).


3.41.1 (released 2019-05-13)
----------------------------

- [designspaceLib] Added ``loadSourceFonts`` method to load source fonts using
  custom opener function (#1606).
- [head] Round font bounding box coordinates to integers to fix compile error
  if CFF font has float coordinates (#1604, #1605).
- [feaLib] Don't write ``None`` in ``ast.ValueRecord.asFea()`` (#1599).
- [subset] Fixed issue ``AssertionError`` when using ``--desubroutinize`` option
  (#1590, #1594).
- [graphite] Fixed bug in ``Silf`` table's ``decompile`` method unmasked by
  previous typo fix (#1597). Decode languange code as UTF-8 in ``Sill`` table's
  ``decompile`` method (#1600).

3.41.0 (released 2019-04-29)
----------------------------

- [varLib/cffLib] Added support for building ``CFF2`` variable font from sparse
  masters, or masters with more than one model (multiple ``VarStore.VarData``).
  In ``cffLib.specializer``, added support for ``CFF2`` CharStrings with
  ``blend`` operators (#1547, #1591).
- [subset] Fixed subsetting ``HVAR`` and ``VVAR`` with ``--retain-gids`` option,
  and when advances mapping is null while sidebearings mappings are non-null
  (#1587, #1588).
- Added ``otlLib.maxContextCalc`` module to compute ``OS/2.usMaxContext`` value.
  Calculate it automatically when compiling features with feaLib. Added option
  ``--recalc-max-context`` to ``subset`` module (#1582).
- [otBase/otTables] Fixed ``AttributeError`` on missing OT table fields after
  importing font from TTX (#1584).
- [graphite] Fixed typo ``Silf`` table's ``decompile`` method (#1586).
- [otlLib] Better compress ``GPOS`` SinglePos (LookupType 1) subtables (#1539).

3.40.0 (released 2019-04-08)
----------------------------

- [subset] Fixed error while subsetting ``VVAR`` with ``--retain-gids``
  option (#1552).
- [designspaceLib] Use up-to-date default location in ``findDefault`` method
  (#1554).
- [voltLib] Allow passing file-like object to Parser.
- [arrayTools/glyf] ``calcIntBounds`` (used to compute bounding boxes of glyf
  table's glyphs) now uses ``otRound`` instead of ``round3`` (#1566).
- [svgLib] Added support for converting more SVG shapes to path ``d`` strings
  (ellipse, line, polyline), as well as support for ``transform`` attributes.
  Only ``matrix`` transformations are currently supported (#1564, #1564).
- [varLib] Added support for building ``VVAR`` table from ``vmtx`` and ``VORG``
  tables (#1551).
- [fontBuilder] Enable making CFF2 fonts with ``post`` table format 2 (#1557).
- Fixed ``DeprecationWarning`` on invalid escape sequences (#1562).

3.39.0 (released 2019-03-19)
----------------------------

- [ttLib/glyf] Raise more specific error when encountering recursive
  component references (#1545, #1546).
- [Doc/designspaceLib] Defined new ``public.skipExportGlyphs`` lib key (#1534,
  unified-font-object/ufo-spec#84).
- [varLib] Use ``vmtx`` to compute vertical phantom points; or ``hhea.ascent``
  and ``head.unitsPerEM`` if ``vmtx`` is missing (#1528).
- [gvar/cvar] Sort XML element's min/value/max attributes in TupleVariation
  toXML to improve readability of TTX dump (#1527).
- [varLib.plot] Added support for 2D plots with only 1 variation axis (#1522).
- [designspaceLib] Use axes maps when normalizing locations in
  DesignSpaceDocument (#1226, #1521), and when finding default source (#1535).
- [mutator] Set ``OVERLAP_SIMPLE`` and ``OVERLAP_COMPOUND`` glyf flags by
  default in ``instantiateVariableFont``. Added ``--no-overlap`` cli option
  to disable this (#1518).
- [subset] Fixed subsetting ``VVAR`` table (#1516, #1517).
  Fixed subsetting an ``HVAR`` table that has an ``AdvanceWidthMap`` when the
  option ``--retain-gids`` is used.
- [feaLib] Added ``forceChained`` in MultipleSubstStatement (#1511).
  Fixed double indentation of ``subtable`` statement (#1512).
  Added support for ``subtable`` statement in more places than just PairPos
  lookups (#1520).
  Handle lookupflag 0 and lookupflag without a value (#1540).
- [varLib] In ``load_designspace``, provide a default English name for the
  ``ital`` axis tag.
- Remove pyftinspect because it is unmaintained and bitrotted.

3.38.0 (released 2019-02-18)
----------------------------

- [cffLib] Fixed RecursionError when unpickling or deepcopying TTFont with
  CFF table (#1488, 649dc49).
- [subset] Fixed AttributeError when using --desubroutinize option (#1490).
  Also, fixed desubroutinizing bug when subrs contain hints (#1499).
- [CPAL] Make Color a subclass of namedtuple (173a0f5).
- [feaLib] Allow hyphen in glyph class names.
- [feaLib] Added 'tables' option to __main__.py (#1497).
- [feaLib] Add support for special-case contextual positioning formatting
  (#1501).
- [svgLib] Support converting SVG basic shapes (rect, circle, etc.) into
  equivalent SVG paths (#1500, #1508).
- [Snippets] Added name-viewer.ipynb Jupyter notebook.


3.37.3 (released 2019-02-05)
----------------------------

- The previous release accidentally changed several files from Unix to DOS
  line-endings. Fix that.

3.37.2 (released 2019-02-05)
----------------------------

- [varLib] Temporarily revert the fix to ``load_masters()``, which caused a
  crash in ``interpolate_layout()`` when ``deepcopy``-ing OTFs.

3.37.1 (released 2019-02-05)
----------------------------

- [varLib] ``load_masters()`` now actually assigns the fonts it loads to the
  source.font attributes.
- [varLib] Fixed an MVAR table generation crash when sparse masters were
  involved.
- [voltLib] ``parse_coverage_()`` returns a tuple instead of an ast.Enum.
- [feaLib] A MarkClassDefinition inside a block is no longer doubly indented
  compared to the rest of the block.

3.37.0 (released 2019-01-28)
----------------------------

- [svgLib] Added support for converting elliptical arcs to cubic bezier curves
  (#1464).
- [py23] Added backport for ``math.isfinite``.
- [varLib] Apply HIDDEN flag to fvar axis if designspace axis has attribute
  ``hidden=1``.
- Fixed "DeprecationWarning: invalid escape sequence" in Python 3.7.
- [voltLib] Fixed parsing glyph groups. Distinguish different PROCESS_MARKS.
  Accept COMPONENT glyph type.
- [feaLib] Distinguish missing value and explicit ``<NULL>`` for PairPos2
  format A (#1459). Round-trip ``useExtension`` keyword. Implemented
  ``ValueRecord.asFea`` method.
- [subset] Insert empty widths into hdmx when retaining gids (#1458).

3.36.0 (released 2019-01-17)
----------------------------

- [ttx] Added ``--no-recalc-timestamp`` option to keep the original font's
  ``head.modified`` timestamp (#1455, #46).
- [ttx/psCharStrings] Fixed issues while dumping and round-tripping CFF2 table
  with ttx (#1451, #1452, #1456).
- [voltLib] Fixed check for duplicate anchors (#1450). Don't try to read past
  the ``END`` operator in .vtp file (#1453).
- [varLib] Use sentinel value -0x8000 (-32768) to ignore post.underlineThickness
  and post.underlinePosition when generating MVAR deltas (#1449,
  googlei18n/ufo2ft#308).
- [subset] Added ``--retain-gids`` option to subset font without modifying the
  current glyph indices (#1443, #1447).
- [ufoLib] Replace deprecated calls to ``getbytes`` and ``setbytes`` with new
  equivalent ``readbytes`` and ``writebytes`` calls. ``fs`` >= 2.2 no required.
- [varLib] Allow loading masters from TTX files as well (#1441).

3.35.2 (released 2019-01-14)
----------------------------

- [hmtx/vmtx]: Allow to compile/decompile ``hmtx`` and ``vmtx`` tables even
  without the corresponding (required) metrics header tables, ``hhea`` and
  ``vhea`` (#1439).
- [varLib] Added support for localized axes' ``labelname`` and named instances'
  ``stylename`` (#1438).

3.35.1 (released 2019-01-09)
----------------------------

- [_m_a_x_p] Include ``maxComponentElements`` in ``maxp`` table's recalculation.

3.35.0 (released 2019-01-07)
----------------------------

- [psCharStrings] In ``encodeFloat`` function, use float's "general format" with
  8 digits of precision (i.e. ``%8g``) instead of ``str()``. This works around
  a macOS rendering issue when real numbers in CFF table are too long, and
  also makes sure that floats are encoded with the same precision in python 2.7
  and 3.x (#1430, googlei18n/ufo2ft#306).
- [_n_a_m_e/fontBuilder] Make ``_n_a_m_e_table.addMultilingualName`` also add
  Macintosh (platformID=1) names by default. Added options to ``FontBuilder``
  ``setupNameTable`` method to optionally disable Macintosh or Windows names.
  (#1359, #1431).
- [varLib] Make ``build`` optionally accept a ``DesignSpaceDocument`` object,
  instead of a designspace file path. The caller can now set the ``font``
  attribute of designspace's sources to a TTFont object, thus allowing to
  skip filenames manipulation altogether (#1416, #1425).
- [sfnt] Allow SFNTReader objects to be deep-copied.
- Require typing>=3.6.4 on py27 to fix issue with singledispatch (#1423).
- [designspaceLib/t1Lib/macRes] Fixed some cases where pathlib.Path objects were
  not accepted (#1421).
- [varLib] Fixed merging of multiple PairPosFormat2 subtables (#1411).
- [varLib] The default STAT table version is now set to 1.1, to improve
  compatibility with legacy applications (#1413).

3.34.2 (released 2018-12-17)
----------------------------

- [merge] Fixed AssertionError when none of the script tables in GPOS/GSUB have
  a DefaultLangSys record (#1408, 135a4a1).

3.34.1 (released 2018-12-17)
----------------------------

- [varLib] Work around macOS rendering issue for composites without gvar entry (#1381).

3.34.0 (released 2018-12-14)
----------------------------

- [varLib] Support generation of CFF2 variable fonts. ``model.reorderMasters()``
  now supports arbitrary mapping. Fix handling of overlapping ranges for feature
  variations (#1400).
- [cffLib, subset] Code clean-up and fixing related to CFF2 support.
- [ttLib.tables.ttProgram] Use raw strings for regex patterns (#1389).
- [fontbuilder] Initial support for building CFF2 fonts. Set CFF's
  ``FontMatrix`` automatically from unitsPerEm.
- [plistLib] Accept the more general ``collections.Mapping`` instead of the
  specific ``dict`` class to support custom data classes that should serialize
  to dictionaries.

3.33.0 (released 2018-11-30)
----------------------------
- [subset] subsetter bug fix with variable fonts.
- [varLib.featureVar] Improve FeatureVariations generation with many rules.
- [varLib] Enable sparse masters when building variable fonts:
  https://github.com/fonttools/fonttools/pull/1368#issuecomment-437257368
- [varLib.mutator] Add IDEF for GETVARIATION opcode, for handling hints in an
  instance.
- [ttLib] Ignore the length of kern table subtable format 0

3.32.0 (released 2018-11-01)
----------------------------

- [ufoLib] Make ``UFOWriter`` a subclass of ``UFOReader``, and use mixins
  for shared methods (#1344).
- [featureVars] Fixed normalization error when a condition's minimum/maximum
  attributes are missing in designspace ``<rule>`` (#1366).
- [setup.py] Added ``[plot]`` to extras, to optionally install ``matplotlib``,
  needed to use the ``fonTools.varLib.plot`` module.
- [varLib] Take total bounding box into account when resolving model (7ee81c8).
  If multiple axes have the same range ratio, cut across both (62003f4).
- [subset] Don't error if ``STAT`` has no ``AxisValue`` tables.
- [fontBuilder] Added a new submodule which contains a ``FontBuilder`` wrapper
  class around ``TTFont`` that makes it easier to create a working TTF or OTF
  font from scratch with code. NOTE: the API is still experimental and may
  change in future versions.

3.31.0 (released 2018-10-21)
----------------------------

- [ufoLib] Merged the `ufoLib <https://github.com/unified-font-objects/ufoLib>`__
  master branch into a new ``fontTools.ufoLib`` package (#1335, #1095).
  Moved ``ufoLib.pointPen`` module to ``fontTools.pens.pointPen``.
  Moved ``ufoLib.etree`` module to ``fontTools.misc.etree``.
  Moved ``ufoLib.plistlib`` module to ``fontTools.misc.plistlib``.
  To use the new ``fontTools.ufoLib`` module you need to install fonttools
  with the ``[ufo]`` extra, or you can manually install the required additional
  dependencies (cf. README.rst).
- [morx] Support AAT action type to insert glyphs and clean up compilation
  of AAT action tables (4a1871f, 2011ccf).
- [subset] The ``--no-hinting`` on a CFF font now also drops the optional
  hinting keys in Private dict: ``ForceBold``, ``LanguageGroup``, and
  ``ExpansionFactor`` (#1322).
- [subset] Include nameIDs referenced by STAT table (#1327).
- [loggingTools] Added ``msg=None`` argument to
  ``CapturingLogHandler.assertRegex`` (0245f2c).
- [varLib.mutator] Implemented ``FeatureVariations`` instantiation (#1244).
- [g_l_y_f] Added PointPen support to ``_TTGlyph`` objects (#1334).

3.30.0 (released 2018-09-18)
----------------------------

- [feaLib] Skip building noop class PairPos subtables when Coverage is NULL
  (#1318).
- [ttx] Expose the previously reserved bit flag ``OVERLAP_SIMPLE`` of
  glyf table's contour points in the TTX dump. This is used in some
  implementations to specify a non-zero fill with overlapping contours (#1316).
- [ttLib] Added support for decompiling/compiling ``TS1C`` tables containing
  VTT sources for ``cvar`` variation table (#1310).
- [varLib] Use ``fontTools.designspaceLib`` to read DesignSpaceDocument. The
  ``fontTools.varLib.designspace`` module is now deprecated and will be removed
  in future versions. The presence of an explicit ``axes`` element is now
  required in order to build a variable font (#1224, #1313).
- [varLib] Implemented building GSUB FeatureVariations table from the ``rules``
  element of DesignSpace document (#1240, #713, #1314).
- [subset] Added ``--no-layout-closure`` option to not expand the subset with
  the glyphs produced by OpenType layout features. Instead, OpenType features
  will be subset to only rules that are relevant to the otherwise-specified
  glyph set (#43, #1121).

3.29.1 (released 2018-09-10)
----------------------------

- [feaLib] Fixed issue whereby lookups from DFLT/dflt were not included in the
  DFLT/non-dflt language systems (#1307).
- [graphite] Fixed issue on big-endian architectures (e.g. ppc64) (#1311).
- [subset] Added ``--layout-scripts`` option to add/exclude set of OpenType
  layout scripts that will be preserved. By default all scripts are retained
  (``'*'``) (#1303).

3.29.0 (released 2018-07-26)
----------------------------

- [feaLib] In the OTL table builder, when the ``name`` table is excluded
  from the list of tables to be build, skip compiling ``featureNames`` blocks,
  as the records referenced in ``FeatureParams`` table don't exist (68951b7).
- [otBase] Try ``ExtensionLookup`` if other offset-overflow methods fail
  (05f95f0).
- [feaLib] Added support for explicit ``subtable;`` break statements in
  PairPos lookups; previously these were ignored (#1279, #1300, #1302).
- [cffLib.specializer] Make sure the stack depth does not exceed maxstack - 1,
  so that a subroutinizer can insert subroutine calls (#1301,
  https://github.com/googlei18n/ufo2ft/issues/266).
- [otTables] Added support for fixing offset overflow errors occurring inside
  ``MarkBasePos`` subtables (#1297).
- [subset] Write the default output file extension based on ``--flavor`` option,
  or the value of ``TTFont.sfntVersion`` (d7ac0ad).
- [unicodedata] Updated Blocks, Scripts and ScriptExtensions for Unicode 11
  (452c85e).
- [xmlWriter] Added context manager to XMLWriter class to autoclose file
  descriptor on exit (#1290).
- [psCharStrings] Optimize the charstring's bytecode by encoding as integers
  all float values that have no decimal portion (8d7774a).
- [ttFont] Fixed missing import of ``TTLibError`` exception (#1285).
- [feaLib] Allow any languages other than ``dflt`` under ``DFLT`` script
  (#1278, #1292).

3.28.0 (released 2018-06-19)
----------------------------

- [featureVars] Added experimental module to build ``FeatureVariations``
  tables. Still needs to be hooked up to ``varLib.build`` (#1240).
- [fixedTools] Added ``otRound`` to round floats to nearest integer towards
  positive Infinity. This is now used where we deal with visual data like X/Y
  coordinates, advance widths/heights, variation deltas, and similar (#1274,
  #1248).
- [subset] Improved GSUB closure memoize algorithm.
- [varLib.models] Fixed regression in model resolution (180124, #1269).
- [feaLib.ast] Fixed error when converting ``SubtableStatement`` to string
  (#1275).
- [varLib.mutator] Set ``OS/2.usWeightClass`` and ``usWidthClass``, and
  ``post.italicAngle`` based on the 'wght', 'wdth' and 'slnt' axis values
  (#1276, #1264).
- [py23/loggingTools] Don't automatically set ``logging.lastResort`` handler
  on py27. Moved ``LastResortLogger`` to the ``loggingTools`` module (#1277).

3.27.1 (released 2018-06-11)
----------------------------

- [ttGlyphPen] Issue a warning and skip building non-existing components
  (https://github.com/googlei18n/fontmake/issues/411).
- [tests] Fixed issue running ttx_test.py from a tagged commit.

3.27.0 (released 2018-06-11)
----------------------------

- [designspaceLib] Added new ``conditionSet`` element to ``rule`` element in
  designspace document. Bumped ``format`` attribute to ``4.0`` (previously,
  it was formatted as an integer). Removed ``checkDefault``, ``checkAxes``
  methods, and any kind of guessing about the axes when the ``<axes>`` element
  is missing. The default master is expected at the intersection of all default
  values for each axis (#1254, #1255, #1267).
- [cffLib] Fixed issues when compiling CFF2 or converting from CFF when the
  font has an FDArray (#1211, #1271).
- [varLib] Avoid attempting to build ``cvar`` table when ``glyf`` table is not
  present, as is the case for CFF2 fonts.
- [subset] Handle None coverages in MarkGlyphSets; revert commit 02616ab that
  sets empty Coverage tables in MarkGlyphSets to None, to make OTS happy.
- [ttFont] Allow to build glyph order from ``maxp.numGlyphs`` when ``post`` or
  ``cmap`` are missing.
- [ttFont] Added ``__len__`` method to ``_TTGlyphSet``.
- [glyf] Ensure ``GlyphCoordinates`` never overflow signed shorts (#1230).
- [py23] Added alias for ``itertools.izip`` shadowing the built-in ``zip``.
- [loggingTools] Memoize ``log`` property of ``LogMixin`` class (fbab12).
- [ttx] Impoved test coverage (#1261).
- [Snippets] Addded script to append a suffix to all family names in a font.
- [varLib.plot] Make it work with matplotlib >= 2.1 (b38e2b).

3.26.0 (released 2018-05-03)
----------------------------

- [designspace] Added a new optional ``layer`` attribute to the source element,
  and a corresponding ``layerName`` attribute to the ``SourceDescriptor``
  object (#1253).
  Added ``conditionset`` element to the ``rule`` element to the spec, but not
  implemented in designspace reader/writer yet (#1254).
- [varLib.models] Refine modeling one last time (0ecf5c5).
- [otBase] Fixed sharing of tables referred to by different offset sizes
  (795f2f9).
- [subset] Don't drop a GDEF that only has VarStore (fc819d6). Set to None
  empty Coverage tables in MarkGlyphSets (02616ab).
- [varLib]: Added ``--master-finder`` command-line option (#1249).
- [varLib.mutator] Prune fvar nameIDs from instance's name table (#1245).
- [otTables] Allow decompiling bad ClassDef tables with invalid format, with
  warning (#1236).
- [varLib] Make STAT v1.2 and reuse nameIDs from fvar table (#1242).
- [varLib.plot] Show master locations. Set axis limits to -1, +1.
- [subset] Handle HVAR direct mapping. Passthrough 'cvar'.
  Added ``--font-number`` command-line option for collections.
- [t1Lib] Allow a text encoding to be specified when parsing a Type 1 font
  (#1234). Added ``kind`` argument to T1Font constructor (c5c161c).
- [ttLib] Added context manager API to ``TTFont`` class, so it can be used in
  ``with`` statements to auto-close the file when exiting the context (#1232).

3.25.0 (released 2018-04-03)
----------------------------

- [varLib] Improved support-resolution algorithm. Previously, the on-axis
  masters would always cut the space. They don't anymore. That's more
  consistent, and fixes the main issue Erik showed at TYPO Labs 2017.
  Any varfont built that had an unusual master configuration will change
  when rebuilt (42bef17, a523a697,
  https://github.com/googlei18n/fontmake/issues/264).
- [varLib.models] Added a ``main()`` entry point, that takes positions and
  prints model results.
- [varLib.plot] Added new module to plot a designspace's
  VariationModel. Requires ``matplotlib``.
- [varLib.mutator] Added -o option to specify output file path (2ef60fa).
- [otTables] Fixed IndexError while pruning of HVAR pre-write (6b6c34a).
- [varLib.models] Convert delta array to floats if values overflows signed
  short integer (0055f94).

3.24.2 (released 2018-03-26)
----------------------------

- [otBase] Don't fail during ``ValueRecord`` copy if src has more items.
  We drop hinting in the subsetter by simply changing ValueFormat, without
  cleaning up the actual ValueRecords. This was causing assertion error if
  a variable font was subsetted without hinting and then passed directly to
  the mutator for instantiation without first it saving to disk.

3.24.1 (released 2018-03-06)
----------------------------

- [varLib] Don't remap the same ``DeviceTable`` twice in VarStore optimizer
  (#1206).
- [varLib] Add ``--disable-iup`` option to ``fonttools varLib`` script,
  and a ``optimize=True`` keyword argument to ``varLib.build`` function,
  to optionally disable IUP optimization while building varfonts.
- [ttCollection] Fixed issue while decompiling ttc with python3 (#1207).

3.24.0 (released 2018-03-01)
----------------------------

- [ttGlyphPen] Decompose composite glyphs if any components' transform is too
  large to fit a ``F2Dot14`` value, or clamp transform values that are
  (almost) equal to +2.0 to make them fit and avoid decomposing (#1200,
  #1204, #1205).
- [ttx] Added new ``-g`` option to dump glyphs from the ``glyf`` table
  splitted as individual ttx files (#153, #1035, #1132, #1202).
- Copied ``ufoLib.filenames`` module to ``fontTools.misc.filenames``, used
  for the ttx split-glyphs option (#1202).
- [feaLib] Added support for ``cvParameters`` blocks in Character Variant
  feautures ``cv01-cv99`` (#860, #1169).
- [Snippets] Added ``checksum.py`` script to generate/check SHA1 hash of
  ttx files (#1197).
- [varLib.mutator] Fixed issue while instantiating some variable fonts
  whereby the horizontal advance width computed from ``gvar`` phantom points
  could turn up to be negative (#1198).
- [varLib/subset] Fixed issue with subsetting GPOS variation data not
  picking up ``ValueRecord`` ``Device`` objects (54fd71f).
- [feaLib/voltLib] In all AST elements, the ``location`` is no longer a
  required positional argument, but an optional kewyord argument (defaults
  to ``None``). This will make it easier to construct feature AST from
  code (#1201).


3.23.0 (released 2018-02-26)
----------------------------

- [designspaceLib] Added an optional ``lib`` element to the designspace as a
  whole, as well as to the instance elements, to store arbitrary data in a
  property list dictionary, similar to the UFO's ``lib``. Added an optional
  ``font`` attribute to the ``SourceDescriptor``, to allow operating on
  in-memory font objects (#1175).
- [cffLib] Fixed issue with lazy-loading of attributes when attempting to
  set the CFF TopDict.Encoding (#1177, #1187).
- [ttx] Fixed regression introduced in 3.22.0 that affected the split tables
  ``-s`` option (#1188).
- [feaLib] Added ``IncludedFeaNotFound`` custom exception subclass, raised
  when an included feature file cannot be found (#1186).
- [otTables] Changed ``VarIdxMap`` to use glyph names internally instead of
  glyph indexes. The old ttx dumps of HVAR/VVAR tables that contain indexes
  can still be imported (21cbab8, 38a0ffb).
- [varLib] Implemented VarStore optimizer (#1184).
- [subset] Implemented pruning of GDEF VarStore, HVAR and MVAR (#1179).
- [sfnt] Restore backward compatiblity with ``numFonts`` attribute of
  ``SFNTReader`` object (#1181).
- [merge] Initial support for merging ``LangSysRecords`` (#1180).
- [ttCollection] don't seek(0) when writing to possibly unseekable strems.
- [subset] Keep all ``--name-IDs`` from 0 to 6 by default (#1170, #605, #114).
- [cffLib] Added ``width`` module to calculate optimal CFF default and
  nominal glyph widths.
- [varLib] Donâ€™t fail if STAT already in the master fonts (#1166).

3.22.0 (released 2018-02-04)
----------------------------

- [subset] Support subsetting ``endchar`` acting as ``seac``-like components
  in ``CFF`` (fixes #1162).
- [feaLib] Allow to build from pre-parsed ``ast.FeatureFile`` object.
  Added ``tables`` argument to only build some tables instead of all (#1159,
  #1163).
- [textTools] Replaced ``safeEval`` with ``ast.literal_eval`` (#1139).
- [feaLib] Added option to the parser to not resolve ``include`` statements
  (#1154).
- [ttLib] Added new ``ttCollection`` module to read/write TrueType and
  OpenType Collections. Exports a ``TTCollection`` class with a ``fonts``
  attribute containing a list of ``TTFont`` instances, the methods ``save``
  and ``saveXML``, plus some list-like methods. The ``importXML`` method is
  not implemented yet (#17).
- [unicodeadata] Added ``ot_tag_to_script`` function that converts from
  OpenType script tag to Unicode script code.
- Added new ``designspaceLib`` subpackage, originally from Erik Van Blokland's
  ``designSpaceDocument``: https://github.com/LettError/designSpaceDocument
  NOTE: this is not yet used internally by varLib, and the API may be subject
  to changes (#911, #1110, LettError/designSpaceDocument#28).
- Added new FontTools icon images (8ee7c32).
- [unicodedata] Added ``script_horizontal_direction`` function that returns
  either "LTR" or "RTL" given a unicode script code.
- [otConverters] Don't write descriptive name string as XML comment if the
  NameID value is 0 (== NULL) (#1151, #1152).
- [unicodedata] Add ``ot_tags_from_script`` function to get the list of
  OpenType script tags associated with unicode script code (#1150).
- [feaLib] Don't error when "enumerated" kern pairs conflict with preceding
  single pairs; emit warning and chose the first value (#1147, #1148).
- [loggingTools] In ``CapturingLogHandler.assertRegex`` method, match the
  fully formatted log message.
- [sbix] Fixed TypeError when concatenating str and bytes (#1154).
- [bezierTools] Implemented cusp support and removed ``approximate_fallback``
  arg in ``calcQuadraticArcLength``. Added ``calcCubicArcLength`` (#1142).

3.21.2 (released 2018-01-08)
----------------------------

- [varLib] Fixed merging PairPos Format1/2 with missing subtables (#1125).

3.21.1 (released 2018-01-03)
----------------------------

- [feaLib] Allow mixed single/multiple substitutions (#612)
- Added missing ``*.afm`` test assets to MAINFEST.in (#1137).
- Fixed dumping ``SVG`` tables containing color palettes (#1124).

3.21.0 (released 2017-12-18)
----------------------------

- [cmap] when compiling format6 subtable, don't assume gid0 is always called
  '.notdef' (1e42224).
- [ot] Allow decompiling fonts with bad Coverage format number (1aafae8).
- Change FontTools licence to MIT (#1127).
- [post] Prune extra names already in standard Mac set (df1e8c7).
- [subset] Delete empty SubrsIndex after subsetting (#994, #1118).
- [varLib] Don't share points in cvar by default, as it currently fails on
  some browsers (#1113).
- [afmLib] Make poor old afmLib work on python3.

3.20.1 (released 2017-11-22)
----------------------------

- [unicodedata] Fixed issue with ``script`` and ``script_extension`` functions
  returning inconsistent short vs long names. They both return the short four-
  letter script codes now. Added ``script_name`` and ``script_code`` functions
  to look up the long human-readable script name from the script code, and
  viceversa (#1109, #1111).

3.20.0 (released 2017-11-21)
----------------------------

- [unicodedata] Addded new module ``fontTools.unicodedata`` which exports the
  same interface as the built-in ``unicodedata`` module, with the addition of
  a few functions that are missing from the latter, such as ``script``,
  ``script_extension`` and ``block``. Added a ``MetaTools/buildUCD.py`` script
  to download and parse data files from the Unicode Character Database and
  generate python modules containing lists of ranges and property values.
- [feaLib] Added ``__str__`` method to all ``ast`` elements (delegates to the
  ``asFea`` method).
- [feaLib] ``Parser`` constructor now accepts a ``glyphNames`` iterable
  instead of ``glyphMap`` dict. The latter still works but with a pending
  deprecation warning (#1104).
- [bezierTools] Added arc length calculation functions originally from
  ``pens.perimeterPen`` module (#1101).
- [varLib] Started generating STAT table (8af4309). Right now it just reflects
  the axes, and even that with certain limitations:
  * AxisOrdering is set to the order axes are defined,
  * Name-table entries are not shared with fvar.
- [py23] Added backports for ``redirect_stdout`` and ``redirect_stderr``
  context managers (#1097).
- [Graphite] Fixed some round-trip bugs (#1093).

3.19.0 (released 2017-11-06)
----------------------------

- [varLib] Try set of used points instead of all points when testing whether to
  share points between tuples (#1090).
- [CFF2] Fixed issue with reading/writing PrivateDict BlueValues to TTX file.
  Read the commit message 8b02b5a and issue #1030 for more details.
  NOTE: this change invalidates all the TTX files containing CFF2 tables
  that where dumped with previous verisons of fonttools.
  CFF2 Subr items can have values on the stack after the last operator, thus
  a ``CFF2Subr`` class was added to accommodate this (#1091).
- [_k_e_r_n] Fixed compilation of AAT kern version=1.0 tables (#1089, #1094)
- [ttLib] Added getBestCmap() convenience method to TTFont class and cmap table
  class that returns a preferred Unicode cmap subtable given a list of options
  (#1092).
- [morx] Emit more meaningful subtable flags. Implement InsertionMorphAction

3.18.0 (released 2017-10-30)
----------------------------

- [feaLib] Fixed writing back nested glyph classes (#1086).
- [TupleVariation] Reactivated shared points logic, bugfixes (#1009).
- [AAT] Implemented ``morx`` ligature subtables (#1082).
- [reverseContourPen] Keep duplicate lineTo following a moveTo (#1080,
  https://github.com/googlei18n/cu2qu/issues/51).
- [varLib.mutator] Suport instantiation of GPOS, GDEF and MVAR (#1079).
- [sstruct] Fixed issue with ``unicode_literals`` and ``struct`` module in
  old versions of python 2.7 (#993).

3.17.0 (released 2017-10-16)
----------------------------

- [svgPathPen] Added an ``SVGPathPen`` that translates segment pen commands
  into SVG path descriptions. Copied from Tal Leming's ``ufo2svg.svgPathPen``
  https://github.com/typesupply/ufo2svg/blob/d69f992/Lib/ufo2svg/svgPathPen.py
- [reverseContourPen] Added ``ReverseContourPen``, a filter pen that draws
  contours with the winding direction reversed, while keeping the starting
  point (#1071).
- [filterPen] Added ``ContourFilterPen`` to manipulate contours as a whole
  rather than segment by segment.
- [arrayTools] Added ``Vector`` class to apply math operations on an array
  of numbers, and ``pairwise`` function to loop over pairs of items in an
  iterable.
- [varLib] Added support for building and interpolation of ``cvar`` table
  (f874cf6, a25a401).

3.16.0 (released 2017-10-03)
----------------------------

- [head] Try using ``SOURCE_DATE_EPOCH`` environment variable when setting
  the ``head`` modified timestamp to ensure reproducible builds (#1063).
  See https://reproducible-builds.org/specs/source-date-epoch/
- [VTT] Decode VTT's ``TSI*`` tables text as UTF-8 (#1060).
- Added support for Graphite font tables: Feat, Glat, Gloc, Silf and Sill.
  Thanks @mhosken! (#1054).
- [varLib] Default to using axis "name" attribute if "labelname" element
  is missing (588f524).
- [merge] Added support for merging Script records. Remove unused features
  and lookups after merge (d802580, 556508b).
- Added ``fontTools.svgLib`` package. Includes a parser for SVG Paths that
  supports the Pen protocol (#1051). Also, added a snippet to convert SVG
  outlines to UFO GLIF (#1053).
- [AAT] Added support for ``ankr``, ``bsln``, ``mort``, ``morx``, ``gcid``,
  and ``cidg``.
- [subset] Implemented subsetting of ``prop``, ``opbd``, ``bsln``, ``lcar``.

3.15.1 (released 2017-08-18)
----------------------------

- [otConverters] Implemented ``__add__`` and ``__radd__`` methods on
  ``otConverters._LazyList`` that decompile a lazy list before adding
  it to another list or ``_LazyList`` instance. Fixes an ``AttributeError``
  in the ``subset`` module when attempting to sum ``_LazyList`` objects
  (6ef48bd2, 1aef1683).
- [AAT] Support the `opbd` table with optical bounds (a47f6588).
- [AAT] Support `prop` table with glyph properties (d05617b4).


3.15.0 (released 2017-08-17)
----------------------------

- [AAT] Added support for AAT lookups. The ``lcar`` table can be decompiled
  and recompiled; futher work needed to handle ``morx`` table (#1025).
- [subset] Keep (empty) DefaultLangSys for Script 'DFLT' (6eb807b5).
- [subset] Support GSUB/GPOS.FeatureVariations (fe01d87b).
- [varLib] In ``models.supportScalars``, ignore an axis when its peak value
  is 0 (fixes #1020).
- [varLib] Add default mappings to all axes in avar to fix rendering issue
  in some rasterizers (19c4b377, 04eacf13).
- [varLib] Flatten multiple tail PairPosFormat2 subtables before merging
  (c55ef525).
- [ttLib] Added support for recalculating font bounding box in ``CFF`` and
  ``head`` tables, and min/max values in ``hhea`` and ``vhea`` tables (#970).

3.14.0 (released 2017-07-31)
----------------------------

- [varLib.merger] Remove Extensions subtables before merging (f7c20cf8).
- [varLib] Initialize the avar segment map with required default entries
  (#1014).
- [varLib] Implemented optimal IUP optmiziation (#1019).
- [otData] Add ``AxisValueFormat4`` for STAT table v1.2 from OT v1.8.2
  (#1015).
- [name] Fixed BCP46 language tag for Mac langID=9: 'si' -> 'sl'.
- [subset] Return value from ``_DehintingT2Decompiler.op_hintmask``
  (c0d672ba).
- [cffLib] Allow to get TopDict by index as well as by name (dca96c9c).
- [cffLib] Removed global ``isCFF2`` state; use one set of classes for
  both CFF and CFF2, maintaining backward compatibility existing codeÂ (#1007).
- [cffLib] Deprecated maxstack operator, per OpenType spec update 1.8.1.
- [cffLib] Added missing default (-100) for UnderlinePosition (#983).
- [feaLib] Enable setting nameIDs greater than 255 (#1003).
- [varLib] Recalculate ValueFormat when merging SinglePos (#996).
- [varLib] Do not emit MVAR if there are no entries in the variation store
  (#987).
- [ttx] For ``-x`` option, pad with space if table tag length is < 4.

3.13.1 (released 2017-05-30)
----------------------------

- [feaLib.builder] Removed duplicate lookups optimization. The original
  lookup order and semantics of the feature file are preserved (#976).

3.13.0 (released 2017-05-24)
----------------------------

- [varLib.mutator] Implement IUP optimization (#969).
- [_g_l_y_f.GlyphCoordinates] Changed ``__bool__()`` semantics to match those
  of other iterables (e46f949). Removed ``__abs__()`` (3db5be2).
- [varLib.interpolate_layout] Added ``mapped`` keyword argument to
  ``interpolate_layout`` to allow disabling avar mapping: if False (default),
  the location is mapped using the map element of the axes in designspace file;
  if True, it is assumed that location is in designspace's internal space and
  no mapping is performed (#950, #975).
- [varLib.interpolate_layout] Import designspace-loading logic from varLib.
- [varLib] Fixed bug with recombining PairPosClass2 subtables (81498e5, #914).
- [cffLib.specializer] When copying iterables, cast to list (462b7f86).

3.12.1 (released 2017-05-18)
----------------------------

- [pens.t2CharStringPen] Fixed AttributeError when calling addComponent in
  T2CharStringPen (#965).

3.12.0 (released 2017-05-17)
----------------------------

- [cffLib.specializer] Added new ``specializer`` module to optimize CFF
  charstrings, used by the T2CharStringPen (#948).
- [varLib.mutator] Sort glyphs by component depth before calculating composite
  glyphs' bounding boxes to ensure deltas are correctly caclulated (#945).
- [_g_l_y_f] Fixed loss of precision in GlyphCoordinates by using 'd' (double)
  instead of 'f' (float) as ``array.array`` typecode (#963, #964).

3.11.0 (released 2017-05-03)
----------------------------

- [t2CharStringPen] Initial support for specialized Type2 path operators:
  vmoveto, hmoveto, vlineto, hlineto, vvcurveto, hhcurveto, vhcurveto and
  hvcurveto. This should produce more compact charstrings (#940, #403).
- [Doc] Added Sphinx sources for the documentation. Thanks @gferreira (#935).
- [fvar] Expose flags in XML (#932)
- [name] Add helper function for building multi-lingual names (#921)
- [varLib] Fixed kern merging when a PairPosFormat2 has ClassDef1 with glyphs
  that are NOT present in the Coverage (1b5e1c4, #939).
- [varLib] Fixed non-deterministic ClassDef order with PY3 (f056c12, #927).
- [feLib] Throw an error when the same glyph is defined in multiple mark
  classes within the same lookup (3e3ff00, #453).

3.10.0 (released 2017-04-14)
----------------------------

- [varLib] Added support for building ``avar`` table, using the designspace
  ``<map>`` elements.
- [varLib] Removed unused ``build(..., axisMap)`` argument. Axis map should
  be specified in designspace file now. We do not accept nonstandard axes
  if ``<axes>`` element is not present.
- [varLib] Removed "custom" axis from the ``standard_axis_map``. This was
  added before when glyphsLib was always exporting the (unused) custom axis.
- [varLib] Added partial support for building ``MVAR`` table; does not
  implement ``gasp`` table variations yet.
- [pens] Added FilterPen base class, for pens that control another pen;
  factored out ``addComponent`` method from BasePen into a separate abstract
  DecomposingPen class; added DecomposingRecordingPen, which records
  components decomposed as regular contours.
- [TSI1] Fixed computation of the textLength of VTT private tables (#913).
- [loggingTools] Added ``LogMixin`` class providing a ``log`` property to
  subclasses, which returns a ``logging.Logger`` named after the latter.
- [loggingTools] Added ``assertRegex`` method to ``CapturingLogHandler``.
- [py23] Added backport for python 3's ``types.SimpleNamespace`` class.
- [EBLC] Fixed issue with python 3 ``zip`` iterator.

3.9.2 (released 2017-04-08)
---------------------------

- [pens] Added pen to draw glyphs using WxPython ``GraphicsPath`` class:
  https://wxpython.org/docs/api/wx.GraphicsPath-class.html
- [varLib.merger] Fixed issue with recombining multiple PairPosFormat2
  subtables (#888)
- [varLib] Do not encode gvar deltas that are all zeroes, or if all values
  are smaller than tolerance.
- [ttLib] _TTGlyphSet glyphs now also have ``height`` and ``tsb`` (top
  side bearing) attributes from the ``vmtx`` table, if present.
- [glyf] In ``GlyphCoordintes`` class, added ``__bool__`` / ``__nonzero__``
  methods, and ``array`` property to get raw array.
- [ttx] Support reading TTX files with BOM (#896)
- [CFF2] Fixed the reporting of the number of regions in the font.

3.9.1 (released 2017-03-20)
---------------------------

- [varLib.merger] Fixed issue while recombining multiple PairPosFormat2
  subtables if they were split because of offset overflows (9798c30).
- [varLib.merger] Only merge multiple PairPosFormat1 subtables if there is
  at least one of the fonts with a non-empty Format1 subtable (0f5a46b).
- [varLib.merger] Fixed IndexError with empty ClassDef1 in PairPosFormat2
  (aad0d46).
- [varLib.merger] Avoid reusing Class2Record (mutable) objects (e6125b3).
- [varLib.merger] Calculate ClassDef1 and ClassDef2's Format when merging
  PairPosFormat2 (23511fd).
- [macUtils] Added missing ttLib import (b05f203).

3.9.0 (released 2017-03-13)
---------------------------

- [feaLib] Added (partial) support for parsing feature file comments ``# ...``
  appearing in between statements (#879).
- [feaLib] Cleaned up syntax tree for FeatureNames.
- [ttLib] Added support for reading/writing ``CFF2`` table (thanks to
  @readroberts at Adobe), and ``TTFA`` (ttfautohint) table.
- [varLib] Fixed regression introduced with 3.8.0 in the calculation of
  ``NumShorts``, i.e. the number of deltas in ItemVariationData's delta sets
  that use a 16-bitÂ representation (b2825ff).

3.8.0 (released 2017-03-05)
---------------------------

- New pens: MomentsPen, StatisticsPen, RecordingPen, and TeePen.
- [misc] Added new ``fontTools.misc.symfont`` module, for symbolic font
  statistical analysis; requires ``sympy`` (http://www.sympy.org/en/index.html)
- [varLib] Added experimental ``fontTools.varLib.interpolatable`` module for
  finding wrong contour order between different masters
- [varLib] designspace.load() now returns a dictionary, instead of a tuple,
  and supports <axes> element (#864); the 'masters' item was renamed 'sources',
  like the <sources> element in the designspace document
- [ttLib] Fixed issue with recalculating ``head`` modified timestamp when
  saving CFF fonts
- [ttLib] In TupleVariation, round deltas before compiling (#861, fixed #592)
- [feaLib] Ignore duplicate glyphs in classes used as MarkFilteringSet and
  MarkAttachmentType (#863)
- [merge] Changed the ``gasp`` table merge logic so that only the one from
  the first font is retained, similar to other hinting tables (#862)
- [Tests] Added tests for the ``varLib`` package, as well as test fonts
  from the "Annotated OpenType Specification" (AOTS) to exercise ``ttLib``'s
  table readers/writers (<https://github.com/adobe-type-tools/aots>)

3.7.2 (released 2017-02-17)
---------------------------

- [subset] Keep advance widths when stripping ".notdef" glyph outline in
  CID-keyed CFF fonts (#845)
- [feaLib] Zero values now produce the same results as makeotf (#633, #848)
- [feaLib] More compact encoding for â€œContextual positioning with in-line
  single positioning rulesâ€ (#514)

3.7.1 (released 2017-02-15)
---------------------------

- [subset] Fixed issue with ``--no-hinting`` option whereby advance widths in
  Type 2 charstrings were also being stripped (#709, #343)
- [feaLib] include statements now resolve relative paths like makeotf (#838)
- [feaLib] table ``name`` now handles Unicode codepoints beyond the Basic
  Multilingual Plane, also supports old-style MacOS platform encodings (#842)
- [feaLib] correctly escape string literals when emitting feature syntax (#780)

3.7.0 (released 2017-02-11)
---------------------------

- [ttx, mtiLib] Preserve ordering of glyph alternates in GSUB type 3 (#833).
- [feaLib] Glyph names can have dashes, as per new AFDKO syntax v1.20 (#559).
- [feaLib] feaLib.Parser now needs the font's glyph map for parsing.
- [varLib] Fix regression where GPOS values were stored as 0.
- [varLib] Allow merging of class-based kerning when ClassDefs are different

3.6.3 (released 2017-02-06)
---------------------------

- [varLib] Fix building variation of PairPosFormat2 (b5c34ce).
- Populate defaults even for otTables that have postRead (e45297b).
- Fix compiling of MultipleSubstFormat1 with zero 'out' glyphs (b887860).

3.6.2 (released 2017-01-30)
---------------------------

- [varLib.merger] Fixed "TypeError: reduce() of empty sequence with no
  initial value" (3717dc6).

3.6.1 (released 2017-01-28)
---------------------------

-  [py23] Fixed unhandled exception occurring at interpreter shutdown in
   the "lastÂ resort" logging handler (972b3e6).
-  [agl] Ensure all glyph names are of native 'str' type; avoid mixing
   'str' and 'unicode' in TTFont.glyphOrder (d8c4058).
-  Fixed inconsistent title levels in README.rst that caused PyPI to
   incorrectly render the reStructuredText page.

3.6.0 (released 2017-01-26)
---------------------------

-  [varLib] Refactored and improved the variation-font-building process.
-  Assembly code in the fpgm, prep, and glyf tables is now indented in
   XML output for improved readability. The ``instruction`` element is
   written as a simple tag if empty (#819).
-  [ttx] Fixed 'I/O operation on closed file' error when dumping
   multiple TTXs to standard output with the '-o -' option.
-  The unit test modules (``*_test.py``) have been moved outside of the
   fontTools package to the Tests folder, thus they are no longer
   installed (#811).

3.5.0 (released 2017-01-14)
---------------------------

-  Font tables read from XML can now be written back to XML with no
   loss.
-  GSUB/GPOS LookupType is written out in XML as an element, not
   comment. (#792)
-  When parsing cmap table, do not store items mapped to glyph id 0.
   (#790)
-  [otlLib] Make ClassDef sorting deterministic. Fixes #766 (7d1ddb2)
-  [mtiLib] Added unit tests (#787)
-  [cvar] Implemented cvar table
-  [gvar] Renamed GlyphVariation to TupleVariation to match OpenType
   terminology.
-  [otTables] Handle gracefully empty VarData.Item array when compiling
   XML. (#797)
-  [varLib] Re-enabled generation of ``HVAR`` table for fonts with
   TrueType outlines; removed ``--build-HVAR`` command-line option.
-  [feaLib] The parser can now be extended to support non-standard
   statements in FEA code by using a customized Abstract Syntax Tree.
   See, for example, ``feaLib.builder_test.test_extensions`` and
   baseClass.feax (#794, fixes #773).
-  [feaLib] Added ``feaLib`` command to the 'fonttools' command-line
   tool; applies a feature file to a font. ``fonttools feaLib -h`` for
   help.
-  [pens] The ``T2CharStringPen`` now takes an optional
   ``roundTolerance`` argument to control the rounding of coordinates
   (#804, fixes #769).
-  [ci] Measure test coverage on all supported python versions and OSes,
   combine coverage data and upload to
   https://codecov.io/gh/fonttools/fonttools (#786)
-  [ci] Configured Travis and Appveyor for running tests on Python 3.6
   (#785, 55c03bc)
-  The manual pages installation directory can be customized through
   ``FONTTOOLS_MANPATH`` environment variable (#799, fixes #84).
-  [Snippets] Added otf2ttf.py, for converting fonts from CFF to
   TrueType using the googlei18n/cu2qu module (#802)

3.4.0 (released 2016-12-21)
---------------------------

-  [feaLib] Added support for generating FEA text from abstract syntax
   tree (AST) objects (#776). Thanks @mhosken
-  Added ``agl.toUnicode`` function to convert AGL-compliant glyph names
   to Unicode strings (#774)
-  Implemented MVAR table (b4d5381)

3.3.1 (released 2016-12-15)
---------------------------

-  [setup] We no longer use versioneer.py to compute fonttools version
   from git metadata, as this has caused issues for some users (#767).
   Now we bump the version strings manually with a custom ``release``
   command of setup.py script.

3.3.0 (released 2016-12-06)
---------------------------

-  [ttLib] Implemented STAT table from OpenType 1.8 (#758)
-  [cffLib] Fixed decompilation of CFF fonts containing non-standard
   key/value pairs in FontDict (issue #740; PR #744)
-  [py23] minor: in ``round3`` function, allow the second argument to be
   ``None`` (#757)
-  The standalone ``sstruct`` and ``xmlWriter`` modules, deprecated
   since vesion 3.2.0, have been removed. They can be imported from the
   ``fontTools.misc`` package.

3.2.3 (released 2016-12-02)
---------------------------

-  [py23] optimized performance of round3 function; added backport for
   py35 math.isclose() (9d8dacb)
-  [subset] fixed issue with 'narrow' (UCS-2) Python 2 builds and
   ``--text``/``--text-file`` options containing non-BMP chararcters
   (16d0e5e)
-  [varLib] fixed issuewhen normalizing location values (8fa2ee1, #749)
-  [inspect] Made it compatible with both python2 and python3 (167ee60,
   #748). Thanks @pnemade

3.2.2 (released 2016-11-24)
---------------------------

-  [varLib] Do not emit null axes in fvar (1bebcec). Thanks @robmck-ms
-  [varLib] Handle fonts without GPOS (7915a45)
-  [merge] Ignore LangSys if None (a11bc56)
-  [subset] Fix subsetting MathVariants (78d3cbe)
-  [OS/2] Fix "Private Use (plane 15)" range (08a0d55). Thanks @mashabow

3.2.1 (released 2016-11-03)
---------------------------

-  [OS/2] fix checking ``fsSelection`` bits matching ``head.macStyle``
   bits
-  [varLib] added ``--build-HVAR`` option to generate ``HVAR`` table for
   fonts with TrueType outlines. For ``CFF2``, it is enabled by default.

3.2.0 (released 2016-11-02)
---------------------------

-  [varLib] Improve support for OpenType 1.8 Variable Fonts:
-  Implement GDEF's VariationStore
-  Implement HVAR/VVAR tables
-  Partial support for loading MutatorMath .designspace files with
   varLib.designspace module
-  Add varLib.models with Variation fonts interpolation models
-  Implement GSUB/GPOS FeatureVariations
-  Initial support for interpolating and merging OpenType Layout tables
   (see ``varLib.interpolate_layout`` and ``varLib.merger`` modules)
-  [API change] Change version to be an integer instead of a float in
   XML output for GSUB, GPOS, GDEF, MATH, BASE, JSTF, HVAR, VVAR, feat,
   hhea and vhea tables. Scripts that set the Version for those to 1.0
   or other float values also need fixing. A warning is emitted when
   code or XML needs fix.
-  several bug fixes to the cffLib module, contributed by Adobe's
   @readroberts
-  The XML output for CFF table now has a 'major' and 'minor' elements
   for specifying whether it's version 1.0 or 2.0 (support for CFF2 is
   coming soon)
-  [setup.py] remove undocumented/deprecated ``extra_path`` Distutils
   argument. This means that we no longer create a "FontTools" subfolder
   in site-packages containing the actual fontTools package, as well as
   the standalone xmlWriter and sstruct modules. The latter modules are
   also deprecated, and scheduled for removal in upcoming releases.
   Please change your import statements to point to from fontTools.misc
   import xmlWriter and from fontTools.misc import sstruct.
-  [scripts] Add a 'fonttools' command-line tool that simply runs
   ``fontTools.*`` sub-modules: e.g. ``fonttools ttx``,
   ``fonttools subset``, etc.
-  [hmtx/vmts] Read advance width/heights as unsigned short (uint16);
   automatically round float values to integers.
-  [ttLib/xmlWriter] add 'newlinestr=None' keyword argument to
   ``TTFont.saveXML`` for overriding os-specific line endings (passed on
   to ``XMLWriter`` instances).
-  [versioning] Use versioneer instead of ``setuptools_scm`` to
   dynamically load version info from a git checkout at import time.
-  [feaLib] Support backslash-prefixed glyph names.

3.1.2 (released 2016-09-27)
---------------------------

-  restore Makefile as an alternative way to build/check/install
-  README.md: update instructions for installing package from source,
   and for running test suite
-  NEWS: Change log was out of sync with tagged release

3.1.1 (released 2016-09-27)
---------------------------

-  Fix ``ttLibVersion`` attribute in TTX files still showing '3.0'
   instead of '3.1'.
-  Use ``setuptools_scm`` to manage package versions.

3.1.0 (released 2016-09-26)
---------------------------

-  [feaLib] New library to parse and compile Adobe FDK OpenType Feature
   files.
-  [mtiLib] New library to parse and compile Monotype 'FontDame'
   OpenType Layout Tables files.
-  [voltLib] New library to parse Microsoft VOLT project files.
-  [otlLib] New library to work with OpenType Layout tables.
-  [varLib] New library to work with OpenType Font Variations.
-  [pens] Add ttGlyphPen to draw to TrueType glyphs, and t2CharStringPen
   to draw to Type 2 Charstrings (CFF); add areaPen and perimeterPen.
-  [ttLib.tables] Implement 'meta' and 'trak' tables.
-  [ttx] Add --flavor option for compiling to 'woff' or 'woff2'; add
   ``--with-zopfli`` option to use Zopfli to compress WOFF 1.0 fonts.
-  [subset] Support subsetting 'COLR'/'CPAL' and 'CBDT'/'CBLC' color
   fonts tables, and 'gvar' table for variation fonts.
-  [Snippets] Add ``symfont.py``, for symbolic font statistics analysis;
   interpolatable.py, a preliminary script for detecting interpolation
   errors; ``{merge,dump}_woff_metadata.py``.
-  [classifyTools] Helpers to classify things into classes.
-  [CI] Run tests on Windows, Linux and macOS using Appveyor and Travis
   CI; check unit test coverage with Coverage.py/Coveralls; automatic
   deployment to PyPI on tags.
-  [loggingTools] Use Python built-in logging module to print messages.
-  [py23] Make round() behave like Python 3 built-in round(); define
   round2() and round3().

3.0 (released 2015-09-01)
-------------------------

-  Add Snippet scripts for cmap subtable format conversion, printing
   GSUB/GPOS features, building a GX font from two masters
-  TTX WOFF2 support and a ``-f`` option to overwrite output file(s)
-  Support GX tables: ``avar``, ``gvar``, ``fvar``, ``meta``
-  Support ``feat`` and gzip-compressed SVG tables
-  Upgrade Mac East Asian encodings to native implementation if
   available
-  Add Roman Croatian and Romanian encodings, codecs for mac-extended
   East Asian encodings
-  Implement optimal GLYF glyph outline packing; disabled by default

2.5 (released 2014-09-24)
-------------------------

-  Add a Qt pen
-  Add VDMX table converter
-  Load all OpenType sub-structures lazily
-  Add support for cmap format 13.
-  Add pyftmerge tool
-  Update to Unicode 6.3.0d3
-  Add pyftinspect tool
-  Add support for Google CBLC/CBDT color bitmaps, standard EBLC/EBDT
   embedded bitmaps, and ``SVG`` table (thanks to Read Roberts at Adobe)
-  Add support for loading, saving and ttx'ing WOFF file format
-  Add support for Microsoft COLR/CPAL layered color glyphs
-  Support PyPy
-  Support Jython, by replacing numpy with array/lists modules and
   removed it, pure-Python StringIO, not cStringIO
-  Add pyftsubset and Subsetter object, supporting CFF and TTF
-  Add to ttx args for -q for quiet mode, -z to choose a bitmap dump
   format

2.4 (released 2013-06-22)
-------------------------

-  Option to write to arbitrary files
-  Better dump format for DSIG
-  Better detection of OTF XML
-  Fix issue with Apple's kern table format
-  Fix mangling of TT glyph programs
-  Fix issues related to mona.ttf
-  Fix Windows Installer instructions
-  Fix some modern MacOS issues
-  Fix minor issues and typos

2.3 (released 2009-11-08)
-------------------------

-  TrueType Collection (TTC) support
-  Python 2.6 support
-  Update Unicode data to 5.2.0
-  Couple of bug fixes

2.2 (released 2008-05-18)
-------------------------

-  ClearType support
-  cmap format 1 support
-  PFA font support
-  Switched from Numeric to numpy
-  Update Unicode data to 5.1.0
-  Update AGLFN data to 1.6
-  Many bug fixes

2.1 (released 2008-01-28)
-------------------------

-  Many years worth of fixes and features

2.0b2 (released 2002-??-??)
---------------------------

-  Be "forgiving" when interpreting the maxp table version field:
   interpret any value as 1.0 if it's not 0.5. Fixes dumping of these
   GPL fonts: http://www.freebsd.org/cgi/pds.cgi?ports/chinese/wangttf
-  Fixed ttx -l: it turned out this part of the code didn't work with
   Python 2.2.1 and earlier. My bad to do most of my testing with a
   different version than I shipped TTX with :-(
-  Fixed bug in ClassDef format 1 subtable (Andreas Seidel bumped into
   this one).

2.0b1 (released 2002-09-10)
---------------------------

-  Fixed embarrassing bug: the master checksum in the head table is now
   calculated correctly even on little-endian platforms (such as Intel).
-  Made the cmap format 4 compiler smarter: the binary data it creates
   is now more or less as compact as possible. TTX now makes more
   compact data than in any shipping font I've tested it with.
-  Dump glyph names as a separate "GlyphOrder" pseudo table as opposed
   to as part of the glyf table (obviously needed for CFF-OTF's).
-  Added proper support for the CFF table.
-  Don't barf on empty tables (questionable, but "there are font out
   there...")
-  When writing TT glyf data, align glyphs on 4-byte boundaries. This
   seems to be the current recommendation by MS. Also: don't barf on
   fonts which are already 4-byte aligned.
-  Windows installer contributed bu Adam Twardoch! Yay!
-  Changed the command line interface again, now by creating one new
   tool replacing the old ones: ttx It dumps and compiles, depending on
   input file types. The options have changed somewhat.
-  The -d option is back (output dir)
-  ttcompile's -i options is now called -m (as in "merge"), to avoid
   clash with dump's -i.
-  The -s option ("split tables") no longer creates a directory, but
   instead outputs a small .ttx file containing references to the
   individual table files. This is not a true link, it's a simple file
   name, and the referenced file should be in the same directory so
   ttcompile can find them.
-  compile no longer accepts a directory as input argument. Instead it
   can parse the new "mini-ttx" format as output by "ttx -s".
-  all arguments are input files
-  Renamed the command line programs and moved them to the Tools
   subdirectory. They are now installed by the setup.py install script.
-  Added OpenType support. BASE, GDEF, GPOS, GSUB and JSTF are (almost)
   fully supported. The XML output is not yet final, as I'm still
   considering to output certain subtables in a more human-friendly
   manner.
-  Fixed 'kern' table to correctly accept subtables it doesn't know
   about, as well as interpreting Apple's definition of the 'kern' table
   headers correctly.
-  Fixed bug where glyphnames were not calculated from 'cmap' if it was
   (one of the) first tables to be decompiled. More specifically: it
   cmap was the first to ask for a glyphID -> glyphName mapping.
-  Switched XML parsers: use expat instead of xmlproc. Should be faster.
-  Removed my UnicodeString object: I now require Python 2.0 or up,
   which has unicode support built in.
-  Removed assert in glyf table: redundant data at the end of the table
   is now ignored instead of raising an error. Should become a warning.
-  Fixed bug in hmtx/vmtx code that only occured if all advances were
   equal.
-  Fixed subtle bug in TT instruction disassembler.
-  Couple of fixes to the 'post' table.
-  Updated OS/2 table to latest spec.

1.0b1 (released 2001-08-10)
---------------------------

-  Reorganized the command line interface for ttDump.py and
   ttCompile.py, they now behave more like "normal" command line tool,
   in that they accept multiple input files for batch processing.
-  ttDump.py and ttCompile.py don't silently override files anymore, but
   ask before doing so. Can be overridden by -f.
-  Added -d option to both ttDump.py and ttCompile.py.
-  Installation is now done with distutils. (Needs work for environments
   without compilers.)
-  Updated installation instructions.
-  Added some workarounds so as to handle certain buggy fonts more
   gracefully.
-  Updated Unicode table to Unicode 3.0 (Thanks Antoine!)
-  Included a Python script by Adam Twardoch that adds some useful stuff
   to the Windows registry.
-  Moved the project to SourceForge.

1.0a6 (released 2000-03-15)
---------------------------

-  Big reorganization: made ttLib a subpackage of the new fontTools
   package, changed several module names. Called the entire suite
   "FontTools"
-  Added several submodules to fontTools, some new, some older.
-  Added experimental CFF/GPOS/GSUB support to ttLib, read-only (but XML
   dumping of GPOS/GSUB is for now disabled)
-  Fixed hdmx endian bug
-  Added -b option to ttCompile.py, it disables recalculation of
   bounding boxes, as requested by Werner Lemberg.
-  Renamed tt2xml.pt to ttDump.py and xml2tt.py to ttCompile.py
-  Use ".ttx" as file extension instead of ".xml".
-  TTX is now the name of the XML-based *format* for TT fonts, and not
   just an application.

1.0a5
-----

Never released

-  More tables supported: hdmx, vhea, vmtx

1.0a3 & 1.0a4
-------------

Never released

-  fixed most portability issues
-  retracted the "Euro_or_currency" change from 1.0a2: it was
   nonsense!

1.0a2 (released 1999-05-02)
---------------------------

-  binary release for MacOS
-  genenates full FOND resources: including width table, PS font name
   info and kern table if applicable.
-  added cmap format 4 support. Extra: dumps Unicode char names as XML
   comments!
-  added cmap format 6 support
-  now accepts true type files starting with "true" (instead of just
   0x00010000 and "OTTO")
-  'glyf' table support is now complete: I added support for composite
   scale, xy-scale and two-by-two for the 'glyf' table. For now,
   component offset scale behaviour defaults to Apple-style. This only
   affects the (re)calculation of the glyph bounding box.
-  changed "Euro" to "Euro_or_currency" in the Standard Apple Glyph
   order list, since we cannot tell from the 'post' table which is
   meant. I should probably doublecheck with a Unicode encoding if
   available. (This does not affect the output!)

Fixed bugs: - 'hhea' table is now recalculated correctly - fixed wrong
assumption about sfnt resource names

1.0a1 (released 1999-04-27)
---------------------------

-  initial binary release for MacOS
"""^^xsd:string ;
    schema1:name "fonttools"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "4.55.4"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/fs> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3> .

<https://data.siemens-energy.com/cybersec/package/fsspec> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2022.11.0> .

<https://data.siemens-energy.com/cybersec/package/gcsfs> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2022.11.0> .

<https://data.siemens-energy.com/cybersec/package/gitpython> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3> .

<https://data.siemens-energy.com/cybersec/package/gprof2dot> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2022.7.29> .

<https://data.siemens-energy.com/cybersec/package/h2> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/5> .

<https://data.siemens-energy.com/cybersec/package/html5lib> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.1> .

<https://data.siemens-energy.com/cybersec/package/hypothesis> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/6.46.1> .

<https://data.siemens-energy.com/cybersec/package/idna> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/4> ;
    schema1:description """Internationalized Domain Names in Applications (IDNA)
=====================================================

Support for the Internationalized Domain Names in
Applications (IDNA) protocol as specified in `RFC 5891
<https://tools.ietf.org/html/rfc5891>`_. This is the latest version of
the protocol and is sometimes referred to as â€œIDNA 2008â€.

This library also provides support for Unicode Technical
Standard 46, `Unicode IDNA Compatibility Processing
<https://unicode.org/reports/tr46/>`_.

This acts as a suitable replacement for the â€œencodings.idnaâ€
module that comes with the Python standard library, but which
only supports the older superseded IDNA specification (`RFC 3490
<https://tools.ietf.org/html/rfc3490>`_).

Basic functions are simply executed:

.. code-block:: pycon

    >>> import idna
    >>> idna.encode('ãƒ‰ãƒ¡ã‚¤ãƒ³.ãƒ†ã‚¹ãƒˆ')
    b'xn--eckwd4c7c.xn--zckzah'
    >>> print(idna.decode('xn--eckwd4c7c.xn--zckzah'))
    ãƒ‰ãƒ¡ã‚¤ãƒ³.ãƒ†ã‚¹ãƒˆ


Installation
------------

This package is available for installation from PyPI:

.. code-block:: bash

    $ python3 -m pip install idna


Usage
-----

For typical usage, the ``encode`` and ``decode`` functions will take a
domain name argument and perform a conversion to A-labels or U-labels
respectively.

.. code-block:: pycon

    >>> import idna
    >>> idna.encode('ãƒ‰ãƒ¡ã‚¤ãƒ³.ãƒ†ã‚¹ãƒˆ')
    b'xn--eckwd4c7c.xn--zckzah'
    >>> print(idna.decode('xn--eckwd4c7c.xn--zckzah'))
    ãƒ‰ãƒ¡ã‚¤ãƒ³.ãƒ†ã‚¹ãƒˆ

You may use the codec encoding and decoding methods using the
``idna.codec`` module:

.. code-block:: pycon

    >>> import idna.codec
    >>> print('Ð´Ð¾Ð¼ÐµÐ½.Ð¸ÑÐ¿Ñ‹Ñ‚Ð°Ð½Ð¸Ðµ'.encode('idna2008'))
    b'xn--d1acufc.xn--80akhbyknj4f'
    >>> print(b'xn--d1acufc.xn--80akhbyknj4f'.decode('idna2008'))
    Ð´Ð¾Ð¼ÐµÐ½.Ð¸ÑÐ¿Ñ‹Ñ‚Ð°Ð½Ð¸Ðµ

Conversions can be applied at a per-label basis using the ``ulabel`` or
``alabel`` functions if necessary:

.. code-block:: pycon

    >>> idna.alabel('æµ‹è¯•')
    b'xn--0zwm56d'

Compatibility Mapping (UTS #46)
+++++++++++++++++++++++++++++++

As described in `RFC 5895 <https://tools.ietf.org/html/rfc5895>`_, the
IDNA specification does not normalize input from different potential
ways a user may input a domain name. This functionality, known as
a â€œmappingâ€, is considered by the specification to be a local
user-interface issue distinct from IDNA conversion functionality.

This library provides one such mapping that was developed by the
Unicode Consortium. Known as `Unicode IDNA Compatibility Processing
<https://unicode.org/reports/tr46/>`_, it provides for both a regular
mapping for typical applications, as well as a transitional mapping to
help migrate from older IDNA 2003 applications. Strings are
preprocessed according to Section 4.4 â€œPreprocessing for IDNA2008â€
prior to the IDNA operations.

For example, â€œKÃ¶nigsgÃ¤ÃŸchenâ€ is not a permissible label as *LATIN
CAPITAL LETTER K* is not allowed (nor are capital letters in general).
UTS 46 will convert this into lower case prior to applying the IDNA
conversion.

.. code-block:: pycon

    >>> import idna
    >>> idna.encode('KÃ¶nigsgÃ¤ÃŸchen')
    ...
    idna.core.InvalidCodepoint: Codepoint U+004B at position 1 of 'KÃ¶nigsgÃ¤ÃŸchen' not allowed
    >>> idna.encode('KÃ¶nigsgÃ¤ÃŸchen', uts46=True)
    b'xn--knigsgchen-b4a3dun'
    >>> print(idna.decode('xn--knigsgchen-b4a3dun'))
    kÃ¶nigsgÃ¤ÃŸchen

Transitional processing provides conversions to help transition from
the older 2003 standard to the current standard. For example, in the
original IDNA specification, the *LATIN SMALL LETTER SHARP S* (ÃŸ) was
converted into two *LATIN SMALL LETTER S* (ss), whereas in the current
IDNA specification this conversion is not performed.

.. code-block:: pycon

    >>> idna.encode('KÃ¶nigsgÃ¤ÃŸchen', uts46=True, transitional=True)
    'xn--knigsgsschen-lcb0w'

Implementers should use transitional processing with caution, only in
rare cases where conversion from legacy labels to current labels must be
performed (i.e. IDNA implementations that pre-date 2008). For typical
applications that just need to convert labels, transitional processing
is unlikely to be beneficial and could produce unexpected incompatible
results.

``encodings.idna`` Compatibility
++++++++++++++++++++++++++++++++

Function calls from the Python built-in ``encodings.idna`` module are
mapped to their IDNA 2008 equivalents using the ``idna.compat`` module.
Simply substitute the ``import`` clause in your code to refer to the new
module name.

Exceptions
----------

All errors raised during the conversion following the specification
should raise an exception derived from the ``idna.IDNAError`` base
class.

More specific exceptions that may be generated as ``idna.IDNABidiError``
when the error reflects an illegal combination of left-to-right and
right-to-left characters in a label; ``idna.InvalidCodepoint`` when
a specific codepoint is an illegal character in an IDN label (i.e.
INVALID); and ``idna.InvalidCodepointContext`` when the codepoint is
illegal based on its positional context (i.e. it is CONTEXTO or CONTEXTJ
but the contextual requirements are not satisfied.)

Building and Diagnostics
------------------------

The IDNA and UTS 46 functionality relies upon pre-calculated lookup
tables for performance. These tables are derived from computing against
eligibility criteria in the respective standards. These tables are
computed using the command-line script ``tools/idna-data``.

This tool will fetch relevant codepoint data from the Unicode repository
and perform the required calculations to identify eligibility. There are
three main modes:

* ``idna-data make-libdata``. Generates ``idnadata.py`` and
  ``uts46data.py``, the pre-calculated lookup tables used for IDNA and
  UTS 46 conversions. Implementers who wish to track this library against
  a different Unicode version may use this tool to manually generate a
  different version of the ``idnadata.py`` and ``uts46data.py`` files.

* ``idna-data make-table``. Generate a table of the IDNA disposition
  (e.g. PVALID, CONTEXTJ, CONTEXTO) in the format found in Appendix
  B.1 of RFC 5892 and the pre-computed tables published by `IANA
  <https://www.iana.org/>`_.

* ``idna-data U+0061``. Prints debugging output on the various
  properties associated with an individual Unicode codepoint (in this
  case, U+0061), that are used to assess the IDNA and UTS 46 status of a
  codepoint. This is helpful in debugging or analysis.

The tool accepts a number of arguments, described using ``idna-data
-h``. Most notably, the ``--version`` argument allows the specification
of the version of Unicode to be used in computing the table data. For
example, ``idna-data --version 9.0.0 make-libdata`` will generate
library data against Unicode 9.0.0.


Additional Notes
----------------

* **Packages**. The latest tagged release version is published in the
  `Python Package Index <https://pypi.org/project/idna/>`_.

* **Version support**. This library supports Python 3.6 and higher.
  As this library serves as a low-level toolkit for a variety of
  applications, many of which strive for broad compatibility with older
  Python versions, there is no rush to remove older interpreter support.
  Removing support for older versions should be well justified in that the
  maintenance burden has become too high.

* **Python 2**. Python 2 is supported by version 2.x of this library.
  Use "idna<3" in your requirements file if you need this library for
  a Python 2 application. Be advised that these versions are no longer
  actively developed.

* **Testing**. The library has a test suite based on each rule of the
  IDNA specification, as well as tests that are provided as part of the
  Unicode Technical Standard 46, `Unicode IDNA Compatibility Processing
  <https://unicode.org/reports/tr46/>`_.

* **Emoji**. It is an occasional request to support emoji domains in
  this library. Encoding of symbols like emoji is expressly prohibited by
  the technical standard IDNA 2008 and emoji domains are broadly phased
  out across the domain industry due to associated security risks. For
  now, applications that need to support these non-compliant labels
  may wish to consider trying the encode/decode operation in this library
  first, and then falling back to using `encodings.idna`. See `the Github
  project <https://github.com/kjd/idna/issues/18>`_ for more discussion.

"""^^xsd:string ;
    schema1:name "idna"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.8"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/ipywidgets> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/7.5> .

<https://data.siemens-energy.com/cybersec/package/isort> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/5.13.0> ;
    schema1:description """[![isort - isort your imports, so you don't have to.](https://raw.githubusercontent.com/pycqa/isort/main/art/logo_large.png)](https://pycqa.github.io/isort/)

------------------------------------------------------------------------

[![PyPI version](https://badge.fury.io/py/isort.svg)](https://badge.fury.io/py/isort)
[![Test Status](https://github.com/pycqa/isort/workflows/Test/badge.svg?branch=develop)](https://github.com/pycqa/isort/actions?query=workflow%3ATest)
[![Lint Status](https://github.com/pycqa/isort/workflows/Lint/badge.svg?branch=develop)](https://github.com/pycqa/isort/actions?query=workflow%3ALint)
[![Code coverage Status](https://codecov.io/gh/pycqa/isort/branch/main/graph/badge.svg)](https://codecov.io/gh/pycqa/isort)
[![License](https://img.shields.io/github/license/mashape/apistatus.svg)](https://pypi.org/project/isort/)
[![Join the chat at https://gitter.im/timothycrosley/isort](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/timothycrosley/isort?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Downloads](https://pepy.tech/badge/isort)](https://pepy.tech/project/isort)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)
[![DeepSource](https://static.deepsource.io/deepsource-badge-light-mini.svg)](https://deepsource.io/gh/pycqa/isort/?ref=repository-badge)
_________________

[Read Latest Documentation](https://pycqa.github.io/isort/) - [Browse GitHub Code Repository](https://github.com/pycqa/isort/)
_________________

isort your imports, so you don't have to.

isort is a Python utility / library to sort imports alphabetically and
automatically separate into sections and by type. It provides a command line
utility, Python library and [plugins for various
editors](https://github.com/pycqa/isort/wiki/isort-Plugins) to
quickly sort all your imports. It requires Python 3.8+ to run but
supports formatting Python 2 code too.

- [Try isort now from your browser!](https://pycqa.github.io/isort/docs/quick_start/0.-try.html)
- [Using black? See the isort and black compatibility guide.](https://pycqa.github.io/isort/docs/configuration/black_compatibility.html)
- [isort has official support for pre-commit!](https://pycqa.github.io/isort/docs/configuration/pre-commit.html)

![Example Usage](https://raw.github.com/pycqa/isort/main/example.gif)

Before isort:

```python
from my_lib import Object

import os

from my_lib import Object3

from my_lib import Object2

import sys

from third_party import lib15, lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8, lib9, lib10, lib11, lib12, lib13, lib14

import sys

from __future__ import absolute_import

from third_party import lib3

print("Hey")
print("yo")
```

After isort:

```python
from __future__ import absolute_import

import os
import sys

from third_party import (lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8,
                         lib9, lib10, lib11, lib12, lib13, lib14, lib15)

from my_lib import Object, Object2, Object3

print("Hey")
print("yo")
```

## Installing isort

Installing isort is as simple as:

```bash
pip install isort
```

## Using isort

**From the command line**:

To run on specific files:

```bash
isort mypythonfile.py mypythonfile2.py
```

To apply recursively:

```bash
isort .
```

If [globstar](https://www.gnu.org/software/bash/manual/html_node/The-Shopt-Builtin.html)
is enabled, `isort .` is equivalent to:

```bash
isort **/*.py
```

To view proposed changes without applying them:

```bash
isort mypythonfile.py --diff
```

Finally, to atomically run isort against a project, only applying
changes if they don't introduce syntax errors:

```bash
isort --atomic .
```

(Note: this is disabled by default, as it prevents isort from
running against code written using a different version of Python.)

**From within Python**:

```python
import isort

isort.file("pythonfile.py")
```

or:

```python
import isort

sorted_code = isort.code("import b\\nimport a\\n")
```

## Installing isort's for your preferred text editor

Several plugins have been written that enable to use isort from within a
variety of text-editors. You can find a full list of them [on the isort
wiki](https://github.com/pycqa/isort/wiki/isort-Plugins).
Additionally, I will enthusiastically accept pull requests that include
plugins for other text editors and add documentation for them as I am
notified.

## Multi line output modes

You will notice above the \\"multi\\_line\\_output\\" setting. This setting
defines how from imports wrap when they extend past the line\\_length
limit and has [12 possible settings](https://pycqa.github.io/isort/docs/configuration/multi_line_output_modes.html).

## Indentation

To change the how constant indents appear - simply change the
indent property with the following accepted formats:

-   Number of spaces you would like. For example: 4 would cause standard
    4 space indentation.
-   Tab
-   A verbatim string with quotes around it.

For example:

```python
"    "
```

is equivalent to 4.

For the import styles that use parentheses, you can control whether or
not to include a trailing comma after the last import with the
`include_trailing_comma` option (defaults to `False`).

## Intelligently Balanced Multi-line Imports

As of isort 3.1.0 support for balanced multi-line imports has been
added. With this enabled isort will dynamically change the import length
to the one that produces the most balanced grid, while staying below the
maximum import length defined.

Example:

```python
from __future__ import (absolute_import, division,
                        print_function, unicode_literals)
```

Will be produced instead of:

```python
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)
```

To enable this set `balanced_wrapping` to `True` in your config or pass
the `-e` option into the command line utility.

## Custom Sections and Ordering

isort provides configuration options to change almost every aspect of how
imports are organized, ordered, or grouped together in sections.

[Click here](https://pycqa.github.io/isort/docs/configuration/custom_sections_and_ordering.html) for an overview of all these options.

## Skip processing of imports (outside of configuration)

To make isort ignore a single import simply add a comment at the end of
the import line containing the text `isort:skip`:

```python
import module  # isort:skip
```

or:

```python
from xyz import (abc,  # isort:skip
                 yo,
                 hey)
```

To make isort skip an entire file simply add `isort:skip_file` to the
module's doc string:

```python
\"\"\" my_module.py
    Best module ever

   isort:skip_file
\"\"\"

import b
import a
```

## Adding or removing an import from multiple files

isort can be ran or configured to add / remove imports automatically.

[See a complete guide here.](https://pycqa.github.io/isort/docs/configuration/add_or_remove_imports.html)

## Using isort to verify code

The `--check-only` option
-------------------------

isort can also be used to verify that code is correctly formatted
by running it with `-c`. Any files that contain incorrectly sorted
and/or formatted imports will be outputted to `stderr`.

```bash
isort **/*.py -c -v

SUCCESS: /home/timothy/Projects/Open_Source/isort/isort_kate_plugin.py Everything Looks Good!
ERROR: /home/timothy/Projects/Open_Source/isort/isort/isort.py Imports are incorrectly sorted.
```

One great place this can be used is with a pre-commit git hook, such as
this one by \\@acdha:

<https://gist.github.com/acdha/8717683>

This can help to ensure a certain level of code quality throughout a
project.

## Git hook

isort provides a hook function that can be integrated into your Git
pre-commit script to check Python code before committing.

[More info here.](https://pycqa.github.io/isort/docs/configuration/git_hook.html)

## Setuptools integration

Upon installation, isort enables a `setuptools` command that checks
Python files declared by your project.

[More info here.](https://pycqa.github.io/isort/docs/configuration/setuptools_integration.html)

## Spread the word

[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)

Place this badge at the top of your repository to let others know your project uses isort.

For README.md:

```markdown
[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)
```

Or README.rst:

```rst
.. image:: https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336
    :target: https://pycqa.github.io/isort/
```

## Security contact information

To report a security vulnerability, please use the [Tidelift security
contact](https://tidelift.com/security). Tidelift will coordinate the
fix and disclosure.

## Why isort?

isort simply stands for import sort. It was originally called
"sortImports" however I got tired of typing the extra characters and
came to the realization camelCase is not pythonic.

I wrote isort because in an organization I used to work in the manager
came in one day and decided all code must have alphabetically sorted
imports. The code base was huge - and he meant for us to do it by hand.
However, being a programmer - I\\'m too lazy to spend 8 hours mindlessly
performing a function, but not too lazy to spend 16 hours automating it.
I was given permission to open source sortImports and here we are :)

------------------------------------------------------------------------

[Get professionally supported isort with the Tidelift
Subscription](https://tidelift.com/subscription/pkg/pypi-isort?utm_source=pypi-isort&utm_medium=referral&utm_campaign=readme)

Professional support for isort is available as part of the [Tidelift
Subscription](https://tidelift.com/subscription/pkg/pypi-isort?utm_source=pypi-isort&utm_medium=referral&utm_campaign=readme).
Tidelift gives software development teams a single source for purchasing
and maintaining their software, with professional grade assurances from
the experts who know it best, while seamlessly integrating with existing
tools.

------------------------------------------------------------------------

Thanks and I hope you find isort useful!

~Timothy Crosley

"""^^xsd:string ;
    schema1:name "isort"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "5.13.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/itsdangerous> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.2> ;
    schema1:description """# ItsDangerous

... so better sign this

Various helpers to pass data to untrusted environments and to get it
back safe and sound. Data is cryptographically signed to ensure that a
token has not been tampered with.

It's possible to customize how data is serialized. Data is compressed as
needed. A timestamp can be added and verified automatically while
loading a token.


## A Simple Example

Here's how you could generate a token for transmitting a user's id and
name between web requests.

```python
from itsdangerous import URLSafeSerializer
auth_s = URLSafeSerializer("secret key", "auth")
token = auth_s.dumps({"id": 5, "name": "itsdangerous"})

print(token)
# eyJpZCI6NSwibmFtZSI6Iml0c2Rhbmdlcm91cyJ9.6YP6T0BaO67XP--9UzTrmurXSmg

data = auth_s.loads(token)
print(data["name"])
# itsdangerous
```


## Donate

The Pallets organization develops and supports ItsDangerous and other
popular packages. In order to grow the community of contributors and
users, and allow the maintainers to devote more time to the projects,
[please donate today][].

[please donate today]: https://palletsprojects.com/donate

"""^^xsd:string ;
    schema1:name "itsdangerous"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.2.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/jedi> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.16> ;
    schema1:description """####################################################################################
Jedi - an awesome autocompletion, static analysis and refactoring library for Python
####################################################################################

.. image:: http://isitmaintained.com/badge/open/davidhalter/jedi.svg
    :target: https://github.com/davidhalter/jedi/issues
    :alt: The percentage of open issues and pull requests

.. image:: http://isitmaintained.com/badge/resolution/davidhalter/jedi.svg
    :target: https://github.com/davidhalter/jedi/issues
    :alt: The resolution time is the median time an issue or pull request stays open.

.. image:: https://github.com/davidhalter/jedi/workflows/ci/badge.svg?branch=master
    :target: https://github.com/davidhalter/jedi/actions
    :alt: Tests

.. image:: https://pepy.tech/badge/jedi
    :target: https://pepy.tech/project/jedi
    :alt: PyPI Downloads


Jedi is a static analysis tool for Python that is typically used in
IDEs/editors plugins. Jedi has a focus on autocompletion and goto
functionality. Other features include refactoring, code search and finding
references.

Jedi has a simple API to work with. There is a reference implementation as a
`VIM-Plugin <https://github.com/davidhalter/jedi-vim>`_. Autocompletion in your
REPL is also possible, IPython uses it natively and for the CPython REPL you
can install it. Jedi is well tested and bugs should be rare.

Jedi can currently be used with the following editors/projects:

- Vim (jedi-vim_, YouCompleteMe_, deoplete-jedi_, completor.vim_)
- `Visual Studio Code`_ (via `Python Extension <https://marketplace.visualstudio.com/items?itemName=ms-python.python>`_)
- Emacs (Jedi.el_, company-mode_, elpy_, anaconda-mode_, ycmd_)
- Sublime Text (SublimeJEDI_ [ST2 + ST3], anaconda_ [only ST3])
- TextMate_ (Not sure if it's actually working)
- Kate_ version 4.13+ supports it natively, you have to enable it, though.  [`see
  <https://projects.kde.org/projects/kde/applications/kate/repository/show?rev=KDE%2F4.13>`_]
- Atom_ (autocomplete-python-jedi_)
- `GNOME Builder`_ (with support for GObject Introspection)
- Gedit (gedi_)
- wdb_ - Web Debugger
- `Eric IDE`_
- `IPython 6.0.0+ <https://ipython.readthedocs.io/en/stable/whatsnew/version6.html>`_
- `xonsh shell <https://xon.sh/contents.html>`_ has `jedi extension <https://xon.sh/xontribs.html#jedi>`_

and many more!

There are a few language servers that use Jedi:

- `jedi-language-server <https://github.com/pappasam/jedi-language-server>`_
- `python-language-server <https://github.com/palantir/python-language-server>`_ (currently unmaintained)
- `python-lsp-server <https://github.com/python-lsp/python-lsp-server>`_ (fork from python-language-server)
- `anakin-language-server <https://github.com/muffinmad/anakin-language-server>`_

Here are some pictures taken from jedi-vim_:

.. image:: https://github.com/davidhalter/jedi/raw/master/docs/_screenshots/screenshot_complete.png

Completion for almost anything:

.. image:: https://github.com/davidhalter/jedi/raw/master/docs/_screenshots/screenshot_function.png

Documentation:

.. image:: https://github.com/davidhalter/jedi/raw/master/docs/_screenshots/screenshot_pydoc.png


Get the latest version from `github <https://github.com/davidhalter/jedi>`_
(master branch should always be kind of stable/working).

Docs are available at `https://jedi.readthedocs.org/en/latest/
<https://jedi.readthedocs.org/en/latest/>`_. Pull requests with enhancements
and/or fixes are awesome and most welcome. Jedi uses `semantic versioning
<https://semver.org/>`_.

If you want to stay **up-to-date** with releases, please **subscribe** to this
mailing list: https://groups.google.com/g/jedi-announce. To subscribe you can
simply send an empty email to ``jedi-announce+subscribe@googlegroups.com``.

Issues & Questions
==================

You can file issues and questions in the `issue tracker
<https://github.com/davidhalter/jedi/>`. Alternatively you can also ask on
`Stack Overflow <https://stackoverflow.com/questions/tagged/python-jedi>`_ with
the label ``python-jedi``.

Installation
============

`Check out the docs <https://jedi.readthedocs.org/en/latest/docs/installation.html>`_.

Features and Limitations
========================

Jedi's features are listed here:
`Features <https://jedi.readthedocs.org/en/latest/docs/features.html>`_.

You can run Jedi on Python 3.6+ but it should also
understand code that is older than those versions. Additionally you should be
able to use `Virtualenvs <https://jedi.readthedocs.org/en/latest/docs/api.html#environments>`_
very well.

Tips on how to use Jedi efficiently can be found `here
<https://jedi.readthedocs.org/en/latest/docs/features.html#recipes>`_.

API
---

You can find a comprehensive documentation for the
`API here <https://jedi.readthedocs.org/en/latest/docs/api.html>`_.

Autocompletion / Goto / Documentation
-------------------------------------

There are the following commands:

- ``jedi.Script.goto``
- ``jedi.Script.infer``
- ``jedi.Script.help``
- ``jedi.Script.complete``
- ``jedi.Script.get_references``
- ``jedi.Script.get_signatures``
- ``jedi.Script.get_context``

The returned objects are very powerful and are really all you might need.

Autocompletion in your REPL (IPython, etc.)
-------------------------------------------

Jedi is a dependency of IPython. Autocompletion in IPython with Jedi is
therefore possible without additional configuration.

Here is an `example video <https://vimeo.com/122332037>`_ how REPL completion
can look like.
For the ``python`` shell you can enable tab completion in a `REPL
<https://jedi.readthedocs.org/en/latest/docs/usage.html#tab-completion-in-the-python-shell>`_.

Static Analysis
---------------

For a lot of forms of static analysis, you can try to use
``jedi.Script(...).get_names``. It will return a list of names that you can
then filter and work with. There is also a way to list the syntax errors in a
file: ``jedi.Script.get_syntax_errors``.


Refactoring
-----------

Jedi supports the following refactorings:

- ``jedi.Script.inline``
- ``jedi.Script.rename``
- ``jedi.Script.extract_function``
- ``jedi.Script.extract_variable``

Code Search
-----------

There is support for module search with ``jedi.Script.search``, and project
search for ``jedi.Project.search``. The way to search is either by providing a
name like ``foo`` or by using dotted syntax like ``foo.bar``. Additionally you
can provide the API type like ``class foo.bar.Bar``. There are also the
functions ``jedi.Script.complete_search`` and ``jedi.Project.complete_search``.

Development
===========

There's a pretty good and extensive `development documentation
<https://jedi.readthedocs.org/en/latest/docs/development.html>`_.

Testing
=======

The test suite uses ``pytest``::

    pip install pytest

If you want to test only a specific Python version (e.g. Python 3.8), it is as
easy as::

    python3.8 -m pytest

For more detailed information visit the `testing documentation
<https://jedi.readthedocs.org/en/latest/docs/testing.html>`_.

Acknowledgements
================

Thanks a lot to all the
`contributors <https://jedi.readthedocs.org/en/latest/docs/acknowledgements.html>`_!


.. _jedi-vim: https://github.com/davidhalter/jedi-vim
.. _youcompleteme: https://github.com/ycm-core/YouCompleteMe
.. _deoplete-jedi: https://github.com/zchee/deoplete-jedi
.. _completor.vim: https://github.com/maralla/completor.vim
.. _Jedi.el: https://github.com/tkf/emacs-jedi
.. _company-mode: https://github.com/syohex/emacs-company-jedi
.. _elpy: https://github.com/jorgenschaefer/elpy
.. _anaconda-mode: https://github.com/proofit404/anaconda-mode
.. _ycmd: https://github.com/abingham/emacs-ycmd
.. _sublimejedi: https://github.com/srusskih/SublimeJEDI
.. _anaconda: https://github.com/DamnWidget/anaconda
.. _wdb: https://github.com/Kozea/wdb
.. _TextMate: https://github.com/lawrenceakka/python-jedi.tmbundle
.. _Kate: https://kate-editor.org
.. _Atom: https://atom.io/
.. _autocomplete-python-jedi: https://atom.io/packages/autocomplete-python-jedi
.. _GNOME Builder: https://wiki.gnome.org/Apps/Builder
.. _Visual Studio Code: https://code.visualstudio.com/
.. _gedi: https://github.com/isamert/gedi
.. _Eric IDE: https://eric-ide.python-projects.org


.. :changelog:

Changelog
---------

Unreleased
++++++++++

0.19.1 (2023-10-02)
+++++++++++++++++++

- Python 3.12 support (Thanks Peter!)

0.19.0 (2023-07-29)
+++++++++++++++++++

- Python 3.11 support
- Massive improvements in performance for ``Interpreter`` (e.g. IPython) users.
  This especially affects ``pandas`` users with large datasets.
- Add ``jedi.settings.allow_unsafe_interpreter_executions`` to make it easier
  for IPython users to avoid unsafe executions.

0.18.2 (2022-11-21)
+++++++++++++++++++

- Added dataclass-equivalent for attrs.define
- Find fixtures from Pytest entrypoints; Examples of pytest plugins installed
  like this are pytest-django, pytest-sugar and Faker.
- Fixed Project.search, when a venv was involved, which is why for example
  `:Pyimport django.db` did not work in some cases in jedi-vim.
- And many smaller bugfixes

0.18.1 (2021-11-17)
+++++++++++++++++++

- Implict namespaces are now a separate types in ``Name().type``
- Python 3.10 support
- Mostly bugfixes

0.18.0 (2020-12-25)
+++++++++++++++++++

- Dropped Python 2 and Python 3.5
- Using ``pathlib.Path()`` as an output instead of ``str`` in most places:
  - ``Project.path``
  - ``Script.path``
  - ``Definition.module_path``
  - ``Refactoring.get_renames``
  - ``Refactoring.get_changed_files``
- Functions with ``@property`` now return ``property`` instead of ``function``
  in ``Name().type``
- Started using annotations
- Better support for the walrus operator
- Project attributes are now read accessible
- Removed all deprecations

This is likely going to be the last minor release before 1.0.

0.17.2 (2020-07-17)
+++++++++++++++++++

- Added an option to pass environment variables to ``Environment``
- ``Project(...).path`` exists now
- Support for Python 3.9
- A few bugfixes

This will be the last release that supports Python 2 and Python 3.5.
``0.18.0`` will be Python 3.6+.

0.17.1 (2020-06-20)
+++++++++++++++++++

- Django ``Model`` meta class support
- Django Manager support (completion on Managers/QuerySets)
- Added Django Stubs to Jedi, thanks to all contributors of the
  `Django Stubs <https://github.com/typeddjango/django-stubs>`_ project
- Added ``SyntaxError.get_message``
- Python 3.9 support
- Bugfixes (mostly towards Generics)

0.17.0 (2020-04-14)
+++++++++++++++++++

- Added ``Project`` support. This allows a user to specify which folders Jedi
  should work with.
- Added support for Refactoring. The following refactorings have been
  implemented: ``Script.rename``, ``Script.inline``,
  ``Script.extract_variable`` and ``Script.extract_function``.
- Added ``Script.get_syntax_errors`` to display syntax errors in the current
  script.
- Added code search capabilities both for individual files and projects. The
  new functions are ``Project.search``, ``Project.complete_search``,
  ``Script.search`` and ``Script.complete_search``.
- Added ``Script.help`` to make it easier to display a help window to people.
  Now returns pydoc information as well for Python keywords/operators.  This
  means that on the class keyword it will now return the docstring of Python's
  builtin function ``help('class')``.
- The API documentation is now way more readable and complete. Check it out
  under https://jedi.readthedocs.io. A lot of it has been rewritten.
- Removed Python 3.4 support
- Many bugfixes

This is likely going to be the last minor version that supports Python 2 and
Python3.5. Bugfixes will be provided in 0.17.1+. The next minor/major version
will probably be Jedi 1.0.0.

0.16.0 (2020-01-26)
+++++++++++++++++++

- **Added** ``Script.get_context`` to get information where you currently are.
- Completions/type inference of **Pytest fixtures**.
- Tensorflow, Numpy and Pandas completions should now be about **4-10x faster**
  after the first time they are used.
- Dict key completions are working now. e.g. ``d = {1000: 3}; d[10`` will
  expand to ``1000``.
- Completion for "proxies" works now. These are classes that have a
  ``__getattr__(self, name)`` method that does a ``return getattr(x, name)``.
  after loading them initially.
- Goto on a function/attribute in a class now goes to the definition in its
  super class.
- Big **Script API Changes**:
    - The line and column parameters of ``jedi.Script`` are now deprecated
    - ``completions`` deprecated, use ``complete`` instead
    - ``goto_assignments`` deprecated, use ``goto`` instead
    - ``goto_definitions`` deprecated, use ``infer`` instead
    - ``call_signatures`` deprecated, use ``get_signatures`` instead
    - ``usages`` deprecated, use ``get_references`` instead
    - ``jedi.names`` deprecated, use ``jedi.Script(...).get_names()``
- ``BaseName.goto_assignments`` renamed to ``BaseName.goto``
- Add follow_imports to ``Name.goto``. Now its signature matches
  ``Script.goto``.
- **Python 2 support deprecated**. For this release it is best effort. Python 2
  has reached the end of its life and now it's just about a smooth transition.
  Bugs for Python 2 will not be fixed anymore and a third of the tests are
  already skipped.
- Removed ``settings.no_completion_duplicates``. It wasn't tested and nobody
  was probably using it anyway.
- Removed ``settings.use_filesystem_cache`` and
  ``settings.additional_dynamic_modules``, they have no usage anymore. Pretty
  much nobody was probably using them.

0.15.2 (2019-12-20)
+++++++++++++++++++

- Signatures are now detected a lot better
- Add fuzzy completions with ``Script(...).completions(fuzzy=True)``
- Files bigger than one MB (about 20kLOC) get cropped to avoid getting
  stuck completely.
- Many small Bugfixes
- A big refactoring around contexts/values

0.15.1 (2019-08-13)
+++++++++++++++++++

- Small bugfix and removal of a print statement

0.15.0 (2019-08-11)
+++++++++++++++++++

- Added file path completions, there's a **new** ``Completion.type`` now:
  ``path``. Example: ``'/ho`` -> ``'/home/``
- ``*args``/``**kwargs`` resolving. If possible Jedi replaces the parameters
  with the actual alternatives.
- Better support for enums/dataclasses
- When using Interpreter, properties are now executed, since a lot of people
  have complained about this. Discussion in #1299, #1347.

New APIs:

- ``Name.get_signatures() -> List[Signature]``. Signatures are similar to
  ``CallSignature``. ``Name.params`` is therefore deprecated.
- ``Signature.to_string()`` to format signatures.
- ``Signature.params -> List[ParamName]``, ParamName has the
  following additional attributes ``infer_default()``, ``infer_annotation()``,
  ``to_string()``, and ``kind``.
- ``Name.execute() -> List[Name]``, makes it possible to infer
  return values of functions.


0.14.1 (2019-07-13)
+++++++++++++++++++

- CallSignature.index should now be working a lot better
- A couple of smaller bugfixes

0.14.0 (2019-06-20)
+++++++++++++++++++

- Added ``goto_*(prefer_stubs=True)`` as well as ``goto_*(prefer_stubs=True)``
- Stubs are used now for type inference
- Typeshed is used for better type inference
- Reworked Name.full_name, should have more correct return values

0.13.3 (2019-02-24)
+++++++++++++++++++

- Fixed an issue with embedded Python, see https://github.com/davidhalter/jedi-vim/issues/870

0.13.2 (2018-12-15)
+++++++++++++++++++

- Fixed a bug that led to Jedi spawning a lot of subprocesses.

0.13.1 (2018-10-02)
+++++++++++++++++++

- Bugfixes, because tensorflow completions were still slow.

0.13.0 (2018-10-02)
+++++++++++++++++++

- A small release. Some bug fixes.
- Remove Python 3.3 support. Python 3.3 support has been dropped by the Python
  foundation.
- Default environments are now using the same Python version as the Python
  process. In 0.12.x, we used to load the latest Python version on the system.
- Added ``include_builtins`` as a parameter to usages.
- ``goto_assignments`` has a new ``follow_builtin_imports`` parameter that
  changes the previous behavior slightly.

0.12.1 (2018-06-30)
+++++++++++++++++++

- This release forces you to upgrade parso. If you don't, nothing will work
  anymore. Otherwise changes should be limited to bug fixes. Unfortunately Jedi
  still uses a few internals of parso that make it hard to keep compatibility
  over multiple releases. Parso >=0.3.0 is going to be needed.

0.12.0 (2018-04-15)
+++++++++++++++++++

- Virtualenv/Environment support
- F-String Completion/Goto Support
- Cannot crash with segfaults anymore
- Cleaned up import logic
- Understand async/await and autocomplete it (including async generators)
- Better namespace completions
- Passing tests for Windows (including CI for Windows)
- Remove Python 2.6 support

0.11.1 (2017-12-14)
+++++++++++++++++++

- Parso update - the caching layer was broken
- Better usages - a lot of internal code was ripped out and improved.

0.11.0 (2017-09-20)
+++++++++++++++++++

- Split Jedi's parser into a separate project called ``parso``.
- Avoiding side effects in REPL completion.
- Numpy docstring support should be much better.
- Moved the `settings.*recursion*` away, they are no longer usable.

0.10.2 (2017-04-05)
+++++++++++++++++++

- Python Packaging sucks. Some files were not included in 0.10.1.

0.10.1 (2017-04-05)
+++++++++++++++++++

- Fixed a few very annoying bugs.
- Prepared the parser to be factored out of Jedi.

0.10.0 (2017-02-03)
+++++++++++++++++++

- Actual semantic completions for the complete Python syntax.
- Basic type inference for ``yield from`` PEP 380.
- PEP 484 support (most of the important features of it). Thanks Claude! (@reinhrst)
- Added ``get_line_code`` to ``Name`` and ``Completion`` objects.
- Completely rewritten the type inference engine.
- A new and better parser for (fast) parsing diffs of Python code.

0.9.0 (2015-04-10)
++++++++++++++++++

- The import logic has been rewritten to look more like Python's. There is now
  an ``InferState.modules`` import cache, which resembles ``sys.modules``.
- Integrated the parser of 2to3. This will make refactoring possible. It will
  also be possible to check for error messages (like compiling an AST would give)
  in the future.
- With the new parser, the type inference also completely changed. It's now
  simpler and more readable.
- Completely rewritten REPL completion.
- Added ``jedi.names``, a command to do static analysis. Thanks to that
  sourcegraph guys for sponsoring this!
- Alpha version of the linter.


0.8.1 (2014-07-23)
+++++++++++++++++++

- Bugfix release, the last release forgot to include files that improve
  autocompletion for builtin libraries. Fixed.

0.8.0 (2014-05-05)
+++++++++++++++++++

- Memory Consumption for compiled modules (e.g. builtins, sys) has been reduced
  drastically. Loading times are down as well (it takes basically as long as an
  import).
- REPL completion is starting to become usable.
- Various small API changes. Generally this release focuses on stability and
  refactoring of internal APIs.
- Introducing operator precedence, which makes calculating correct Array
  indices and ``__getattr__`` strings possible.

0.7.0 (2013-08-09)
++++++++++++++++++

- Switched from LGPL to MIT license.
- Added an Interpreter class to the API to make autocompletion in REPL
  possible.
- Added autocompletion support for namespace packages.
- Add sith.py, a new random testing method.

0.6.0 (2013-05-14)
++++++++++++++++++

- Much faster parser with builtin part caching.
- A test suite, thanks @tkf.

0.5 versions (2012)
+++++++++++++++++++

- Initial development.


"""^^xsd:string ;
    schema1:name "jedi"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.19.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/jsonpointer> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.13> ;
    schema1:description """python-json-pointer
===================

[![PyPI version](https://img.shields.io/pypi/v/jsonpointer.svg)](https://pypi.python.org/pypi/jsonpointer/)
[![Supported Python versions](https://img.shields.io/pypi/pyversions/jsonpointer.svg)](https://pypi.python.org/pypi/jsonpointer/)
[![Coverage Status](https://coveralls.io/repos/stefankoegl/python-json-pointer/badge.svg?branch=master)](https://coveralls.io/r/stefankoegl/python-json-pointer?branch=master)


Resolve JSON Pointers in Python
-------------------------------

Library to resolve JSON Pointers according to
[RFC 6901](http://tools.ietf.org/html/rfc6901)

See source code for examples
* Website: https://github.com/stefankoegl/python-json-pointer
* Repository: https://github.com/stefankoegl/python-json-pointer.git
* Documentation: https://python-json-pointer.readthedocs.org/
* PyPI: https://pypi.python.org/pypi/jsonpointer
* Travis CI: https://travis-ci.org/stefankoegl/python-json-pointer
* Coveralls: https://coveralls.io/r/stefankoegl/python-json-pointer
"""^^xsd:string ;
    schema1:name "jsonpointer"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.0.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/jsonschema> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/attrs>,
        <https://data.siemens-energy.com/cybersec/package/importlib-resources>,
        <https://data.siemens-energy.com/cybersec/package/jsonpointer>,
        <https://data.siemens-energy.com/cybersec/package/jsonschema-specifications>,
        <https://data.siemens-energy.com/cybersec/package/pkgutil-resolve-name>,
        <https://data.siemens-energy.com/cybersec/package/referencing>,
        <https://data.siemens-energy.com/cybersec/package/rfc3986-validator>,
        <https://data.siemens-energy.com/cybersec/package/rpds-py>,
        <https://data.siemens-energy.com/cybersec/package/webcolors> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.6> ;
    schema1:description """==========
jsonschema
==========

|PyPI| |Pythons| |CI| |ReadTheDocs| |Precommit| |Zenodo|

.. |PyPI| image:: https://img.shields.io/pypi/v/jsonschema.svg
   :alt: PyPI version
   :target: https://pypi.org/project/jsonschema/

.. |Pythons| image:: https://img.shields.io/pypi/pyversions/jsonschema.svg
   :alt: Supported Python versions
   :target: https://pypi.org/project/jsonschema/

.. |CI| image:: https://github.com/python-jsonschema/jsonschema/workflows/CI/badge.svg
  :alt: Build status
  :target: https://github.com/python-jsonschema/jsonschema/actions?query=workflow%3ACI

.. |ReadTheDocs| image:: https://readthedocs.org/projects/python-jsonschema/badge/?version=stable&style=flat
   :alt: ReadTheDocs status
   :target: https://python-jsonschema.readthedocs.io/en/stable/

.. |Precommit| image:: https://results.pre-commit.ci/badge/github/python-jsonschema/jsonschema/main.svg
   :alt: pre-commit.ci status
   :target: https://results.pre-commit.ci/latest/github/python-jsonschema/jsonschema/main

.. |Zenodo| image:: https://zenodo.org/badge/3072629.svg
   :alt: Zenodo DOI
   :target: https://zenodo.org/badge/latestdoi/3072629


``jsonschema`` is an implementation of the `JSON Schema <https://json-schema.org>`_ specification for Python.

.. code:: python

    >>> from jsonschema import validate

    >>> # A sample schema, like what we'd get from json.load()
    >>> schema = {
    ...     "type" : "object",
    ...     "properties" : {
    ...         "price" : {"type" : "number"},
    ...         "name" : {"type" : "string"},
    ...     },
    ... }

    >>> # If no exception is raised by validate(), the instance is valid.
    >>> validate(instance={"name" : "Eggs", "price" : 34.99}, schema=schema)

    >>> validate(
    ...     instance={"name" : "Eggs", "price" : "Invalid"}, schema=schema,
    ... )                                   # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValidationError: 'Invalid' is not of type 'number'

It can also be used from the command line by installing `check-jsonschema <https://github.com/python-jsonschema/check-jsonschema>`_.

Features
--------

* Full support for `Draft 2020-12 <https://python-jsonschema.readthedocs.io/en/latest/api/jsonschema/validators/#jsonschema.validators.Draft202012Validator>`_, `Draft 2019-09 <https://python-jsonschema.readthedocs.io/en/latest/api/jsonschema/validators/#jsonschema.validators.Draft201909Validator>`_, `Draft 7 <https://python-jsonschema.readthedocs.io/en/latest/api/jsonschema/validators/#jsonschema.validators.Draft7Validator>`_, `Draft 6 <https://python-jsonschema.readthedocs.io/en/latest/api/jsonschema/validators/#jsonschema.validators.Draft6Validator>`_, `Draft 4 <https://python-jsonschema.readthedocs.io/en/latest/api/jsonschema/validators/#jsonschema.validators.Draft4Validator>`_ and `Draft 3 <https://python-jsonschema.readthedocs.io/en/latest/api/jsonschema/validators/#jsonschema.validators.Draft3Validator>`_

* `Lazy validation <https://python-jsonschema.readthedocs.io/en/latest/api/jsonschema/protocols/#jsonschema.protocols.Validator.iter_errors>`_ that can iteratively report *all* validation errors.

* `Programmatic querying <https://python-jsonschema.readthedocs.io/en/latest/errors/>`_ of which properties or items failed validation.


Installation
------------

``jsonschema`` is available on `PyPI <https://pypi.org/project/jsonschema/>`_. You can install using `pip <https://pip.pypa.io/en/stable/>`_:

.. code:: bash

    $ pip install jsonschema


Extras
======

Two extras are available when installing the package, both currently related to ``format`` validation:

    * ``format``
    * ``format-nongpl``

They can be used when installing in order to include additional dependencies, e.g.:

.. code:: bash

    $ pip install jsonschema'[format]'

Be aware that the mere presence of these dependencies â€“ or even the specification of ``format`` checks in a schema â€“ do *not* activate format checks (as per the specification).
Please read the `format validation documentation <https://python-jsonschema.readthedocs.io/en/latest/validate/#validating-formats>`_ for further details.

About
-----

I'm Julian Berman.

``jsonschema`` is on `GitHub <https://github.com/python-jsonschema/jsonschema>`_.

Get in touch, via GitHub or otherwise, if you've got something to contribute, it'd be most welcome!

You can also generally find me on Libera (nick: ``Julian``) in various channels, including ``#python``.

If you feel overwhelmingly grateful, you can also `sponsor me <https://github.com/sponsors/Julian/>`_.

And for companies who appreciate ``jsonschema`` and its continued support and growth, ``jsonschema`` is also now supportable via `TideLift <https://tidelift.com/subscription/pkg/pypi-jsonschema?utm_source=pypi-jsonschema&utm_medium=referral&utm_campaign=readme>`_.


Release Information
-------------------

v4.23.0
=======

* Do not reorder dictionaries (schemas, instances) that are printed as part of validation errors.
* Declare support for Py3.13
"""^^xsd:string ;
    schema1:name "jsonschema"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "4.23.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/jsonschema-specifications> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/importlib-resources>,
        <https://data.siemens-energy.com/cybersec/package/referencing> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2023.03.6> ;
    schema1:description """=============================
``jsonschema-specifications``
=============================

|PyPI| |Pythons| |CI| |ReadTheDocs|

JSON support files from the `JSON Schema Specifications <https://json-schema.org/specification.html>`_ (metaschemas, vocabularies, etc.), packaged for runtime access from Python as a `referencing-based Schema Registry <https://referencing.readthedocs.io/en/stable/api/#referencing.Registry>`_.

.. |PyPI| image:: https://img.shields.io/pypi/v/jsonschema-specifications.svg
  :alt: PyPI version
  :target: https://pypi.org/project/jsonschema-specifications/

.. |Pythons| image:: https://img.shields.io/pypi/pyversions/jsonschema-specifications.svg
  :alt: Supported Python versions
  :target: https://pypi.org/project/jsonschema-specifications/

.. |CI| image:: https://github.com/python-jsonschema/jsonschema-specifications/workflows/CI/badge.svg
  :alt: Build status
  :target: https://github.com/python-jsonschema/jsonschema-specifications/actions?query=workflow%3ACI

.. |ReadTheDocs| image:: https://readthedocs.org/projects/jsonschema-specifications/badge/?version=stable&style=flat
  :alt: ReadTheDocs status
  :target: https://jsonschema-specifications.readthedocs.io/en/stable/
"""^^xsd:string ;
    schema1:name "jsonschema-specifications"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2023.12.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/jupyter-client> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/6.1.12> .

<https://data.siemens-energy.com/cybersec/package/kiwisolver> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.3.1> ;
    schema1:description """Welcome to Kiwi
===============

.. image:: https://github.com/nucleic/kiwi/workflows/Continuous%20Integration/badge.svg
    :target: https://github.com/nucleic/kiwi/actions
.. image:: https://github.com/nucleic/kiwi/workflows/Documentation%20building/badge.svg
    :target: https://github.com/nucleic/kiwi/actions
.. image:: https://codecov.io/gh/nucleic/kiwi/branch/main/graph/badge.svg
  :target: https://codecov.io/gh/nucleic/kiwi
.. image:: https://readthedocs.org/projects/kiwisolver/badge/?version=latest
    :target: https://kiwisolver.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status

Kiwi is an efficient C++ implementation of the Cassowary constraint solving
algorithm. Kiwi is an implementation of the algorithm based on the
`seminal Cassowary paper <https://constraints.cs.washington.edu/solvers/cassowary-tochi.pdf>`_.
It is *not* a refactoring of the original C++ solver. Kiwi has been designed
from the ground up to be lightweight and fast. Kiwi ranges from 10x to 500x
faster than the original Cassowary solver with typical use cases gaining a 40x
improvement. Memory savings are consistently > 5x.

In addition to the C++ solver, Kiwi ships with hand-rolled Python bindings for
Python 3.7+.
"""^^xsd:string ;
    schema1:name "kiwisolver"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.4.8"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/lz4> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.7.4.2> .

<https://data.siemens-energy.com/cybersec/package/markupsafe> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.0> .

<https://data.siemens-energy.com/cybersec/package/mccabe> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.8> ;
    schema1:description """McCabe complexity checker
=========================

Ned's script to check McCabe complexity.

This module provides a plugin for ``flake8``, the Python code checker.


Installation
------------

You can install, upgrade, or uninstall ``mccabe`` with these commands::

  $ pip install mccabe
  $ pip install --upgrade mccabe
  $ pip uninstall mccabe


Standalone script
-----------------

The complexity checker can be used directly::

  $ python -m mccabe --min 5 mccabe.py
  ("185:1: 'PathGraphingAstVisitor.visitIf'", 5)
  ("71:1: 'PathGraph.to_dot'", 5)
  ("245:1: 'McCabeChecker.run'", 5)
  ("283:1: 'main'", 7)
  ("203:1: 'PathGraphingAstVisitor.visitTryExcept'", 5)
  ("257:1: 'get_code_complexity'", 5)


Plugin for Flake8
-----------------

When both ``flake8 2+`` and ``mccabe`` are installed, the plugin is
available in ``flake8``::

  $ flake8 --version
  2.0 (pep8: 1.4.2, pyflakes: 0.6.1, mccabe: 0.2)

By default the plugin is disabled.  Use the ``--max-complexity`` switch to
enable it.  It will emit a warning if the McCabe complexity of a function is
higher than the provided value::

    $ flake8 --max-complexity 10 coolproject
    ...
    coolproject/mod.py:1204:1: C901 'CoolFactory.prepare' is too complex (14)

This feature is quite useful for detecting over-complex code.  According to McCabe,
anything that goes beyond 10 is too complex.

Flake8 has many features that mccabe does not provide. Flake8 allows users to
ignore violations reported by plugins with ``# noqa``. Read more about this in
`their documentation
<http://flake8.pycqa.org/en/latest/user/violations.html#in-line-ignoring-errors>`__.
To silence violations reported by ``mccabe``, place your ``# noqa: C901`` on
the function definition line, where the error is reported for (possibly a
decorator).


Links
-----

* Feedback and ideas: http://mail.python.org/mailman/listinfo/code-quality

* Cyclomatic complexity: http://en.wikipedia.org/wiki/Cyclomatic_complexity

* Ned Batchelder's script:
  http://nedbatchelder.com/blog/200803/python_code_complexity_microtool.html

* McCabe complexity: http://en.wikipedia.org/wiki/Cyclomatic_complexity


Changes
-------

0.7.0 - 2021-01-23
``````````````````

* Drop support for all versions of Python lower than 3.6

* Add support for Python 3.8, 3.9, and 3.10

* Fix option declaration for Flake8

0.6.1 - 2017-01-26
``````````````````

* Fix signature for ``PathGraphingAstVisitor.default`` to match the signature
  for ``ASTVisitor``

0.6.0 - 2017-01-23
``````````````````

* Add support for Python 3.6

* Fix handling for missing statement types

0.5.3 - 2016-12-14
``````````````````

* Report actual column number of violation instead of the start of the line

0.5.2 - 2016-07-31
``````````````````

* When opening files ourselves, make sure we always name the file variable

0.5.1 - 2016-07-28
``````````````````

* Set default maximum complexity to -1 on the class itself

0.5.0 - 2016-05-30
``````````````````

* PyCon 2016 PDX release

* Add support for Flake8 3.0

0.4.0 - 2016-01-27
``````````````````

* Stop testing on Python 3.2

* Add support for async/await keywords on Python 3.5 from PEP 0492

0.3.1 - 2015-06-14
``````````````````

* Include ``test_mccabe.py`` in releases.

* Always coerce the ``max_complexity`` value from Flake8's entry-point to an
  integer.

0.3 - 2014-12-17
````````````````

* Computation was wrong: the mccabe complexity starts at 1, not 2.

* The ``max-complexity`` value is now inclusive.  E.g.: if the
  value is 10 and the reported complexity is 10, then it passes.

* Add tests.


0.2.1 - 2013-04-03
``````````````````

* Do not require ``setuptools`` in setup.py.  It works around an issue
  with ``pip`` and Python 3.


0.2 - 2013-02-22
````````````````

* Rename project to ``mccabe``.

* Provide ``flake8.extension`` setuptools entry point.

* Read ``max-complexity`` from the configuration file.

* Rename argument ``min_complexity`` to ``threshold``.


0.1 - 2013-02-11
````````````````
* First release


"""^^xsd:string ;
    schema1:name "mccabe"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.7.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/meson-python> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.17.0> .

<https://data.siemens-energy.com/cybersec/package/mistune> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/4> ;
    schema1:description """Mistune v3
==========

A fast yet powerful Python Markdown parser with renderers and plugins.

Overview
--------

Convert Markdown to HTML with ease:

.. code-block:: python

    import mistune
    mistune.html(your_markdown_text)

Useful Links
------------

1. GitHub: https://github.com/lepture/mistune
2. Docs: https://mistune.lepture.com/

License
-------

Mistune is licensed under BSD. Please see LICENSE for licensing details.
"""^^xsd:string ;
    schema1:name "mistune"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.0.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/more-itertools> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/8.8> .

<https://data.siemens-energy.com/cybersec/package/nbclient> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/ipykernel>,
        <https://data.siemens-energy.com/cybersec/package/jupyter-client>,
        <https://data.siemens-energy.com/cybersec/package/jupyter-core>,
        <https://data.siemens-energy.com/cybersec/package/nbconvert>,
        <https://data.siemens-energy.com/cybersec/package/nbformat>,
        <https://data.siemens-energy.com/cybersec/package/pytest>,
        <https://data.siemens-energy.com/cybersec/package/pytest-cov>,
        <https://data.siemens-energy.com/cybersec/package/sphinx>,
        <https://data.siemens-energy.com/cybersec/package/traitlets> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.5.0> ;
    schema1:description """[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jupyter/nbclient/main?filepath=binder%2Frun_nbclient.ipynb)
[![Build Status](https://github.com/jupyter/nbclient/workflows/CI/badge.svg)](https://github.com/jupyter/nbclient/actions)
[![Documentation Status](https://readthedocs.org/projects/nbclient/badge/?version=latest)](https://nbclient.readthedocs.io/en/latest/?badge=latest)
[![Python 3.7](https://img.shields.io/badge/python-3.7-blue.svg)](https://www.python.org/downloads/release/python-370/)
[![Python 3.8](https://img.shields.io/badge/python-3.8-blue.svg)](https://www.python.org/downloads/release/python-380/)
[![Python 3.9](https://img.shields.io/badge/python-3.9-blue.svg)](https://www.python.org/downloads/release/python-390/)
[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/release/python-3110/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)

# nbclient

**NBClient** lets you **execute** notebooks.

A client library for programmatic notebook execution, **NBClient** is a tool for running Jupyter Notebooks in
different execution contexts, including the command line.

## Interactive Demo

To demo **NBClient** interactively, click this Binder badge to start the demo:

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jupyter/nbclient/main?filepath=binder%2Frun_nbclient.ipynb)

## Installation

In a terminal, run:

```
python3 -m pip install nbclient
```

## Documentation

See [ReadTheDocs](https://nbclient.readthedocs.io/en/latest/) for more in-depth details about the project and the
[API Reference](https://nbclient.readthedocs.io/en/latest/reference/index.html).

## Python Version Support

This library currently supports Python 3.6+ versions. As minor Python
versions are officially sunset by the Python org, nbclient will similarly
drop support in the future.

## Origins

This library used to be part of the [nbconvert](https://nbconvert.readthedocs.io/en/latest/) project. NBClient extracted nbconvert's `ExecutePreprocessor`into its own library for easier updating and importing by downstream libraries and applications.

## Relationship to JupyterClient

NBClient and JupyterClient are distinct projects.

`jupyter_client` is a client library for the jupyter protocol. Specifically, `jupyter_client` provides the Python API
for starting, managing and communicating with Jupyter kernels.

While, nbclient allows notebooks to be run in different execution contexts.

## About the Jupyter Development Team

The Jupyter Development Team is the set of all contributors to the Jupyter project.
This includes all of the Jupyter subprojects.

The core team that coordinates development on GitHub can be found here:
https://github.com/jupyter/.

## Our Copyright Policy

Jupyter uses a shared copyright model. Each contributor maintains copyright
over their contributions to Jupyter. But, it is important to note that these
contributions are typically only changes to the repositories. Thus, the Jupyter
source code, in its entirety is not the copyright of any single person or
institution.  Instead, it is the collective copyright of the entire Jupyter
Development Team.  If individual contributors want to maintain a record of what
changes/contributions they have specific copyright on, they should indicate
their copyright in the commit message of the change, when they commit the
change to one of the Jupyter repositories.

With this in mind, the following banner should be used in any source code file
to indicate the copyright and license terms:

```
# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.
```
"""^^xsd:string ;
    schema1:name "nbclient"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.10.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/nbconvert> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/bleach>,
        <https://data.siemens-energy.com/cybersec/package/importlib-metadata>,
        <https://data.siemens-energy.com/cybersec/package/ipywidgets>,
        <https://data.siemens-energy.com/cybersec/package/jinja2>,
        <https://data.siemens-energy.com/cybersec/package/jupyter-core>,
        <https://data.siemens-energy.com/cybersec/package/markupsafe>,
        <https://data.siemens-energy.com/cybersec/package/mistune>,
        <https://data.siemens-energy.com/cybersec/package/nbclient>,
        <https://data.siemens-energy.com/cybersec/package/nbformat>,
        <https://data.siemens-energy.com/cybersec/package/nbsphinx>,
        <https://data.siemens-energy.com/cybersec/package/pandocfilters>,
        <https://data.siemens-energy.com/cybersec/package/pygments>,
        <https://data.siemens-energy.com/cybersec/package/pyqtwebengine>,
        <https://data.siemens-energy.com/cybersec/package/pytest>,
        <https://data.siemens-energy.com/cybersec/package/sphinx>,
        <https://data.siemens-energy.com/cybersec/package/tornado>,
        <https://data.siemens-energy.com/cybersec/package/traitlets> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/7.0.0> ;
    schema1:description """# nbconvert

### Jupyter Notebook Conversion

[![Build Status](https://github.com/jupyter/nbconvert/actions/workflows/tests.yml/badge.svg?query=branch%3Amain++)](https://github.com/jupyter/nbconvert/actions/workflows/tests.yml/badge.svg?query=branch%3Amain++)
[![Documentation Status](https://readthedocs.org/projects/nbconvert/badge/?version=latest)](https://nbconvert.readthedocs.io/en/latest/?badge=latest)

The **nbconvert** tool, `jupyter nbconvert`, converts notebooks to various other
formats via [Jinja] templates. The nbconvert tool allows you to convert an
`.ipynb` notebook file into various static formats including:

- HTML
- LaTeX
- PDF
- Reveal JS
- Markdown (md)
- ReStructured Text (rst)
- executable script

## Usage

From the command line, use nbconvert to convert a Jupyter notebook (_input_) to a
a different format (_output_). The basic command structure is:

```
$ jupyter nbconvert --to <output format> <input notebook>
```

where `<output format>` is the desired output format and `<input notebook>` is the
filename of the Jupyter notebook.

### Example: Convert a notebook to HTML

Convert Jupyter notebook file, `mynotebook.ipynb`, to HTML using:

```
$ jupyter nbconvert --to html mynotebook.ipynb
```

This command creates an HTML output file named `mynotebook.html`.

## Dev Install

Check if pandoc is installed (`pandoc --version`); if needed, install:

```
sudo apt-get install pandoc
```

Or

```
brew install pandoc
```

Install nbconvert for development using:

```
git clone https://github.com/jupyter/nbconvert.git
cd nbconvert
pip install -e .
```

Running the tests after a dev install above:

```
pip install nbconvert[test]
py.test --pyargs nbconvert
```

## Documentation

- [Documentation for Jupyter nbconvert](https://nbconvert.readthedocs.io/en/latest/)
- [nbconvert examples on GitHub](https://github.com/jupyter/nbconvert-examples)
- [Documentation for Project Jupyter](https://jupyter.readthedocs.io/en/latest/index.html)

## Technical Support

- [Issues and Bug Reports](https://github.com/jupyter/nbconvert/issues): A place to report
  bugs or regressions found for nbconvert
- [Community Technical Support and Discussion - Discourse](https://discourse.jupyter.org/): A place for
  installation, configuration, and troubleshooting assistannce by the Jupyter community.
  As a non-profit project and maintainers who are primarily volunteers, we encourage you
  to ask questions and share your knowledge on Discourse.

## Jupyter Resources

- [Jupyter mailing list](https://groups.google.com/forum/#!forum/jupyter)
- [Project Jupyter website](https://jupyter.org)

## About the Jupyter Development Team

The Jupyter Development Team is the set of all contributors to the Jupyter project.
This includes all of the Jupyter subprojects.

The core team that coordinates development on GitHub can be found here:
https://github.com/jupyter/.

## Our Copyright Policy

Jupyter uses a shared copyright model. Each contributor maintains copyright
over their contributions to Jupyter. But, it is important to note that these
contributions are typically only changes to the repositories. Thus, the Jupyter
source code, in its entirety is not the copyright of any single person or
institution.  Instead, it is the collective copyright of the entire Jupyter
Development Team.  If individual contributors want to maintain a record of what
changes/contributions they have specific copyright on, they should indicate
their copyright in the commit message of the change, when they commit the
change to one of the Jupyter repositories.

With this in mind, the following banner should be used in any source code file
to indicate the copyright and license terms:

```
# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.
```

[jinja]: http://jinja.pocoo.org/
"""^^xsd:string ;
    schema1:name "nbconvert"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "7.16.4"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/nbsphinx> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.2.12> .

<https://data.siemens-energy.com/cybersec/package/numba> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.56.4> .

<https://data.siemens-energy.com/cybersec/package/numexpr> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.8.4> .

<https://data.siemens-energy.com/cybersec/package/objgraph> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.7.2> .

<https://data.siemens-energy.com/cybersec/package/odfpy> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.4.1> .

<https://data.siemens-energy.com/cybersec/package/openpyxl> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.1.0> ;
    schema1:description """.. image:: https://coveralls.io/repos/bitbucket/openpyxl/openpyxl/badge.svg?branch=default
    :target: https://coveralls.io/bitbucket/openpyxl/openpyxl?branch=default
    :alt: coverage status

Introduction
------------

openpyxl is a Python library to read/write Excel 2010 xlsx/xlsm/xltx/xltm files.

It was born from lack of existing library to read/write natively from Python
the Office Open XML format.

All kudos to the PHPExcel team as openpyxl was initially based on PHPExcel.


Security
--------

By default openpyxl does not guard against quadratic blowup or billion laughs
xml attacks. To guard against these attacks install defusedxml.

Mailing List
------------

The user list can be found on http://groups.google.com/group/openpyxl-users


Sample code::

    from openpyxl import Workbook
    wb = Workbook()

    # grab the active worksheet
    ws = wb.active

    # Data can be assigned directly to cells
    ws['A1'] = 42

    # Rows can also be appended
    ws.append([1, 2, 3])

    # Python types will automatically be converted
    import datetime
    ws['A2'] = datetime.datetime.now()

    # Save the file
    wb.save("sample.xlsx")


Documentation
-------------

The documentation is at: https://openpyxl.readthedocs.io

* installation methods
* code examples
* instructions for contributing

Release notes: https://openpyxl.readthedocs.io/en/stable/changes.html
"""^^xsd:string ;
    schema1:name "openpyxl"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.1.5"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pandas-gbq> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.19.0> .

<https://data.siemens-energy.com/cybersec/package/pandocfilters> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.4.1> ;
    schema1:description """pandocfilters
=============

A python module for writing `pandoc <http://pandoc.org/>`_ filters

What are pandoc filters?
--------------------------
Pandoc filters
are pipes that read a JSON serialization of the Pandoc AST
from stdin, transform it in some way, and write it to stdout.
They can be used with pandoc (>= 1.12) either using pipes ::

    pandoc -t json -s | ./caps.py | pandoc -f json

or using the ``--filter`` (or ``-F``) command-line option. ::

    pandoc --filter ./caps.py -s

For more on pandoc filters, see the pandoc documentation under ``--filter``
and `the tutorial on writing filters`__.

__ http://johnmacfarlane.net/pandoc/scripting.html

For an alternative library for writing pandoc filters, with
a more "Pythonic" design, see `panflute`__.

__ https://github.com/sergiocorreia/panflute

Compatibility
----------------
Pandoc 1.16 introduced link and image `attributes` to the existing
`caption` and `target` arguments, requiring a change in pandocfilters
that breaks backwards compatibility. Consequently, you should use:

- pandocfilters version <= 1.2.4 for pandoc versions 1.12--1.15, and
- pandocfilters version >= 1.3.0 for pandoc versions >= 1.16.

Pandoc 1.17.3 (pandoc-types 1.17.*) introduced a new JSON format.
pandocfilters 1.4.0 should work with both the old and the new
format.

Installing
--------------
Run this inside the present directory::

    python setup.py install

Or install from PyPI::

    pip install pandocfilters

Available functions
----------------------
The main functions ``pandocfilters`` exports are

-  ``walk(x, action, format, meta)``

   Walk a tree, applying an action to every object. Returns a modified
   tree. An action is a function of the form
   ``action(key, value, format, meta)``, where:

   -  ``key`` is the type of the pandoc object (e.g. 'Str', 'Para')
   -  ``value`` is the contents of the object (e.g. a string for 'Str', a list of
      inline elements for 'Para')
   -  ``format`` is the target output format (as supplied by the
      ``format`` argument of ``walk``)
   -  ``meta`` is the document's metadata

   The return of an action is either:

   -  ``None``: this means that the object should remain unchanged
   -  a pandoc object: this will replace the original object
   -  a list of pandoc objects: these will replace the original object;
      the list is merged with the neighbors of the original objects
      (spliced into the list the original object belongs to); returning
      an empty list deletes the object

-  ``toJSONFilter(action)``

   Like ``toJSONFilters``, but takes a single action as argument.

-  ``toJSONFilters(actions)``

   Generate a JSON-to-JSON filter from stdin to stdout

   The filter:

   -  reads a JSON-formatted pandoc document from stdin
   -  transforms it by walking the tree and performing the actions
   -  returns a new JSON-formatted pandoc document to stdout

   The argument ``actions`` is a list of functions of the form
   ``action(key, value, format, meta)``, as described in more detail
   under ``walk``.

   This function calls ``applyJSONFilters``, with the ``format``
   argument provided by the first command-line argument, if present.
   (Pandoc sets this by default when calling filters.)

-  ``applyJSONFilters(actions, source, format="")``

   Walk through JSON structure and apply filters

   This:

   -  reads a JSON-formatted pandoc document from a source string
   -  transforms it by walking the tree and performing the actions
   -  returns a new JSON-formatted pandoc document as a string

   The ``actions`` argument is a list of functions (see ``walk`` for a
   full description).

   The argument ``source`` is a string encoded JSON object.

   The argument ``format`` is a string describing the output format.

   Returns a new JSON-formatted pandoc document.

-  ``stringify(x)``

   Walks the tree x and returns concatenated string content, leaving out
   all formatting.

-  ``attributes(attrs)``

   Returns an attribute list, constructed from the dictionary attrs.

How to use
----------
Most users will only need ``toJSONFilter``.  Here is a simple example
of its use::

    #!/usr/bin/env python

    \"\"\"
    Pandoc filter to convert all regular text to uppercase.
    Code, link URLs, etc. are not affected.
    \"\"\"

    from pandocfilters import toJSONFilter, Str

    def caps(key, value, format, meta):
      if key == 'Str':
        return Str(value.upper())

    if __name__ == "__main__":
      toJSONFilter(caps)

Examples
--------

The examples subdirectory in the source repository contains the
following filters. These filters should provide a useful starting point
for developing your own pandocfilters.

``abc.py``
    Pandoc filter to process code blocks with class ``abc`` containing ABC
    notation into images. Assumes that abcm2ps and ImageMagick's convert
    are in the path. Images are put in the abc-images directory.

``caps.py``
    Pandoc filter to convert all regular text to uppercase. Code, link
    URLs, etc. are not affected.

``blockdiag.py``
    Pandoc filter to process code blocks with class "blockdiag" into
    generated images. Needs utils from http://blockdiag.com.

``comments.py``
    Pandoc filter that causes everything between
    ``<!-- BEGIN COMMENT -->`` and ``<!-- END COMMENT -->`` to be ignored.
    The comment lines must appear on lines by themselves, with blank
    lines surrounding

``deemph.py``
    Pandoc filter that causes emphasized text to be displayed in ALL
    CAPS.

``deflists.py``
    Pandoc filter to convert definition lists to bullet lists with the
    defined terms in strong emphasis (for compatibility with standard
    markdown).

``gabc.py``
    Pandoc filter to convert code blocks with class "gabc" to LaTeX
    \\\\gabcsnippet commands in LaTeX output, and to images in HTML output.

``graphviz.py``
    Pandoc filter to process code blocks with class ``graphviz`` into
    graphviz-generated images.

``lilypond.py``
    Pandoc filter to process code blocks with class "ly" containing
    Lilypond notation.

``metavars.py``
    Pandoc filter to allow interpolation of metadata fields into a
    document. ``%{fields}`` will be replaced by the field's value, assuming
    it is of the type ``MetaInlines`` or ``MetaString``.

``myemph.py``
    Pandoc filter that causes emphasis to be rendered using the custom
    macro ``\\myemph{...}`` rather than ``\\emph{...}`` in latex. Other output
    formats are unaffected.

``plantuml.py``
    Pandoc filter to process code blocks with class ``plantuml`` to images.
    Needs `plantuml.jar` from http://plantuml.com/.

``ditaa.py``
    Pandoc filter to process code blocks with class ``ditaa`` to images.
    Needs `ditaa.jar` from http://ditaa.sourceforge.net/.

``theorem.py``
    Pandoc filter to convert divs with ``class="theorem"`` to LaTeX theorem
    environments in LaTeX output, and to numbered theorems in HTML
    output.

``tikz.py``
    Pandoc filter to process raw latex tikz environments into images.
    Assumes that pdflatex is in the path, and that the standalone
    package is available. Also assumes that ImageMagick's convert is in
    the path. Images are put in the ``tikz-images`` directory.

API documentation
-----------------

By default most filters use ``get_filename4code`` to
create a directory ``...-images`` to save temporary
files. This directory doesn't get removed as it can be used as a cache so that
later pandoc runs don't have to recreate files if they already exist. The
directory is generated in the current directory.

If you prefer to have a clean directory after running pandoc filters, you
can set an environment variable ``PANDOCFILTER_CLEANUP`` to any non-empty value such as `1`
which forces the code to create a temporary directory that will be removed
by the end of execution.
"""^^xsd:string ;
    schema1:name "pandocfilters"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.5.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pexpect> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/4.3> .

<https://data.siemens-energy.com/cybersec/package/pillow> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/coverage>,
        <https://data.siemens-energy.com/cybersec/package/sphinx>,
        <https://data.siemens-energy.com/cybersec/package/trove-classifiers> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/8> ;
    schema1:description """<p align="center">
    <img width="248" height="250" src="https://raw.githubusercontent.com/python-pillow/pillow-logo/main/pillow-logo-248x250.png" alt="Pillow logo">
</p>

# Pillow

## Python Imaging Library (Fork)

Pillow is the friendly PIL fork by [Jeffrey A. Clark and
contributors](https://github.com/python-pillow/Pillow/graphs/contributors).
PIL is the Python Imaging Library by Fredrik Lundh and contributors.
As of 2019, Pillow development is
[supported by Tidelift](https://tidelift.com/subscription/pkg/pypi-pillow?utm_source=pypi-pillow&utm_medium=readme&utm_campaign=enterprise).

<table>
    <tr>
        <th>docs</th>
        <td>
            <a href="https://pillow.readthedocs.io/?badge=latest"><img
                alt="Documentation Status"
                src="https://readthedocs.org/projects/pillow/badge/?version=latest"></a>
        </td>
    </tr>
    <tr>
        <th>tests</th>
        <td>
            <a href="https://github.com/python-pillow/Pillow/actions/workflows/lint.yml"><img
                alt="GitHub Actions build status (Lint)"
                src="https://github.com/python-pillow/Pillow/workflows/Lint/badge.svg"></a>
            <a href="https://github.com/python-pillow/Pillow/actions/workflows/test.yml"><img
                alt="GitHub Actions build status (Test Linux and macOS)"
                src="https://github.com/python-pillow/Pillow/workflows/Test/badge.svg"></a>
            <a href="https://github.com/python-pillow/Pillow/actions/workflows/test-windows.yml"><img
                alt="GitHub Actions build status (Test Windows)"
                src="https://github.com/python-pillow/Pillow/workflows/Test%20Windows/badge.svg"></a>
            <a href="https://github.com/python-pillow/Pillow/actions/workflows/test-mingw.yml"><img
                alt="GitHub Actions build status (Test MinGW)"
                src="https://github.com/python-pillow/Pillow/workflows/Test%20MinGW/badge.svg"></a>
            <a href="https://github.com/python-pillow/Pillow/actions/workflows/test-cygwin.yml"><img
                alt="GitHub Actions build status (Test Cygwin)"
                src="https://github.com/python-pillow/Pillow/workflows/Test%20Cygwin/badge.svg"></a>
            <a href="https://github.com/python-pillow/Pillow/actions/workflows/test-docker.yml"><img
                alt="GitHub Actions build status (Test Docker)"
                src="https://github.com/python-pillow/Pillow/workflows/Test%20Docker/badge.svg"></a>
            <a href="https://ci.appveyor.com/project/python-pillow/Pillow"><img
                alt="AppVeyor CI build status (Windows)"
                src="https://img.shields.io/appveyor/build/python-pillow/Pillow/main.svg?label=Windows%20build"></a>
            <a href="https://github.com/python-pillow/Pillow/actions/workflows/wheels.yml"><img
                alt="GitHub Actions build status (Wheels)"
                src="https://github.com/python-pillow/Pillow/workflows/Wheels/badge.svg"></a>
            <a href="https://app.codecov.io/gh/python-pillow/Pillow"><img
                alt="Code coverage"
                src="https://codecov.io/gh/python-pillow/Pillow/branch/main/graph/badge.svg"></a>
            <a href="https://issues.oss-fuzz.com/issues?q=title:pillow"><img
                alt="Fuzzing Status"
                src="https://oss-fuzz-build-logs.storage.googleapis.com/badges/pillow.svg"></a>
        </td>
    </tr>
    <tr>
        <th>package</th>
        <td>
            <a href="https://zenodo.org/badge/latestdoi/17549/python-pillow/Pillow"><img
                alt="Zenodo"
                src="https://zenodo.org/badge/17549/python-pillow/Pillow.svg"></a>
            <a href="https://tidelift.com/subscription/pkg/pypi-pillow?utm_source=pypi-pillow&utm_medium=badge"><img
                alt="Tidelift"
                src="https://tidelift.com/badges/package/pypi/pillow?style=flat"></a>
            <a href="https://pypi.org/project/pillow/"><img
                alt="Newest PyPI version"
                src="https://img.shields.io/pypi/v/pillow.svg"></a>
            <a href="https://pypi.org/project/pillow/"><img
                alt="Number of PyPI downloads"
                src="https://img.shields.io/pypi/dm/pillow.svg"></a>
            <a href="https://www.bestpractices.dev/projects/6331"><img
                alt="OpenSSF Best Practices"
                src="https://www.bestpractices.dev/projects/6331/badge"></a>
        </td>
    </tr>
    <tr>
        <th>social</th>
        <td>
            <a href="https://gitter.im/python-pillow/Pillow?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge"><img
                alt="Join the chat at https://gitter.im/python-pillow/Pillow"
                src="https://badges.gitter.im/python-pillow/Pillow.svg"></a>
            <a href="https://fosstodon.org/@pillow"><img
                alt="Follow on https://fosstodon.org/@pillow"
                src="https://img.shields.io/badge/publish-on%20Mastodon-595aff.svg"
                rel="me"></a>
        </td>
    </tr>
</table>

## Overview

The Python Imaging Library adds image processing capabilities to your Python interpreter.

This library provides extensive file format support, an efficient internal representation, and fairly powerful image processing capabilities.

The core image library is designed for fast access to data stored in a few basic pixel formats. It should provide a solid foundation for a general image processing tool.

## More Information

- [Documentation](https://pillow.readthedocs.io/)
  - [Installation](https://pillow.readthedocs.io/en/latest/installation/basic-installation.html)
  - [Handbook](https://pillow.readthedocs.io/en/latest/handbook/index.html)
- [Contribute](https://github.com/python-pillow/Pillow/blob/main/.github/CONTRIBUTING.md)
  - [Issues](https://github.com/python-pillow/Pillow/issues)
  - [Pull requests](https://github.com/python-pillow/Pillow/pulls)
- [Release notes](https://pillow.readthedocs.io/en/stable/releasenotes/index.html)
- [Changelog](https://github.com/python-pillow/Pillow/releases)
  - [Pre-fork](https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst#pre-fork)

## Report a Vulnerability

To report a security vulnerability, please follow the procedure described in the [Tidelift security policy](https://tidelift.com/docs/security).
"""^^xsd:string ;
    schema1:name "pillow"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "11.1.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pkgutil-resolve-name> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.3.10> .

<https://data.siemens-energy.com/cybersec/package/poetry-core> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.0.0> .

<https://data.siemens-energy.com/cybersec/package/prompt-toolkit> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.0.37> .

<https://data.siemens-energy.com/cybersec/package/psycopg2> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.9.6> .

<https://data.siemens-energy.com/cybersec/package/pyarrow> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/10.0.1> .

<https://data.siemens-energy.com/cybersec/package/pybind11> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.13.3> .

<https://data.siemens-energy.com/cybersec/package/pydantic-core> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.23.4> .

<https://data.siemens-energy.com/cybersec/package/pygments-github-lexers> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.0.5> .

<https://data.siemens-energy.com/cybersec/package/pymysql> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.0.2> .

<https://data.siemens-energy.com/cybersec/package/pyparsing> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.3.1> ;
    schema1:description """PyParsing -- A Python Parsing Module
====================================

|Version| |Build Status| |Coverage| |License| |Python Versions| |Snyk Score|

Introduction
============

The pyparsing module is an alternative approach to creating and
executing simple grammars, vs. the traditional lex/yacc approach, or the
use of regular expressions. The pyparsing module provides a library of
classes that client code uses to construct the grammar directly in
Python code.

*[Since first writing this description of pyparsing in late 2003, this
technique for developing parsers has become more widespread, under the
name Parsing Expression Grammars - PEGs. See more information on PEGs*
`here <https://en.wikipedia.org/wiki/Parsing_expression_grammar>`__
*.]*

Here is a program to parse ``"Hello, World!"`` (or any greeting of the form
``"salutation, addressee!"``):

.. code:: python

    from pyparsing import Word, alphas
    greet = Word(alphas) + "," + Word(alphas) + "!"
    hello = "Hello, World!"
    print(hello, "->", greet.parseString(hello))

The program outputs the following::

    Hello, World! -> ['Hello', ',', 'World', '!']

The Python representation of the grammar is quite readable, owing to the
self-explanatory class names, and the use of '+', '|' and '^' operator
definitions.

The parsed results returned from ``parseString()`` is a collection of type
``ParseResults``, which can be accessed as a
nested list, a dictionary, or an object with named attributes.

The pyparsing module handles some of the problems that are typically
vexing when writing text parsers:

- extra or missing whitespace (the above program will also handle ``"Hello,World!"``, ``"Hello , World !"``, etc.)
- quoted strings
- embedded comments

The examples directory includes a simple SQL parser, simple CORBA IDL
parser, a config file parser, a chemical formula parser, and a four-
function algebraic notation parser, among many others.

Documentation
=============

There are many examples in the online docstrings of the classes
and methods in pyparsing. You can find them compiled into `online docs <https://pyparsing-docs.readthedocs.io/en/latest/>`__. Additional
documentation resources and project info are listed in the online
`GitHub wiki <https://github.com/pyparsing/pyparsing/wiki>`__. An
entire directory of examples can be found `here <https://github.com/pyparsing/pyparsing/tree/master/examples>`__.

License
=======

MIT License. See header of the `pyparsing __init__.py <https://github.com/pyparsing/pyparsing/blob/master/pyparsing/__init__.py#L1-L23>`__ file.

History
=======

See `CHANGES <https://github.com/pyparsing/pyparsing/blob/master/CHANGES>`__ file.

.. |Build Status| image:: https://github.com/pyparsing/pyparsing/actions/workflows/ci.yml/badge.svg
   :target: https://github.com/pyparsing/pyparsing/actions/workflows/ci.yml

.. |Coverage| image:: https://codecov.io/gh/pyparsing/pyparsing/branch/master/graph/badge.svg
  :target: https://codecov.io/gh/pyparsing/pyparsing

.. |Version| image:: https://img.shields.io/pypi/v/pyparsing?style=flat-square
    :target: https://pypi.org/project/pyparsing/
    :alt: Version

.. |License| image:: https://img.shields.io/pypi/l/pyparsing.svg?style=flat-square
    :target: https://pypi.org/project/pyparsing/
    :alt: License

.. |Python Versions| image:: https://img.shields.io/pypi/pyversions/pyparsing.svg?style=flat-square
    :target: https://pypi.org/project/python-liquid/
    :alt: Python versions

.. |Snyk Score| image:: https://snyk.io//advisor/python/pyparsing/badge.svg
   :target: https://snyk.io//advisor/python/pyparsing
   :alt: pyparsing

"""^^xsd:string ;
    schema1:name "pyparsing"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.1.4"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pyproject-hooks> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.1> .

<https://data.siemens-energy.com/cybersec/package/pyqtwebengine> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/5.15> .

<https://data.siemens-energy.com/cybersec/package/pyreadstat> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.2.0> .

<https://data.siemens-energy.com/cybersec/package/pysocks> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.5.7> .

<https://data.siemens-energy.com/cybersec/package/pytest-asyncio> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.21> .

<https://data.siemens-energy.com/cybersec/package/pytest-checkdocs> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.4> .

<https://data.siemens-energy.com/cybersec/package/pytest-enabler> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.2> .

<https://data.siemens-energy.com/cybersec/package/pytest-env> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.8.2> .

<https://data.siemens-energy.com/cybersec/package/pytest-freezer> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.4.8> .

<https://data.siemens-energy.com/cybersec/package/pytest-home> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.5> .

<https://data.siemens-energy.com/cybersec/package/pytest-randomly> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.12> .

<https://data.siemens-energy.com/cybersec/package/pytest-rerunfailures> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/9.1> .

<https://data.siemens-energy.com/cybersec/package/pytest-ruff> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.2.1> .

<https://data.siemens-energy.com/cybersec/package/python-calamine> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.1.7> .

<https://data.siemens-energy.com/cybersec/package/pywin32> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/300> ;
    schema1:description """# pywin32

[![CI](https://github.com/mhammond/pywin32/workflows/CI/badge.svg)](https://github.com/mhammond/pywin32/actions?query=workflow%3ACI)
[![PyPI - Version](https://img.shields.io/pypi/v/pywin32.svg)](https://pypi.org/project/pywin32)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pywin32.svg)](https://pypi.org/project/pywin32)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/pywin32.svg)](https://pypi.org/project/pywin32)
[![License - PSF-2.0](https://img.shields.io/badge/license-PSF--2.0-9400d3.svg)](https://spdx.org/licenses/PSF-2.0.html)

-----

This is the readme for the Python for Win32 (pywin32) extensions, which provides access to many of the Windows APIs from Python.

See [CHANGES.txt](https://github.com/mhammond/pywin32/blob/master/CHANGES.txt) for recent notable changes.

Only Python 3 is supported. If you want Python 2 support, you want build `228`.

## Docs

The docs are a long and sad story, but [there's now an online version](https://mhammond.github.io/pywin32/)
of the helpfile that ships with the installers (thanks [@ofek](https://github.com/mhammond/pywin32/pull/1774)!).
Lots of that is very old, but some is auto-generated and current. Would love help untangling the docs!

## Support

Feel free to [open issues](https://github.com/mhammond/pywin32/issues) for
all bugs (or suspected bugs) in pywin32. [pull-requests](https://github.com/mhammond/pywin32/pulls)
for all bugs or features are also welcome.

However, please **do not open github issues for general support requests**, or
for problems or questions using the modules in this package - they will be
closed. For such issues, please email the
[python-win32 mailing list](http://mail.python.org/mailman/listinfo/python-win32) -
note that you must be subscribed to the list before posting.

## Binaries
[Binary releases are deprecated.](https://mhammond.github.io/pywin32_installers.html)
While they are still provided, [find them here](https://github.com/mhammond/pywin32/releases)

## Installing via PIP

You should install pywin32 via pip - eg,
> python -m pip install --upgrade pywin32

If you encounter any problems when upgrading (eg, "module not found" errors or similar), you
should execute:

> python Scripts/pywin32_postinstall.py -install

This will make some small attempts to cleanup older conflicting installs.

Note that if you want to use pywin32 for "system wide" features, such as
registering COM objects or implementing Windows Services, then you must run
that command from an elevated (ie, "Run as Administrator) command prompt.

For unreleased changes, you can download builds made by [github actions](https://github.com/mhammond/pywin32/actions/) -
choose any "workflow" from the `main` branch and download its "artifacts")

### `The specified procedure could not be found` / `Entry-point not found` Errors?
A very common report is that people install pywin32, but many imports fail with errors
similar to the above.

In almost all cases, this tends to mean there are other pywin32 DLLs installed in your system,
but in a different location than the new ones. This sometimes happens in environments that
come with pywin32 pre-shipped (eg, anaconda?).

The possible solutions are:

* Run the "post_install" script documented above.

* Otherwise, find and remove all other copies of `pywintypesXX.dll` and `pythoncomXX.dll`
  (where `XX` is the Python version - eg, "39")

### Running as a Windows Service

Modern Python installers do not, by default, install Python in a way that is suitable for
running as a service, particularly for other users.

* Ensure Python is installed in a location where the user running the service has
  access to the installation and is able to load `pywintypesXX.dll` and `pythonXX.dll`.

* Manually copy `pythonservice.exe` from the `site-packages/win32` directory to
  the same place as these DLLs.

## Building from source

Install Visual Studio 2019 (later probably works, but options might be different),
select "Desktop Development with C++", then the following options:
* Windows 10 SDK (latest offered I guess? At time of writing, 10.0.18362)
* "C++ for MFC for ..."
* ARM build tools if necessary.

(the free compilers probably work too, but haven't been tested - let me know your experiences!)

`setup.py` is a standard distutils build script, so you probably want:

> python setup.py install

or

> python setup.py --help

Some modules need obscure SDKs to build - `setup.py` should succeed, gracefully
telling you why it failed to build them - if the build actually fails with your
configuration, please [open an issue](https://github.com/mhammond/pywin32/issues).

## Release process

The following steps are performed when making a new release - this is mainly
to form a checklist so mhammond doesn't forget what to do :)

* Ensure CHANGES.txt has everything worth noting, commit it.

* Update setup.py with the new build number.

* Execute build.bat, wait forever, test the artifacts.

* Upload .whl artifacts to pypi - we do this before pushing the tag because they might be
  rejected for an invalid `README.md`. Done via `py -3.? -m twine upload dist/*XXX*.whl`.

* Commit setup.py (so the new build number is in the repo), create a new git tag

* Upload the .exe installers to github.

* Update setup.py with the new build number + ".1" (eg, 123.1), to ensure
  future test builds aren't mistaken for the real release.

* Make sure everything is pushed to github, including the tag (ie,
  `git push --tags`)

* Send mail to python-win32
"""^^xsd:string ;
    schema1:name "pywin32"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "306"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pyxlsb> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.0.10> .

<https://data.siemens-energy.com/cybersec/package/pyzmq> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/23.0> ;
    schema1:description """# PyZMQ: Python bindings for Ã˜MQ

This package contains Python bindings for [ZeroMQ](https://zeromq.org).
Ã˜MQ is a lightweight and fast messaging implementation.

PyZMQ should work with any reasonable version of Python (â‰¥ 3.7), as well as PyPy.
The Cython backend used by CPython supports libzmq â‰¥ 2.1.4 (including 3.2.x and 4.x),
but the CFFI backend used by PyPy only supports libzmq â‰¥ 3.2.2 (including 4.x).

For a summary of changes to pyzmq, see our
[changelog](https://pyzmq.readthedocs.io/en/latest/changelog.html).

### Ã˜MQ 3.x, 4.x

PyZMQ fully supports the 3.x and 4.x APIs of libzmq,
developed at [zeromq/libzmq](https://github.com/zeromq/libzmq).
No code to change, no flags to pass,
just build pyzmq against the latest and it should work.

PyZMQ does not support the old libzmq 2 API on PyPy.

## Documentation

See PyZMQ's Sphinx-generated
documentation [on Read the Docs](https://pyzmq.readthedocs.io) for API
details, and some notes on Python and Cython development. If you want to
learn about using Ã˜MQ in general, the excellent [Ã˜MQ
Guide](http://zguide.zeromq.org/py:all) is the place to start, which has a
Python version of every example. We also have some information on our
[wiki](https://github.com/zeromq/pyzmq/wiki).

## Downloading

Unless you specifically want to develop PyZMQ, we recommend downloading
the PyZMQ source code or wheels from
[PyPI](https://pypi.io/project/pyzmq/),
or install with conda.

You can also get the latest source code from our GitHub repository, but
building from the repository will require that you install recent Cython.

## Building and installation

For more detail on building pyzmq, see [our docs](https://pyzmq.readthedocs.io/en/latest/howto/build.html).

We build wheels for macOS, Windows, and Linux, so you can get a binary on those platforms with:

```
pip install pyzmq
```

but compiling from source with `pip install pyzmq` should work in most environments.
Make sure you are using the latest pip, or it may not find the right wheels.

If the wheel doesn't work for some reason, or you want to force pyzmq to be compiled
(this is often preferable if you already have libzmq installed and configured the way you want it),
you can force installation from source with:

```
pip install --no-binary=pyzmq pyzmq
```

## Old versions

pyzmq 16 drops support Python 2.6 and 3.2.
If you need to use one of those Python versions, you can pin your pyzmq version to before 16:

```
pip install 'pyzmq<16'
```

For libzmq 2.0.x, use 'pyzmq\\<2.1'

pyzmq-2.1.11 was the last version of pyzmq to support Python 2.5,
and pyzmq â‰¥ 2.2.0 requires Python â‰¥ 2.6.
pyzmq-13.0.0 introduces PyPy support via CFFI, which only supports libzmq-3.2.2 and newer.

PyZMQ releases â‰¤ 2.2.0 matched libzmq versioning, but this is no longer the case,
starting with PyZMQ 13.0.0 (it was the thirteenth release, so why not?).
PyZMQ â‰¥ 13.0 follows semantic versioning conventions accounting only for PyZMQ itself.
"""^^xsd:string ;
    schema1:name "pyzmq"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "26.2.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/qtpy> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.3.0> .

<https://data.siemens-energy.com/cybersec/package/regex> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2019.02.19> ;
    schema1:description """Introduction
------------

This regex implementation is backwards-compatible with the standard 're' module, but offers additional functionality.

Python 2
--------

Python 2 is no longer supported. The last release that supported Python 2 was 2021.11.10.

PyPy
----

This module is targeted at CPython. It expects that all codepoints are the same width, so it won't behave properly with PyPy outside U+0000..U+007F because PyPy stores strings as UTF-8.

Multithreading
--------------

The regex module releases the GIL during matching on instances of the built-in (immutable) string classes, enabling other Python threads to run concurrently. It is also possible to force the regex module to release the GIL during matching by calling the matching methods with the keyword argument ``concurrent=True``. The behaviour is undefined if the string changes during matching, so use it *only* when it is guaranteed that that won't happen.

Unicode
-------

This module supports Unicode 15.1.0. Full Unicode case-folding is supported.

Flags
-----

There are 2 kinds of flag: scoped and global. Scoped flags can apply to only part of a pattern and can be turned on or off; global flags apply to the entire pattern and can only be turned on.

The scoped flags are: ``ASCII (?a)``, ``FULLCASE (?f)``, ``IGNORECASE (?i)``, ``LOCALE (?L)``, ``MULTILINE (?m)``, ``DOTALL (?s)``, ``UNICODE (?u)``, ``VERBOSE (?x)``, ``WORD (?w)``.

The global flags are: ``BESTMATCH (?b)``, ``ENHANCEMATCH (?e)``, ``POSIX (?p)``, ``REVERSE (?r)``, ``VERSION0 (?V0)``, ``VERSION1 (?V1)``.

If neither the ``ASCII``, ``LOCALE`` nor ``UNICODE`` flag is specified, it will default to ``UNICODE`` if the regex pattern is a Unicode string and ``ASCII`` if it's a bytestring.

The ``ENHANCEMATCH`` flag makes fuzzy matching attempt to improve the fit of the next match that it finds.

The ``BESTMATCH`` flag makes fuzzy matching search for the best match instead of the next match.

Old vs new behaviour
--------------------

In order to be compatible with the re module, this module has 2 behaviours:

* **Version 0** behaviour (old behaviour, compatible with the re module):

  Please note that the re module's behaviour may change over time, and I'll endeavour to match that behaviour in version 0.

  * Indicated by the ``VERSION0`` flag.

  * Zero-width matches are not handled correctly in the re module before Python 3.7. The behaviour in those earlier versions is:

    * ``.split`` won't split a string at a zero-width match.

    * ``.sub`` will advance by one character after a zero-width match.

  * Inline flags apply to the entire pattern, and they can't be turned off.

  * Only simple sets are supported.

  * Case-insensitive matches in Unicode use simple case-folding by default.

* **Version 1** behaviour (new behaviour, possibly different from the re module):

  * Indicated by the ``VERSION1`` flag.

  * Zero-width matches are handled correctly.

  * Inline flags apply to the end of the group or pattern, and they can be turned off.

  * Nested sets and set operations are supported.

  * Case-insensitive matches in Unicode use full case-folding by default.

If no version is specified, the regex module will default to ``regex.DEFAULT_VERSION``.

Case-insensitive matches in Unicode
-----------------------------------

The regex module supports both simple and full case-folding for case-insensitive matches in Unicode. Use of full case-folding can be turned on using the ``FULLCASE`` flag. Please note that this flag affects how the ``IGNORECASE`` flag works; the ``FULLCASE`` flag itself does not turn on case-insensitive matching.

Version 0 behaviour: the flag is off by default.

Version 1 behaviour: the flag is on by default.

Nested sets and set operations
------------------------------

It's not possible to support both simple sets, as used in the re module, and nested sets at the same time because of a difference in the meaning of an unescaped ``"["`` in a set.

For example, the pattern ``[[a-z]--[aeiou]]`` is treated in the version 0 behaviour (simple sets, compatible with the re module) as:

* Set containing "[" and the letters "a" to "z"

* Literal "--"

* Set containing letters "a", "e", "i", "o", "u"

* Literal "]"

but in the version 1 behaviour (nested sets, enhanced behaviour) as:

* Set which is:

  * Set containing the letters "a" to "z"

* but excluding:

  * Set containing the letters "a", "e", "i", "o", "u"

Version 0 behaviour: only simple sets are supported.

Version 1 behaviour: nested sets and set operations are supported.

Notes on named groups
---------------------

All groups have a group number, starting from 1.

Groups with the same group name will have the same group number, and groups with a different group name will have a different group number.

The same name can be used by more than one group, with later captures 'overwriting' earlier captures. All the captures of the group will be available from the ``captures`` method of the match object.

Group numbers will be reused across different branches of a branch reset, eg. ``(?|(first)|(second))`` has only group 1. If groups have different group names then they will, of course, have different group numbers, eg. ``(?|(?P<foo>first)|(?P<bar>second))`` has group 1 ("foo") and group 2 ("bar").

In the regex ``(\\s+)(?|(?P<foo>[A-Z]+)|(\\w+) (?P<foo>[0-9]+)`` there are 2 groups:

* ``(\\s+)`` is group 1.

* ``(?P<foo>[A-Z]+)`` is group 2, also called "foo".

* ``(\\w+)`` is group 2 because of the branch reset.

* ``(?P<foo>[0-9]+)`` is group 2 because it's called "foo".

If you want to prevent ``(\\w+)`` from being group 2, you need to name it (different name, different group number).

Additional features
-------------------

The issue numbers relate to the Python bug tracker, except where listed otherwise.

Added ``\\p{Horiz_Space}`` and ``\\p{Vert_Space}`` (`GitHub issue 477 <https://github.com/mrabarnett/mrab-regex/issues/477#issuecomment-1216779547>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``\\p{Horiz_Space}`` or ``\\p{H}`` matches horizontal whitespace and ``\\p{Vert_Space}`` or ``\\p{V}`` matches vertical whitespace.

Added support for lookaround in conditional pattern (`Hg issue 163 <https://github.com/mrabarnett/mrab-regex/issues/163>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The test of a conditional pattern can be a lookaround.

.. sourcecode:: python

  >>> regex.match(r'(?(?=\\d)\\d+|\\w+)', '123abc')
  <regex.Match object; span=(0, 3), match='123'>
  >>> regex.match(r'(?(?=\\d)\\d+|\\w+)', 'abc123')
  <regex.Match object; span=(0, 6), match='abc123'>

This is not quite the same as putting a lookaround in the first branch of a pair of alternatives.

.. sourcecode:: python

  >>> print(regex.match(r'(?:(?=\\d)\\d+\\b|\\w+)', '123abc'))
  <regex.Match object; span=(0, 6), match='123abc'>
  >>> print(regex.match(r'(?(?=\\d)\\d+\\b|\\w+)', '123abc'))
  None

In the first example, the lookaround matched, but the remainder of the first branch failed to match, and so the second branch was attempted, whereas in the second example, the lookaround matched, and the first branch failed to match, but the second branch was **not** attempted.

Added POSIX matching (leftmost longest) (`Hg issue 150 <https://github.com/mrabarnett/mrab-regex/issues/150>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The POSIX standard for regex is to return the leftmost longest match. This can be turned on using the ``POSIX`` flag.

.. sourcecode:: python

  >>> # Normal matching.
  >>> regex.search(r'Mr|Mrs', 'Mrs')
  <regex.Match object; span=(0, 2), match='Mr'>
  >>> regex.search(r'one(self)?(selfsufficient)?', 'oneselfsufficient')
  <regex.Match object; span=(0, 7), match='oneself'>
  >>> # POSIX matching.
  >>> regex.search(r'(?p)Mr|Mrs', 'Mrs')
  <regex.Match object; span=(0, 3), match='Mrs'>
  >>> regex.search(r'(?p)one(self)?(selfsufficient)?', 'oneselfsufficient')
  <regex.Match object; span=(0, 17), match='oneselfsufficient'>

Note that it will take longer to find matches because when it finds a match at a certain position, it won't return that immediately, but will keep looking to see if there's another longer match there.

Added ``(?(DEFINE)...)`` (`Hg issue 152 <https://github.com/mrabarnett/mrab-regex/issues/152>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If there's no group called "DEFINE", then ... will be ignored except that any groups defined within it can be called and that the normal rules for numbering groups still apply.

.. sourcecode:: python

  >>> regex.search(r'(?(DEFINE)(?P<quant>\\d+)(?P<item>\\w+))(?&quant) (?&item)', '5 elephants')
  <regex.Match object; span=(0, 11), match='5 elephants'>

Added ``(*PRUNE)``, ``(*SKIP)`` and ``(*FAIL)`` (`Hg issue 153 <https://github.com/mrabarnett/mrab-regex/issues/153>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``(*PRUNE)`` discards the backtracking info up to that point. When used in an atomic group or a lookaround, it won't affect the enclosing pattern.

``(*SKIP)`` is similar to ``(*PRUNE)``, except that it also sets where in the text the next attempt to match will start. When used in an atomic group or a lookaround, it won't affect the enclosing pattern.

``(*FAIL)`` causes immediate backtracking. ``(*F)`` is a permitted abbreviation.

Added ``\\K`` (`Hg issue 151 <https://github.com/mrabarnett/mrab-regex/issues/151>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Keeps the part of the entire match after the position where ``\\K`` occurred; the part before it is discarded.

It does not affect what groups return.

.. sourcecode:: python

  >>> m = regex.search(r'(\\w\\w\\K\\w\\w\\w)', 'abcdef')
  >>> m[0]
  'cde'
  >>> m[1]
  'abcde'
  >>>
  >>> m = regex.search(r'(?r)(\\w\\w\\K\\w\\w\\w)', 'abcdef')
  >>> m[0]
  'bc'
  >>> m[1]
  'bcdef'

Added capture subscripting for ``expandf`` and ``subf``/``subfn`` (`Hg issue 133 <https://github.com/mrabarnett/mrab-regex/issues/133>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You can use subscripting to get the captures of a repeated group.

.. sourcecode:: python

  >>> m = regex.match(r"(\\w)+", "abc")
  >>> m.expandf("{1}")
  'c'
  >>> m.expandf("{1[0]} {1[1]} {1[2]}")
  'a b c'
  >>> m.expandf("{1[-1]} {1[-2]} {1[-3]}")
  'c b a'
  >>>
  >>> m = regex.match(r"(?P<letter>\\w)+", "abc")
  >>> m.expandf("{letter}")
  'c'
  >>> m.expandf("{letter[0]} {letter[1]} {letter[2]}")
  'a b c'
  >>> m.expandf("{letter[-1]} {letter[-2]} {letter[-3]}")
  'c b a'

Added support for referring to a group by number using ``(?P=...)``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This is in addition to the existing ``\\g<...>``.

Fixed the handling of locale-sensitive regexes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``LOCALE`` flag is intended for legacy code and has limited support. You're still recommended to use Unicode instead.

Added partial matches (`Hg issue 102 <https://github.com/mrabarnett/mrab-regex/issues/102>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A partial match is one that matches up to the end of string, but that string has been truncated and you want to know whether a complete match could be possible if the string had not been truncated.

Partial matches are supported by ``match``, ``search``, ``fullmatch`` and ``finditer`` with the ``partial`` keyword argument.

Match objects have a ``partial`` attribute, which is ``True`` if it's a partial match.

For example, if you wanted a user to enter a 4-digit number and check it character by character as it was being entered:

.. sourcecode:: python

  >>> pattern = regex.compile(r'\\d{4}')

  >>> # Initially, nothing has been entered:
  >>> print(pattern.fullmatch('', partial=True))
  <regex.Match object; span=(0, 0), match='', partial=True>

  >>> # An empty string is OK, but it's only a partial match.
  >>> # The user enters a letter:
  >>> print(pattern.fullmatch('a', partial=True))
  None
  >>> # It'll never match.

  >>> # The user deletes that and enters a digit:
  >>> print(pattern.fullmatch('1', partial=True))
  <regex.Match object; span=(0, 1), match='1', partial=True>
  >>> # It matches this far, but it's only a partial match.

  >>> # The user enters 2 more digits:
  >>> print(pattern.fullmatch('123', partial=True))
  <regex.Match object; span=(0, 3), match='123', partial=True>
  >>> # It matches this far, but it's only a partial match.

  >>> # The user enters another digit:
  >>> print(pattern.fullmatch('1234', partial=True))
  <regex.Match object; span=(0, 4), match='1234'>
  >>> # It's a complete match.

  >>> # If the user enters another digit:
  >>> print(pattern.fullmatch('12345', partial=True))
  None
  >>> # It's no longer a match.

  >>> # This is a partial match:
  >>> pattern.match('123', partial=True).partial
  True

  >>> # This is a complete match:
  >>> pattern.match('1233', partial=True).partial
  False

``*`` operator not working correctly with sub() (`Hg issue 106 <https://github.com/mrabarnett/mrab-regex/issues/106>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Sometimes it's not clear how zero-width matches should be handled. For example, should ``.*`` match 0 characters directly after matching >0 characters?

.. sourcecode:: python

  >>> regex.sub('.*', 'x', 'test')
  'xx'
  >>> regex.sub('.*?', '|', 'test')
  '|||||||||'

Added ``capturesdict`` (`Hg issue 86 <https://github.com/mrabarnett/mrab-regex/issues/86>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``capturesdict`` is a combination of ``groupdict`` and ``captures``:

``groupdict`` returns a dict of the named groups and the last capture of those groups.

``captures`` returns a list of all the captures of a group

``capturesdict`` returns a dict of the named groups and lists of all the captures of those groups.

.. sourcecode:: python

  >>> m = regex.match(r"(?:(?P<word>\\w+) (?P<digits>\\d+)\\n)+", "one 1\\ntwo 2\\nthree 3\\n")
  >>> m.groupdict()
  {'word': 'three', 'digits': '3'}
  >>> m.captures("word")
  ['one', 'two', 'three']
  >>> m.captures("digits")
  ['1', '2', '3']
  >>> m.capturesdict()
  {'word': ['one', 'two', 'three'], 'digits': ['1', '2', '3']}

Added ``allcaptures`` and ``allspans`` (`Git issue 474 <https://github.com/mrabarnett/mrab-regex/issues/474>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``allcaptures`` returns a list of all the captures of all the groups.

``allspans`` returns a list of all the spans of the all captures of all the groups.

.. sourcecode:: python

  >>> m = regex.match(r"(?:(?P<word>\\w+) (?P<digits>\\d+)\\n)+", "one 1\\ntwo 2\\nthree 3\\n")
  >>> m.allcaptures()
  (['one 1\\ntwo 2\\nthree 3\\n'], ['one', 'two', 'three'], ['1', '2', '3'])
  >>> m.allspans()
  ([(0, 20)], [(0, 3), (6, 9), (12, 17)], [(4, 5), (10, 11), (18, 19)])

Allow duplicate names of groups (`Hg issue 87 <https://github.com/mrabarnett/mrab-regex/issues/87>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Group names can be duplicated.

.. sourcecode:: python

  >>> # With optional groups:
  >>>
  >>> # Both groups capture, the second capture 'overwriting' the first.
  >>> m = regex.match(r"(?P<item>\\w+)? or (?P<item>\\w+)?", "first or second")
  >>> m.group("item")
  'second'
  >>> m.captures("item")
  ['first', 'second']
  >>> # Only the second group captures.
  >>> m = regex.match(r"(?P<item>\\w+)? or (?P<item>\\w+)?", " or second")
  >>> m.group("item")
  'second'
  >>> m.captures("item")
  ['second']
  >>> # Only the first group captures.
  >>> m = regex.match(r"(?P<item>\\w+)? or (?P<item>\\w+)?", "first or ")
  >>> m.group("item")
  'first'
  >>> m.captures("item")
  ['first']
  >>>
  >>> # With mandatory groups:
  >>>
  >>> # Both groups capture, the second capture 'overwriting' the first.
  >>> m = regex.match(r"(?P<item>\\w*) or (?P<item>\\w*)?", "first or second")
  >>> m.group("item")
  'second'
  >>> m.captures("item")
  ['first', 'second']
  >>> # Again, both groups capture, the second capture 'overwriting' the first.
  >>> m = regex.match(r"(?P<item>\\w*) or (?P<item>\\w*)", " or second")
  >>> m.group("item")
  'second'
  >>> m.captures("item")
  ['', 'second']
  >>> # And yet again, both groups capture, the second capture 'overwriting' the first.
  >>> m = regex.match(r"(?P<item>\\w*) or (?P<item>\\w*)", "first or ")
  >>> m.group("item")
  ''
  >>> m.captures("item")
  ['first', '']

Added ``fullmatch`` (`issue #16203 <https://bugs.python.org/issue16203>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``fullmatch`` behaves like ``match``, except that it must match all of the string.

.. sourcecode:: python

  >>> print(regex.fullmatch(r"abc", "abc").span())
  (0, 3)
  >>> print(regex.fullmatch(r"abc", "abcx"))
  None
  >>> print(regex.fullmatch(r"abc", "abcx", endpos=3).span())
  (0, 3)
  >>> print(regex.fullmatch(r"abc", "xabcy", pos=1, endpos=4).span())
  (1, 4)
  >>>
  >>> regex.match(r"a.*?", "abcd").group(0)
  'a'
  >>> regex.fullmatch(r"a.*?", "abcd").group(0)
  'abcd'

Added ``subf`` and ``subfn``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``subf`` and ``subfn`` are alternatives to ``sub`` and ``subn`` respectively. When passed a replacement string, they treat it as a format string.

.. sourcecode:: python

  >>> regex.subf(r"(\\w+) (\\w+)", "{0} => {2} {1}", "foo bar")
  'foo bar => bar foo'
  >>> regex.subf(r"(?P<word1>\\w+) (?P<word2>\\w+)", "{word2} {word1}", "foo bar")
  'bar foo'

Added ``expandf`` to match object
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``expandf`` is an alternative to ``expand``. When passed a replacement string, it treats it as a format string.

.. sourcecode:: python

  >>> m = regex.match(r"(\\w+) (\\w+)", "foo bar")
  >>> m.expandf("{0} => {2} {1}")
  'foo bar => bar foo'
  >>>
  >>> m = regex.match(r"(?P<word1>\\w+) (?P<word2>\\w+)", "foo bar")
  >>> m.expandf("{word2} {word1}")
  'bar foo'

Detach searched string
^^^^^^^^^^^^^^^^^^^^^^

A match object contains a reference to the string that was searched, via its ``string`` attribute. The ``detach_string`` method will 'detach' that string, making it available for garbage collection, which might save valuable memory if that string is very large.

.. sourcecode:: python

  >>> m = regex.search(r"\\w+", "Hello world")
  >>> print(m.group())
  Hello
  >>> print(m.string)
  Hello world
  >>> m.detach_string()
  >>> print(m.group())
  Hello
  >>> print(m.string)
  None

Recursive patterns (`Hg issue 27 <https://github.com/mrabarnett/mrab-regex/issues/27>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Recursive and repeated patterns are supported.

``(?R)`` or ``(?0)`` tries to match the entire regex recursively. ``(?1)``, ``(?2)``, etc, try to match the relevant group.

``(?&name)`` tries to match the named group.

.. sourcecode:: python

  >>> regex.match(r"(Tarzan|Jane) loves (?1)", "Tarzan loves Jane").groups()
  ('Tarzan',)
  >>> regex.match(r"(Tarzan|Jane) loves (?1)", "Jane loves Tarzan").groups()
  ('Jane',)

  >>> m = regex.search(r"(\\w)(?:(?R)|(\\w?))\\1", "kayak")
  >>> m.group(0, 1, 2)
  ('kayak', 'k', None)

The first two examples show how the subpattern within the group is reused, but is _not_ itself a group. In other words, ``"(Tarzan|Jane) loves (?1)"`` is equivalent to ``"(Tarzan|Jane) loves (?:Tarzan|Jane)"``.

It's possible to backtrack into a recursed or repeated group.

You can't call a group if there is more than one group with that group name or group number (``"ambiguous group reference"``).

The alternative forms ``(?P>name)`` and ``(?P&name)`` are also supported.

Full Unicode case-folding is supported
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In version 1 behaviour, the regex module uses full case-folding when performing case-insensitive matches in Unicode.

.. sourcecode:: python

  >>> regex.match(r"(?iV1)strasse", "stra\\N{LATIN SMALL LETTER SHARP S}e").span()
  (0, 6)
  >>> regex.match(r"(?iV1)stra\\N{LATIN SMALL LETTER SHARP S}e", "STRASSE").span()
  (0, 7)

In version 0 behaviour, it uses simple case-folding for backward compatibility with the re module.

Approximate "fuzzy" matching (`Hg issue 12 <https://github.com/mrabarnett/mrab-regex/issues/12>`_, `Hg issue 41 <https://github.com/mrabarnett/mrab-regex/issues/41>`_, `Hg issue 109 <https://github.com/mrabarnett/mrab-regex/issues/109>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Regex usually attempts an exact match, but sometimes an approximate, or "fuzzy", match is needed, for those cases where the text being searched may contain errors in the form of inserted, deleted or substituted characters.

A fuzzy regex specifies which types of errors are permitted, and, optionally, either the minimum and maximum or only the maximum permitted number of each type. (You cannot specify only a minimum.)

The 3 types of error are:

* Insertion, indicated by "i"

* Deletion, indicated by "d"

* Substitution, indicated by "s"

In addition, "e" indicates any type of error.

The fuzziness of a regex item is specified between "{" and "}" after the item.

Examples:

* ``foo`` match "foo" exactly

* ``(?:foo){i}`` match "foo", permitting insertions

* ``(?:foo){d}`` match "foo", permitting deletions

* ``(?:foo){s}`` match "foo", permitting substitutions

* ``(?:foo){i,s}`` match "foo", permitting insertions and substitutions

* ``(?:foo){e}`` match "foo", permitting errors

If a certain type of error is specified, then any type not specified will **not** be permitted.

In the following examples I'll omit the item and write only the fuzziness:

* ``{d<=3}`` permit at most 3 deletions, but no other types

* ``{i<=1,s<=2}`` permit at most 1 insertion and at most 2 substitutions, but no deletions

* ``{1<=e<=3}`` permit at least 1 and at most 3 errors

* ``{i<=2,d<=2,e<=3}`` permit at most 2 insertions, at most 2 deletions, at most 3 errors in total, but no substitutions

It's also possible to state the costs of each type of error and the maximum permitted total cost.

Examples:

* ``{2i+2d+1s<=4}`` each insertion costs 2, each deletion costs 2, each substitution costs 1, the total cost must not exceed 4

* ``{i<=1,d<=1,s<=1,2i+2d+1s<=4}`` at most 1 insertion, at most 1 deletion, at most 1 substitution; each insertion costs 2, each deletion costs 2, each substitution costs 1, the total cost must not exceed 4

You can also use "<" instead of "<=" if you want an exclusive minimum or maximum.

You can add a test to perform on a character that's substituted or inserted.

Examples:

* ``{s<=2:[a-z]}`` at most 2 substitutions, which must be in the character set ``[a-z]``.

* ``{s<=2,i<=3:\\d}`` at most 2 substitutions, at most 3 insertions, which must be digits.

By default, fuzzy matching searches for the first match that meets the given constraints. The ``ENHANCEMATCH`` flag will cause it to attempt to improve the fit (i.e. reduce the number of errors) of the match that it has found.

The ``BESTMATCH`` flag will make it search for the best match instead.

Further examples to note:

* ``regex.search("(dog){e}", "cat and dog")[1]`` returns ``"cat"`` because that matches ``"dog"`` with 3 errors (an unlimited number of errors is permitted).

* ``regex.search("(dog){e<=1}", "cat and dog")[1]`` returns ``" dog"`` (with a leading space) because that matches ``"dog"`` with 1 error, which is within the limit.

* ``regex.search("(?e)(dog){e<=1}", "cat and dog")[1]`` returns ``"dog"`` (without a leading space) because the fuzzy search matches ``" dog"`` with 1 error, which is within the limit, and the ``(?e)`` then it attempts a better fit.

In the first two examples there are perfect matches later in the string, but in neither case is it the first possible match.

The match object has an attribute ``fuzzy_counts`` which gives the total number of substitutions, insertions and deletions.

.. sourcecode:: python

  >>> # A 'raw' fuzzy match:
  >>> regex.fullmatch(r"(?:cats|cat){e<=1}", "cat").fuzzy_counts
  (0, 0, 1)
  >>> # 0 substitutions, 0 insertions, 1 deletion.

  >>> # A better match might be possible if the ENHANCEMATCH flag used:
  >>> regex.fullmatch(r"(?e)(?:cats|cat){e<=1}", "cat").fuzzy_counts
  (0, 0, 0)
  >>> # 0 substitutions, 0 insertions, 0 deletions.

The match object also has an attribute ``fuzzy_changes`` which gives a tuple of the positions of the substitutions, insertions and deletions.

.. sourcecode:: python

  >>> m = regex.search('(fuu){i<=2,d<=2,e<=5}', 'anaconda foo bar')
  >>> m
  <regex.Match object; span=(7, 10), match='a f', fuzzy_counts=(0, 2, 2)>
  >>> m.fuzzy_changes
  ([], [7, 8], [10, 11])

What this means is that if the matched part of the string had been:

.. sourcecode:: python

  'anacondfuuoo bar'

it would've been an exact match.

However, there were insertions at positions 7 and 8:

.. sourcecode:: python

  'anaconda fuuoo bar'
          ^^

and deletions at positions 10 and 11:

.. sourcecode:: python

  'anaconda f~~oo bar'
             ^^

So the actual string was:

.. sourcecode:: python

  'anaconda foo bar'

Named lists ``\\L<name>`` (`Hg issue 11 <https://github.com/mrabarnett/mrab-regex/issues/11>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

There are occasions where you may want to include a list (actually, a set) of options in a regex.

One way is to build the pattern like this:

.. sourcecode:: python

  >>> p = regex.compile(r"first|second|third|fourth|fifth")

but if the list is large, parsing the resulting regex can take considerable time, and care must also be taken that the strings are properly escaped and properly ordered, for example, "cats" before "cat".

The new alternative is to use a named list:

.. sourcecode:: python

  >>> option_set = ["first", "second", "third", "fourth", "fifth"]
  >>> p = regex.compile(r"\\L<options>", options=option_set)

The order of the items is irrelevant, they are treated as a set. The named lists are available as the ``.named_lists`` attribute of the pattern object :

.. sourcecode:: python

  >>> print(p.named_lists)
  {'options': frozenset({'third', 'first', 'fifth', 'fourth', 'second'})}

If there are any unused keyword arguments, ``ValueError`` will be raised unless you tell it otherwise:

.. sourcecode:: python

  >>> option_set = ["first", "second", "third", "fourth", "fifth"]
  >>> p = regex.compile(r"\\L<options>", options=option_set, other_options=[])
  Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "C:\\Python310\\lib\\site-packages\\regex\\regex.py", line 353, in compile
      return _compile(pattern, flags, ignore_unused, kwargs, cache_pattern)
    File "C:\\Python310\\lib\\site-packages\\regex\\regex.py", line 500, in _compile
      complain_unused_args()
    File "C:\\Python310\\lib\\site-packages\\regex\\regex.py", line 483, in complain_unused_args
      raise ValueError('unused keyword argument {!a}'.format(any_one))
  ValueError: unused keyword argument 'other_options'
  >>> p = regex.compile(r"\\L<options>", options=option_set, other_options=[], ignore_unused=True)
  >>> p = regex.compile(r"\\L<options>", options=option_set, other_options=[], ignore_unused=False)
  Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "C:\\Python310\\lib\\site-packages\\regex\\regex.py", line 353, in compile
      return _compile(pattern, flags, ignore_unused, kwargs, cache_pattern)
    File "C:\\Python310\\lib\\site-packages\\regex\\regex.py", line 500, in _compile
      complain_unused_args()
    File "C:\\Python310\\lib\\site-packages\\regex\\regex.py", line 483, in complain_unused_args
      raise ValueError('unused keyword argument {!a}'.format(any_one))
  ValueError: unused keyword argument 'other_options'
  >>>

Start and end of word
^^^^^^^^^^^^^^^^^^^^^

``\\m`` matches at the start of a word.

``\\M`` matches at the end of a word.

Compare with ``\\b``, which matches at the start or end of a word.

Unicode line separators
^^^^^^^^^^^^^^^^^^^^^^^

Normally the only line separator is ``\\n`` (``\\x0A``), but if the ``WORD`` flag is turned on then the line separators are ``\\x0D\\x0A``, ``\\x0A``, ``\\x0B``, ``\\x0C`` and ``\\x0D``, plus ``\\x85``, ``\\u2028`` and ``\\u2029`` when working with Unicode.

This affects the regex dot ``"."``, which, with the ``DOTALL`` flag turned off, matches any character except a line separator. It also affects the line anchors ``^`` and ``$`` (in multiline mode).

Set operators
^^^^^^^^^^^^^

**Version 1 behaviour only**

Set operators have been added, and a set ``[...]`` can include nested sets.

The operators, in order of increasing precedence, are:

* ``||`` for union ("x||y" means "x or y")

* ``~~`` (double tilde) for symmetric difference ("x~~y" means "x or y, but not both")

* ``&&`` for intersection ("x&&y" means "x and y")

* ``--`` (double dash) for difference ("x--y" means "x but not y")

Implicit union, ie, simple juxtaposition like in ``[ab]``, has the highest precedence. Thus, ``[ab&&cd]`` is the same as ``[[a||b]&&[c||d]]``.

Examples:

* ``[ab]`` # Set containing 'a' and 'b'

* ``[a-z]`` # Set containing 'a' .. 'z'

* ``[[a-z]--[qw]]`` # Set containing 'a' .. 'z', but not 'q' or 'w'

* ``[a-z--qw]`` # Same as above

* ``[\\p{L}--QW]`` # Set containing all letters except 'Q' and 'W'

* ``[\\p{N}--[0-9]]`` # Set containing all numbers except '0' .. '9'

* ``[\\p{ASCII}&&\\p{Letter}]`` # Set containing all characters which are ASCII and letter

regex.escape (`issue #2650 <https://bugs.python.org/issue2650>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

regex.escape has an additional keyword parameter ``special_only``. When True, only 'special' regex characters, such as '?', are escaped.

.. sourcecode:: python

  >>> regex.escape("foo!?", special_only=False)
  'foo\\\\!\\\\?'
  >>> regex.escape("foo!?", special_only=True)
  'foo!\\\\?'

regex.escape (`Hg issue 249 <https://github.com/mrabarnett/mrab-regex/issues/249>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

regex.escape has an additional keyword parameter ``literal_spaces``. When True, spaces are not escaped.

.. sourcecode:: python

  >>> regex.escape("foo bar!?", literal_spaces=False)
  'foo\\\\ bar!\\\\?'
  >>> regex.escape("foo bar!?", literal_spaces=True)
  'foo bar!\\\\?'

Repeated captures (`issue #7132 <https://bugs.python.org/issue7132>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A match object has additional methods which return information on all the successful matches of a repeated group. These methods are:

* ``matchobject.captures([group1, ...])``

  * Returns a list of the strings matched in a group or groups. Compare with ``matchobject.group([group1, ...])``.

* ``matchobject.starts([group])``

  * Returns a list of the start positions. Compare with ``matchobject.start([group])``.

* ``matchobject.ends([group])``

  * Returns a list of the end positions. Compare with ``matchobject.end([group])``.

* ``matchobject.spans([group])``

  * Returns a list of the spans. Compare with ``matchobject.span([group])``.

.. sourcecode:: python

  >>> m = regex.search(r"(\\w{3})+", "123456789")
  >>> m.group(1)
  '789'
  >>> m.captures(1)
  ['123', '456', '789']
  >>> m.start(1)
  6
  >>> m.starts(1)
  [0, 3, 6]
  >>> m.end(1)
  9
  >>> m.ends(1)
  [3, 6, 9]
  >>> m.span(1)
  (6, 9)
  >>> m.spans(1)
  [(0, 3), (3, 6), (6, 9)]

Atomic grouping ``(?>...)`` (`issue #433030 <https://bugs.python.org/issue433030>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If the following pattern subsequently fails, then the subpattern as a whole will fail.

Possessive quantifiers
^^^^^^^^^^^^^^^^^^^^^^

``(?:...)?+`` ; ``(?:...)*+`` ; ``(?:...)++`` ; ``(?:...){min,max}+``

The subpattern is matched up to 'max' times. If the following pattern subsequently fails, then all the repeated subpatterns will fail as a whole. For example, ``(?:...)++`` is equivalent to ``(?>(?:...)+)``.

Scoped flags (`issue #433028 <https://bugs.python.org/issue433028>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``(?flags-flags:...)``

The flags will apply only to the subpattern. Flags can be turned on or off.

Definition of 'word' character (`issue #1693050 <https://bugs.python.org/issue1693050>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The definition of a 'word' character has been expanded for Unicode. It conforms to the Unicode specification at ``http://www.unicode.org/reports/tr29/``.

Variable-length lookbehind
^^^^^^^^^^^^^^^^^^^^^^^^^^

A lookbehind can match a variable-length string.

Flags argument for regex.split, regex.sub and regex.subn (`issue #3482 <https://bugs.python.org/issue3482>`_)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``regex.split``, ``regex.sub`` and ``regex.subn`` support a 'flags' argument.

Pos and endpos arguments for regex.sub and regex.subn
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``regex.sub`` and ``regex.subn`` support 'pos' and 'endpos' arguments.

'Overlapped' argument for regex.findall and regex.finditer
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``regex.findall`` and ``regex.finditer`` support an 'overlapped' flag which permits overlapped matches.

Splititer
^^^^^^^^^

``regex.splititer`` has been added. It's a generator equivalent of ``regex.split``.

Subscripting match objects for groups
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A match object accepts access to the groups via subscripting and slicing:

.. sourcecode:: python

  >>> m = regex.search(r"(?P<before>.*?)(?P<num>\\d+)(?P<after>.*)", "pqr123stu")
  >>> print(m["before"])
  pqr
  >>> print(len(m))
  4
  >>> print(m[:])
  ('pqr123stu', 'pqr', '123', 'stu')

Named groups
^^^^^^^^^^^^

Groups can be named with ``(?<name>...)`` as well as the existing ``(?P<name>...)``.

Group references
^^^^^^^^^^^^^^^^

Groups can be referenced within a pattern with ``\\g<name>``. This also allows there to be more than 99 groups.

Named characters ``\\N{name}``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Named characters are supported. Note that only those known by Python's Unicode database will be recognised.

Unicode codepoint properties, including scripts and blocks
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``\\p{property=value}``; ``\\P{property=value}``; ``\\p{value}`` ; ``\\P{value}``

Many Unicode properties are supported, including blocks and scripts. ``\\p{property=value}`` or ``\\p{property:value}`` matches a character whose property ``property`` has value ``value``. The inverse of ``\\p{property=value}`` is ``\\P{property=value}`` or ``\\p{^property=value}``.

If the short form ``\\p{value}`` is used, the properties are checked in the order: ``General_Category``, ``Script``, ``Block``, binary property:

* ``Latin``, the 'Latin' script (``Script=Latin``).

* ``BasicLatin``, the 'BasicLatin' block (``Block=BasicLatin``).

* ``Alphabetic``, the 'Alphabetic' binary property (``Alphabetic=Yes``).

A short form starting with ``Is`` indicates a script or binary property:

* ``IsLatin``, the 'Latin' script (``Script=Latin``).

* ``IsAlphabetic``, the 'Alphabetic' binary property (``Alphabetic=Yes``).

A short form starting with ``In`` indicates a block property:

* ``InBasicLatin``, the 'BasicLatin' block (``Block=BasicLatin``).

POSIX character classes
^^^^^^^^^^^^^^^^^^^^^^^

``[[:alpha:]]``; ``[[:^alpha:]]``

POSIX character classes are supported. These are normally treated as an alternative form of ``\\p{...}``.

The exceptions are ``alnum``, ``digit``, ``punct`` and ``xdigit``, whose definitions are different from those of Unicode.

``[[:alnum:]]`` is equivalent to ``\\p{posix_alnum}``.

``[[:digit:]]`` is equivalent to ``\\p{posix_digit}``.

``[[:punct:]]`` is equivalent to ``\\p{posix_punct}``.

``[[:xdigit:]]`` is equivalent to ``\\p{posix_xdigit}``.

Search anchor ``\\G``
^^^^^^^^^^^^^^^^^^^^

A search anchor has been added. It matches at the position where each search started/continued and can be used for contiguous matches or in negative variable-length lookbehinds to limit how far back the lookbehind goes:

.. sourcecode:: python

  >>> regex.findall(r"\\w{2}", "abcd ef")
  ['ab', 'cd', 'ef']
  >>> regex.findall(r"\\G\\w{2}", "abcd ef")
  ['ab', 'cd']

* The search starts at position 0 and matches 'ab'.

* The search continues at position 2 and matches 'cd'.

* The search continues at position 4 and fails to match any letters.

* The anchor stops the search start position from being advanced, so there are no more results.

Reverse searching
^^^^^^^^^^^^^^^^^

Searches can also work backwards:

.. sourcecode:: python

  >>> regex.findall(r".", "abc")
  ['a', 'b', 'c']
  >>> regex.findall(r"(?r).", "abc")
  ['c', 'b', 'a']

Note that the result of a reverse search is not necessarily the reverse of a forward search:

.. sourcecode:: python

  >>> regex.findall(r"..", "abcde")
  ['ab', 'cd']
  >>> regex.findall(r"(?r)..", "abcde")
  ['de', 'bc']

Matching a single grapheme ``\\X``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The grapheme matcher is supported. It conforms to the Unicode specification at ``http://www.unicode.org/reports/tr29/``.

Branch reset ``(?|...|...)``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Group numbers will be reused across the alternatives, but groups with different names will have different group numbers.

.. sourcecode:: python

  >>> regex.match(r"(?|(first)|(second))", "first").groups()
  ('first',)
  >>> regex.match(r"(?|(first)|(second))", "second").groups()
  ('second',)

Note that there is only one group.

Default Unicode word boundary
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``WORD`` flag changes the definition of a 'word boundary' to that of a default Unicode word boundary. This applies to ``\\b`` and ``\\B``.

Timeout
^^^^^^^

The matching methods and functions support timeouts. The timeout (in seconds) applies to the entire operation:

.. sourcecode:: python

  >>> from time import sleep
  >>>
  >>> def fast_replace(m):
  ...     return 'X'
  ...
  >>> def slow_replace(m):
  ...     sleep(0.5)
  ...     return 'X'
  ...
  >>> regex.sub(r'[a-z]', fast_replace, 'abcde', timeout=2)
  'XXXXX'
  >>> regex.sub(r'[a-z]', slow_replace, 'abcde', timeout=2)
  Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    File "C:\\Python310\\lib\\site-packages\\regex\\regex.py", line 278, in sub
      return pat.sub(repl, string, count, pos, endpos, concurrent, timeout)
  TimeoutError: regex timed out
"""^^xsd:string ;
    schema1:name "regex"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2024.7.24"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/rfc3986-validator> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.1.0> .

<https://data.siemens-energy.com/cybersec/package/ruff> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.5.2> .

<https://data.siemens-energy.com/cybersec/package/s3fs> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2022.11.0> .

<https://data.siemens-energy.com/cybersec/package/scipy> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.10.0> .

<https://data.siemens-energy.com/cybersec/package/setuptools_scm> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/7> .

<https://data.siemens-energy.com/cybersec/package/skia-pathops> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.5.0> .

<https://data.siemens-energy.com/cybersec/package/soupsieve> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.2> ;
    schema1:description """[![Donate via PayPal][donate-image]][donate-link]
[![Build][github-ci-image]][github-ci-link]
[![Coverage Status][codecov-image]][codecov-link]
[![PyPI Version][pypi-image]][pypi-link]
[![PyPI Downloads][pypi-down]][pypi-link]
[![PyPI - Python Version][python-image]][pypi-link]
![License][license-image-mit]

# Soup Sieve

## Overview

Soup Sieve is a CSS selector library designed to be used with [Beautiful Soup 4][bs4]. It aims to provide selecting,
matching, and filtering using modern CSS selectors. Soup Sieve currently provides selectors from the CSS level 1
specifications up through the latest CSS level 4 drafts and beyond (though some are not yet implemented).

Soup Sieve was written with the intent to replace Beautiful Soup's builtin select feature, and as of Beautiful Soup
version 4.7.0, it now is :confetti_ball:. Soup Sieve can also be imported in order to use its API directly for
more controlled, specialized parsing.

Soup Sieve has implemented most of the CSS selectors up through the latest CSS draft specifications, though there are a
number that don't make sense in a non-browser environment. Selectors that cannot provide meaningful functionality simply
do not match anything. Some of the supported selectors are:

- `.classes`
- `#ids`
- `[attributes=value]`
- `parent child`
- `parent > child`
- `sibling ~ sibling`
- `sibling + sibling`
- `:not(element.class, element2.class)`
- `:is(element.class, element2.class)`
- `parent:has(> child)`
- and [many more](https://facelessuser.github.io/soupsieve/selectors/)


## Installation

You must have Beautiful Soup already installed:

```
pip install beautifulsoup4
```

In most cases, assuming you've installed version 4.7.0, that should be all you need to do, but if you've installed via
some alternative method, and Soup Sieve is not automatically installed, you can install it directly:

```
pip install soupsieve
```

If you want to manually install it from source, first ensure that [`build`](https://pypi.org/project/build/) is
installed:

```
pip install build
```

Then navigate to the root of the project and build the wheel and install (replacing `<ver>` with the current version):

```
python -m build -w
pip install dist/soupsieve-<ver>-py3-none-any.whl
```

## Documentation

Documentation is found here: https://facelessuser.github.io/soupsieve/.

## License

MIT

[bs4]: https://beautiful-soup-4.readthedocs.io/en/latest/#

[github-ci-image]: https://github.com/facelessuser/soupsieve/workflows/build/badge.svg?branch=master&event=push
[github-ci-link]: https://github.com/facelessuser/soupsieve/actions?query=workflow%3Abuild+branch%3Amaster
[codecov-image]: https://img.shields.io/codecov/c/github/facelessuser/soupsieve/master.svg?logo=codecov&logoColor=aaaaaa&labelColor=333333
[codecov-link]: https://codecov.io/github/facelessuser/soupsieve
[pypi-image]: https://img.shields.io/pypi/v/soupsieve.svg?logo=pypi&logoColor=aaaaaa&labelColor=333333
[pypi-down]: https://img.shields.io/pypi/dm/soupsieve.svg?logo=pypi&logoColor=aaaaaa&labelColor=333333
[pypi-link]: https://pypi.python.org/pypi/soupsieve
[python-image]: https://img.shields.io/pypi/pyversions/soupsieve?logo=python&logoColor=aaaaaa&labelColor=333333
[license-image-mit]: https://img.shields.io/badge/license-MIT-blue.svg?labelColor=333333
[donate-image]: https://img.shields.io/badge/Donate-PayPal-3fabd1?logo=paypal
[donate-link]: https://www.paypal.me/facelessuser
"""^^xsd:string ;
    schema1:name "soupsieve"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.6"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/sphinx-argparse> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.4> .

<https://data.siemens-energy.com/cybersec/package/sphinx-argparse-cli> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.5> .

<https://data.siemens-energy.com/cybersec/package/sphinx-issues> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.0.0> .

<https://data.siemens-energy.com/cybersec/package/sphinx-notfound-page> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2> .

<https://data.siemens-energy.com/cybersec/package/sphinx-reredirects> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.1.2> .

<https://data.siemens-energy.com/cybersec/package/sphinx-rtd-dark-mode> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.3.0> .

<https://data.siemens-energy.com/cybersec/package/sphinx-rtd-theme> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.0.0> .

<https://data.siemens-energy.com/cybersec/package/sphinx_rtd_theme> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.3.0> .

<https://data.siemens-energy.com/cybersec/package/sphinxcontrib-apidoc> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.4.0> .

<https://data.siemens-energy.com/cybersec/package/sphinxcontrib-towncrier> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.2.1> .

<https://data.siemens-energy.com/cybersec/package/sqlalchemy> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.0.0> .

<https://data.siemens-energy.com/cybersec/package/tables> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.8.0> .

<https://data.siemens-energy.com/cybersec/package/tabulate> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.9.0> .

<https://data.siemens-energy.com/cybersec/package/time-machine> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.10> .

<https://data.siemens-energy.com/cybersec/package/tinycss2> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/webencodings> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.3> ;
    schema1:description """tinycss2 is a low-level CSS parser and generator written in Python: it can
parse strings, return objects representing tokens and blocks, and generate CSS
strings corresponding to these objects.

Based on the CSS Syntax Level 3 specification, tinycss2 knows the grammar of
CSS but doesn't know specific rules, properties or values supported in various
CSS modules.

* Free software: BSD license
* For Python 3.8+, tested on CPython and PyPy
* Documentation: https://doc.courtbouillon.org/tinycss2
* Changelog: https://github.com/Kozea/tinycss2/releases
* Code, issues, tests: https://github.com/Kozea/tinycss2
* Code of conduct: https://www.courtbouillon.org/code-of-conduct
* Professional support: https://www.courtbouillon.org
* Donation: https://opencollective.com/courtbouillon

tinycss2 has been created and developed by Kozea (https://kozea.fr).
Professional support, maintenance and community management is provided by
CourtBouillon (https://www.courtbouillon.org).

Copyrights are retained by their contributors, no copyright assignment is
required to contribute to tinycss2. Unless explicitly stated otherwise, any
contribution intentionally submitted for inclusion is licensed under the BSD
3-clause license, without any additional terms or conditions. For full
authorship information, see the version control history.

"""^^xsd:string ;
    schema1:name "tinycss2"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.3.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/tomli-w> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.0.0> .

<https://data.siemens-energy.com/cybersec/package/tomlkit> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.10.1> ;
    schema1:description """[github_release]: https://img.shields.io/github/release/sdispater/tomlkit.svg?logo=github&logoColor=white
[pypi_version]: https://img.shields.io/pypi/v/tomlkit.svg?logo=python&logoColor=white
[python_versions]: https://img.shields.io/pypi/pyversions/tomlkit.svg?logo=python&logoColor=white
[github_license]: https://img.shields.io/github/license/sdispater/tomlkit.svg?logo=github&logoColor=white
[github_action]: https://github.com/sdispater/tomlkit/actions/workflows/tests.yml/badge.svg

[![GitHub Release][github_release]](https://github.com/sdispater/tomlkit/releases/)
[![PyPI Version][pypi_version]](https://pypi.org/project/tomlkit/)
[![Python Versions][python_versions]](https://pypi.org/project/tomlkit/)
[![License][github_license]](https://github.com/sdispater/tomlkit/blob/master/LICENSE)
<br>
[![Tests][github_action]](https://github.com/sdispater/tomlkit/actions/workflows/tests.yml)

# TOML Kit - Style-preserving TOML library for Python

TOML Kit is a **1.0.0-compliant** [TOML](https://toml.io/) library.

It includes a parser that preserves all comments, indentations, whitespace and internal element ordering,
and makes them accessible and editable via an intuitive API.

You can also create new TOML documents from scratch using the provided helpers.

Part of the implementation has been adapted, improved and fixed from [Molten](https://github.com/LeopoldArkham/Molten).

## Usage

See the [documentation](https://tomlkit.readthedocs.io/) for more information.

## Installation

If you are using [Poetry](https://poetry.eustace.io),
add `tomlkit` to your `pyproject.toml` file by using:

```bash
poetry add tomlkit
```

If not, you can use `pip`:

```bash
pip install tomlkit
```

## Running tests

Please clone the repo with submodules with the following command
`git clone --recurse-submodules https://github.com/sdispater/tomlkit.git`.
We need the submodule - `toml-test` for running the tests.

You can run the tests with `poetry run pytest -q tests`

"""^^xsd:string ;
    schema1:name "tomlkit"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.13.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/trove-classifiers> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2024.10.12> .

<https://data.siemens-energy.com/cybersec/package/types-python-dateutil> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.8.10> ;
    schema1:description """## Typing stubs for python-dateutil

This is a [PEP 561](https://peps.python.org/pep-0561/)
type stub package for the [`python-dateutil`](https://github.com/dateutil/dateutil) package.
It can be used by type-checking tools like
[mypy](https://github.com/python/mypy/),
[pyright](https://github.com/microsoft/pyright),
[pytype](https://github.com/google/pytype/),
PyCharm, etc. to check code that uses
`python-dateutil`.

This version of `types-python-dateutil` aims to provide accurate annotations
for `python-dateutil==2.9.*`.
The source for this package can be found at
https://github.com/python/typeshed/tree/main/stubs/python-dateutil. All fixes for
types and metadata should be contributed there.

This stub package is marked as [partial](https://peps.python.org/pep-0561/#partial-stub-packages).
If you find that annotations are missing, feel free to contribute and help complete them.


See https://github.com/python/typeshed/blob/main/README.md for more details.
This package was generated from typeshed commit
[`b19d9670f9cdef5eedd97dc2455cdf2822c95e7d`](https://github.com/python/typeshed/commit/b19d9670f9cdef5eedd97dc2455cdf2822c95e7d) and was tested
with mypy 1.11.1, pyright 1.1.376, and
pytype 2024.4.11.
"""^^xsd:string ;
    schema1:name "types-python-dateutil"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.9.0.20240821"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/tzdata> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2022.7> ;
    schema1:description """tzdata: Python package providing IANA time zone data
====================================================

This is a Python package containing ``zic``-compiled binaries for the IANA time
zone database. It is intended to be a fallback for systems that do not have
system time zone data installed (or don't have it installed in a standard
location), as a part of `PEP 615 <https://www.python.org/dev/peps/pep-0615/>`_

This repository generates a ``pip``-installable package, published on PyPI as
`tzdata <https://pypi.org/project/tzdata>`_.

For more information, see `the documentation <https://tzdata.readthedocs.io>`_.
"""^^xsd:string ;
    schema1:name "tzdata"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2024.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/uharfbuzz> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.23.0> .

<https://data.siemens-energy.com/cybersec/package/unicodedata2> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/15.1.0> .

<https://data.siemens-energy.com/cybersec/package/urllib3> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/brotli>,
        <https://data.siemens-energy.com/cybersec/package/brotlicffi>,
        <https://data.siemens-energy.com/cybersec/package/h2>,
        <https://data.siemens-energy.com/cybersec/package/pysocks>,
        <https://data.siemens-energy.com/cybersec/package/zstandard> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3> ;
    schema1:description """<h1 align="center">

![urllib3](https://github.com/urllib3/urllib3/raw/main/docs/_static/banner_github.svg)

</h1>

<p align="center">
  <a href="https://pypi.org/project/urllib3"><img alt="PyPI Version" src="https://img.shields.io/pypi/v/urllib3.svg?maxAge=86400" /></a>
  <a href="https://pypi.org/project/urllib3"><img alt="Python Versions" src="https://img.shields.io/pypi/pyversions/urllib3.svg?maxAge=86400" /></a>
  <a href="https://discord.gg/urllib3"><img alt="Join our Discord" src="https://img.shields.io/discord/756342717725933608?color=%237289da&label=discord" /></a>
  <a href="https://github.com/urllib3/urllib3/actions?query=workflow%3ACI"><img alt="Coverage Status" src="https://img.shields.io/badge/coverage-100%25-success" /></a>
  <a href="https://github.com/urllib3/urllib3/actions?query=workflow%3ACI"><img alt="Build Status on GitHub" src="https://github.com/urllib3/urllib3/workflows/CI/badge.svg" /></a>
  <a href="https://urllib3.readthedocs.io"><img alt="Documentation Status" src="https://readthedocs.org/projects/urllib3/badge/?version=latest" /></a><br>
  <a href="https://deps.dev/pypi/urllib3"><img alt="OpenSSF Scorecard" src="https://api.securityscorecards.dev/projects/github.com/urllib3/urllib3/badge" /></a>
  <a href="https://slsa.dev"><img alt="SLSA 3" src="https://slsa.dev/images/gh-badge-level3.svg" /></a>
  <a href="https://bestpractices.coreinfrastructure.org/projects/6227"><img alt="CII Best Practices" src="https://bestpractices.coreinfrastructure.org/projects/6227/badge" /></a>
</p>

urllib3 is a powerful, *user-friendly* HTTP client for Python. Much of the
Python ecosystem already uses urllib3 and you should too.
urllib3 brings many critical features that are missing from the Python
standard libraries:

- Thread safety.
- Connection pooling.
- Client-side SSL/TLS verification.
- File uploads with multipart encoding.
- Helpers for retrying requests and dealing with HTTP redirects.
- Support for gzip, deflate, brotli, and zstd encoding.
- Proxy support for HTTP and SOCKS.
- 100% test coverage.

urllib3 is powerful and easy to use:

```python3
>>> import urllib3
>>> resp = urllib3.request("GET", "http://httpbin.org/robots.txt")
>>> resp.status
200
>>> resp.data
b"User-agent: *\\nDisallow: /deny\\n"
```

## Installing

urllib3 can be installed with [pip](https://pip.pypa.io):

```bash
$ python -m pip install urllib3
```

Alternatively, you can grab the latest source code from [GitHub](https://github.com/urllib3/urllib3):

```bash
$ git clone https://github.com/urllib3/urllib3.git
$ cd urllib3
$ pip install .
```


## Documentation

urllib3 has usage and reference documentation at [urllib3.readthedocs.io](https://urllib3.readthedocs.io).


## Community

urllib3 has a [community Discord channel](https://discord.gg/urllib3) for asking questions and
collaborating with other contributors. Drop by and say hello ðŸ‘‹


## Contributing

urllib3 happily accepts contributions. Please see our
[contributing documentation](https://urllib3.readthedocs.io/en/latest/contributing.html)
for some tips on getting started.


## Security Disclosures

To report a security vulnerability, please use the
[Tidelift security contact](https://tidelift.com/security).
Tidelift will coordinate the fix and disclosure with maintainers.


## Maintainers

- [@sethmlarson](https://github.com/sethmlarson) (Seth M. Larson)
- [@pquentin](https://github.com/pquentin) (Quentin Pradet)
- [@illia-v](https://github.com/illia-v) (Illia Volochii)
- [@theacodes](https://github.com/theacodes) (Thea Flowers)
- [@haikuginger](https://github.com/haikuginger) (Jess Shapiro)
- [@lukasa](https://github.com/lukasa) (Cory Benfield)
- [@sigmavirus24](https://github.com/sigmavirus24) (Ian Stapleton Cordasco)
- [@shazow](https://github.com/shazow) (Andrey Petrov)

ðŸ‘‹


## Sponsorship

If your company benefits from this library, please consider [sponsoring its
development](https://urllib3.readthedocs.io/en/latest/sponsors.html).


## For Enterprise

Professional support for urllib3 is available as part of the [Tidelift
Subscription][1].  Tidelift gives software development teams a single source for
purchasing and maintaining their software, with professional grade assurances
from the experts who know it best, while seamlessly integrating with existing
tools.

[1]: https://tidelift.com/subscription/pkg/pypi-urllib3?utm_source=pypi-urllib3&utm_medium=referral&utm_campaign=readme
"""^^xsd:string ;
    schema1:name "urllib3"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.2.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/uv> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.1.18> .

<https://data.siemens-energy.com/cybersec/package/watchdog> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.3> .

<https://data.siemens-energy.com/cybersec/package/webcolors> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.11>,
        <https://data.siemens-energy.com/cybersec/version/24.6.0> ;
    schema1:description """.. -*-restructuredtext-*-

.. image:: https://github.com/ubernostrum/webcolors/workflows/CI/badge.svg
   :alt: CI status image
   :target: https://github.com/ubernostrum/webcolors/actions?query=workflow%3ACI

``webcolors`` is a module for working with and converting between the
various HTML/CSS color formats.

Support is included for normalizing and converting between the
following formats (RGB colorspace only; conversion to/from HSL can be
handled by the ``colorsys`` module in the Python standard library):

* Specification-defined color names

* Six-digit hexadecimal

* Three-digit hexadecimal

* Integer ``rgb()`` triplet

* Percentage ``rgb()`` triplet

For example:

.. code-block:: python

    >>> import webcolors
    >>> webcolors.hex_to_name("#daa520")
    'goldenrod'

Implementations are also provided for the HTML5 color parsing and
serialization algorithms. For example, parsing the infamous
"chucknorris" string into an ``rgb()`` triplet:

.. code-block:: python

    >>> import webcolors
    >>> webcolors.html5_parse_legacy_color("chucknorris")
    HTML5SimpleColor(red=192, green=0, blue=0)

Full documentation is `available online <https://webcolors.readthedocs.io/>`_.
"""^^xsd:string ;
    schema1:name "webcolors"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "24.8.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/webencodings> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.4> ;
    schema1:description """python-webencodings
===================

This is a Python implementation of the `WHATWG Encoding standard
<http://encoding.spec.whatwg.org/>`_.

* Latest documentation: http://packages.python.org/webencodings/
* Source code and issue tracker:
  https://github.com/gsnedders/python-webencodings
* PyPI releases: http://pypi.python.org/pypi/webencodings
* License: BSD
* Python 2.6+ and 3.3+

In order to be compatible with legacy web content
when interpreting something like ``Content-Type: text/html; charset=latin1``,
tools need to use a particular set of aliases for encoding labels
as well as some overriding rules.
For example, ``US-ASCII`` and ``iso-8859-1`` on the web are actually
aliases for ``windows-1252``, and an UTF-8 or UTF-16 BOM takes precedence
over any other encoding declaration.
The Encoding standard defines all such details so that implementations do
not have to reverse-engineer each other.

This module has encoding labels and BOM detection,
but the actual implementation for encoders and decoders is Pythonâ€™s.


"""^^xsd:string ;
    schema1:name "webencodings"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.5.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/xarray> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2022.12.0> .

<https://data.siemens-energy.com/cybersec/package/xlrd> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.0.1> .

<https://data.siemens-energy.com/cybersec/package/xlsxwriter> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.0.5> .

<https://data.siemens-energy.com/cybersec/package/zopfli> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.1.4> .

<https://data.siemens-energy.com/cybersec/version/0.0.5> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "5"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.1.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.1.18> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "18"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.1.2> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.1.4> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "4"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.10> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "10"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.10.1> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "10"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.11.2> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "11"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.13> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "13"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.16> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "16"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.17.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "17"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.18.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "18"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.2> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.2.12> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "12"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.21> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "21"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.23.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "23"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.28.4> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "4"^^xsd:int ;
    software_vocab:version_minor "28"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.3.6> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "6"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.3.7> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "7"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.31.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "31"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.36.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "36"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.4.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.4.5> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "5"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.4.6> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "6"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.4.8> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "8"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.43.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "43"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.44.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "44"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.5> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "5"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.5.2> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "5"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.56.4> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "4"^^xsd:int ;
    software_vocab:version_minor "56"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.6.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "6"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.7> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.7.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.7.1> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.8> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "8"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.8.2> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "8"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.9.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "9"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.0.10> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "10"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.0.2> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.0.9> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "9"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.1.0> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.10> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "10"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.10.0> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "10"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.11> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "11"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.11.1> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "11"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.12.0> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "12"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.13> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "13"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.2> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.21> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "21"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.22.4> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "4"^^xsd:int ;
    software_vocab:version_minor "22"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.23> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "23"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.23.2> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "23"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.23.4> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "4"^^xsd:int ;
    software_vocab:version_minor "23"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.25.2> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "25"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.26.0> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "26"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.3.1> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.3.10> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "10"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.3.6> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "6"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.34> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "34"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.4.0> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.4.4> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "4"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.7> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.7.0> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.7.2> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.7.4.2> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "4"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.8> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "8"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.9> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "9"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.9.0> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "9"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/10.0.1> software_vocab:version_major "10"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/13.0.0> software_vocab:version_major "13"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/15.1.0> software_vocab:version_major "15"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/18.5> software_vocab:version_major "18"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "5"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.1> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.1.0> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.1.1> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.10> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "10"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.12> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "12"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.13.3> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "3"^^xsd:int ;
    software_vocab:version_minor "13"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.15> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "15"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.23.4> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "4"^^xsd:int ;
    software_vocab:version_minor "23"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.3.0> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.3.1> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.4> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.4.0> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.4.1> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.5> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "5"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.6> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "6"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.6.2> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "6"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.7.0> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.8.10> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "10"^^xsd:int ;
    software_vocab:version_minor "8"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.8.2> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "8"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.8.4> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "4"^^xsd:int ;
    software_vocab:version_minor "8"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.9.6> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "6"^^xsd:int ;
    software_vocab:version_minor "9"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/20.0> software_vocab:version_major "20"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/20.0.35> software_vocab:version_major "20"^^xsd:int ;
    software_vocab:version_micro "35"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/20.26.2> software_vocab:version_major "20"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "26"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2017.4.17> software_vocab:version_major "2017"^^xsd:int ;
    software_vocab:version_micro "17"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2019.02.19> software_vocab:version_major "2019"^^xsd:int ;
    software_vocab:version_micro "19"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2020.1> software_vocab:version_major "2020"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2021.1> software_vocab:version_major "2021"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2022.7> software_vocab:version_major "2022"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2022.7.29> software_vocab:version_major "2022"^^xsd:int ;
    software_vocab:version_micro "29"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2023.03.6> software_vocab:version_major "2023"^^xsd:int ;
    software_vocab:version_micro "6"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2023.08.17> software_vocab:version_major "2023"^^xsd:int ;
    software_vocab:version_micro "17"^^xsd:int ;
    software_vocab:version_minor "8"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2023.7.26> software_vocab:version_major "2023"^^xsd:int ;
    software_vocab:version_micro "26"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2023.9.10> software_vocab:version_major "2023"^^xsd:int ;
    software_vocab:version_micro "10"^^xsd:int ;
    software_vocab:version_minor "9"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2024.10.12> software_vocab:version_major "2024"^^xsd:int ;
    software_vocab:version_micro "12"^^xsd:int ;
    software_vocab:version_minor "10"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/22.2> software_vocab:version_major "22"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/22.2.0> software_vocab:version_major "22"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/23.0> software_vocab:version_major "23"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/23.1> software_vocab:version_major "23"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/23.2> software_vocab:version_major "23"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/23.6> software_vocab:version_major "23"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "6"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/24> software_vocab:version_major "24"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/24.6.0> software_vocab:version_major "24"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "6"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/24.7> software_vocab:version_major "24"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.0> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.0.0> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.0.11> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "11"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.0.3> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "3"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.0.37> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "37"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.0.5> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "5"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.1> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.1.0> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.10.0> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "10"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.11.1> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "11"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.2> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.2.0> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.5> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "5"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.6> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "6"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.6.3> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "3"^^xsd:int ;
    software_vocab:version_minor "6"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.7> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.7.4.3> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "4"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.8.0> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "8"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/300> software_vocab:version_major "300"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.0.0> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.1> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.11.2> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "11"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.12.2> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "12"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.22.0> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "22"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.3> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.3.0> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.6> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "6"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.6.0> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "6"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.6.1> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "6"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.7> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.8> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "8"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.8.3> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "3"^^xsd:int ;
    software_vocab:version_minor "8"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.9.2> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "9"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/42.0.0> software_vocab:version_major "42"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/5.0.0> software_vocab:version_major "5"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/5.10.2> software_vocab:version_major "5"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "10"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/5.13.0> software_vocab:version_major "5"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "13"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/5.15> software_vocab:version_major "5"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "15"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/5.15.9> software_vocab:version_major "5"^^xsd:int ;
    software_vocab:version_micro "9"^^xsd:int ;
    software_vocab:version_minor "15"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/5.3> software_vocab:version_major "5"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/5.4> software_vocab:version_major "5"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/5.7> software_vocab:version_major "5"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/56.0.0> software_vocab:version_major "56"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/6.1> software_vocab:version_major "6"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/6.1.12> software_vocab:version_major "6"^^xsd:int ;
    software_vocab:version_micro "12"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/6.14> software_vocab:version_major "6"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "14"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/6.19.3> software_vocab:version_major "6"^^xsd:int ;
    software_vocab:version_micro "3"^^xsd:int ;
    software_vocab:version_minor "19"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/6.2> software_vocab:version_major "6"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/6.2.4> software_vocab:version_major "6"^^xsd:int ;
    software_vocab:version_micro "4"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/6.46.1> software_vocab:version_major "6"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "46"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/6.6> software_vocab:version_major "6"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "6"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/64> software_vocab:version_major "64"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/67.8.0> software_vocab:version_major "67"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "8"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/68> software_vocab:version_major "68"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7.0.2> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7.1> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7.2> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7.2.0> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7.2.6> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "6"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7.2.7> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "7"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7.3> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7.4> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7.4.2> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7.4.3> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "3"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7.5> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "5"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/8.0.1> software_vocab:version_major "8"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/8.1> software_vocab:version_major "8"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/8.1.3> software_vocab:version_major "8"^^xsd:int ;
    software_vocab:version_micro "3"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/8.2> software_vocab:version_major "8"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/8.2.0> software_vocab:version_major "8"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/8.8> software_vocab:version_major "8"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "8"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/9.1> software_vocab:version_major "9"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/package/MarkupSafe> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.0>,
        <https://data.siemens-energy.com/cybersec/version/2.1.1> ;
    schema1:description """MarkupSafe
==========

MarkupSafe implements a text object that escapes characters so it is
safe to use in HTML and XML. Characters that have special meanings are
replaced so that they display as the actual characters. This mitigates
injection attacks, meaning untrusted user input can safely be displayed
on a page.


Installing
----------

Install and update using `pip`_:

.. code-block:: text

    pip install -U MarkupSafe

.. _pip: https://pip.pypa.io/en/stable/getting-started/


Examples
--------

.. code-block:: pycon

    >>> from markupsafe import Markup, escape

    >>> # escape replaces special characters and wraps in Markup
    >>> escape("<script>alert(document.cookie);</script>")
    Markup('&lt;script&gt;alert(document.cookie);&lt;/script&gt;')

    >>> # wrap in Markup to mark text "safe" and prevent escaping
    >>> Markup("<strong>Hello</strong>")
    Markup('<strong>hello</strong>')

    >>> escape(Markup("<strong>Hello</strong>"))
    Markup('<strong>hello</strong>')

    >>> # Markup is a str subclass
    >>> # methods and operators escape their arguments
    >>> template = Markup("Hello <em>{name}</em>")
    >>> template.format(name='"World"')
    Markup('Hello <em>&#34;World&#34;</em>')


Donate
------

The Pallets organization develops and supports MarkupSafe and other
popular packages. In order to grow the community of contributors and
users, and allow the maintainers to devote more time to the projects,
`please donate today`_.

.. _please donate today: https://palletsprojects.com/donate


Links
-----

-   Documentation: https://markupsafe.palletsprojects.com/
-   Changes: https://markupsafe.palletsprojects.com/changes/
-   PyPI Releases: https://pypi.org/project/MarkupSafe/
-   Source Code: https://github.com/pallets/markupsafe/
-   Issue Tracker: https://github.com/pallets/markupsafe/issues/
-   Chat: https://discord.gg/pallets
"""^^xsd:string ;
    schema1:name "MarkupSafe"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.1.5"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/astroid> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/typing-extensions> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2>,
        <https://data.siemens-energy.com/cybersec/version/3.4.0>,
        <https://data.siemens-energy.com/cybersec/version/4> ;
    schema1:description """Astroid
=======

.. image:: https://codecov.io/gh/pylint-dev/astroid/branch/main/graph/badge.svg?token=Buxy4WptLb
    :target: https://codecov.io/gh/pylint-dev/astroid
    :alt: Coverage badge from codecov

.. image:: https://readthedocs.org/projects/astroid/badge/?version=latest
    :target: http://astroid.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/ambv/black

.. image:: https://results.pre-commit.ci/badge/github/pylint-dev/astroid/main.svg
   :target: https://results.pre-commit.ci/latest/github/pylint-dev/astroid/main
   :alt: pre-commit.ci status

.. |tidelift_logo| image:: https://raw.githubusercontent.com/pylint-dev/astroid/main/doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White.png
   :width: 200
   :alt: Tidelift

.. list-table::
   :widths: 10 100

   * - |tidelift_logo|
     - Professional support for astroid is available as part of the
       `Tidelift Subscription`_.  Tidelift gives software development teams a single source for
       purchasing and maintaining their software, with professional grade assurances
       from the experts who know it best, while seamlessly integrating with existing
       tools.

.. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-astroid?utm_source=pypi-astroid&utm_medium=referral&utm_campaign=readme



What's this?
------------

The aim of this module is to provide a common base representation of
python source code. It is currently the library powering pylint's capabilities.

It provides a compatible representation which comes from the `_ast`
module.  It rebuilds the tree generated by the builtin _ast module by
recursively walking down the AST and building an extended ast. The new
node classes have additional methods and attributes for different
usages. They include some support for static inference and local name
scopes. Furthermore, astroid can also build partial trees by inspecting living
objects.


Installation
------------

Extract the tarball, jump into the created directory and run::

    pip install .


If you want to do an editable installation, you can run::

    pip install -e .


If you have any questions, please mail the code-quality@python.org
mailing list for support. See
http://mail.python.org/mailman/listinfo/code-quality for subscription
information and archives.

Documentation
-------------
http://astroid.readthedocs.io/en/latest/


Python Versions
---------------

astroid 2.0 is currently available for Python 3 only. If you want Python 2
support, use an older version of astroid (though note that these versions
are no longer supported).

Test
----

Tests are in the 'test' subdirectory. To launch the whole tests suite, you can use
either `tox` or `pytest`::

    tox
    pytest
"""^^xsd:string ;
    schema1:name "astroid"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.3.5"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/asttokens> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/astroid>,
        <https://data.siemens-energy.com/cybersec/package/six> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.1.0> ;
    schema1:description """ASTTokens
=========

.. image:: https://img.shields.io/pypi/v/asttokens.svg
    :target: https://pypi.python.org/pypi/asttokens/
.. image:: https://img.shields.io/pypi/pyversions/asttokens.svg
    :target: https://pypi.python.org/pypi/asttokens/
.. image:: https://github.com/gristlabs/asttokens/actions/workflows/build-and-test.yml/badge.svg
    :target: https://github.com/gristlabs/asttokens/actions/workflows/build-and-test.yml
.. image:: https://readthedocs.org/projects/asttokens/badge/?version=latest
    :target: http://asttokens.readthedocs.io/en/latest/index.html
.. image:: https://coveralls.io/repos/github/gristlabs/asttokens/badge.svg
    :target: https://coveralls.io/github/gristlabs/asttokens

.. Start of user-guide

The ``asttokens`` module annotates Python abstract syntax trees (ASTs) with the positions of tokens
and text in the source code that generated them.

It makes it possible for tools that work with logical AST nodes to find the particular text that
resulted in those nodes, for example for automated refactoring or highlighting.

Installation
------------
asttokens is available on PyPI: https://pypi.python.org/pypi/asttokens/::

    pip install asttokens

The code is on GitHub: https://github.com/gristlabs/asttokens.

The API Reference is here: http://asttokens.readthedocs.io/en/latest/api-index.html.

Usage
-----
ASTTokens works with both Python2 and Python3.

ASTTokens can annotate both trees built by `ast <https://docs.python.org/2/library/ast.html>`_,
AND those built by `astroid <https://github.com/PyCQA/astroid>`_.

Here's an example:

.. code-block:: python

    import asttokens, ast
    source = "Robot('blue').walk(steps=10*n)"
    atok = asttokens.ASTTokens(source, parse=True)

Once the tree has been marked, nodes get ``.first_token``, ``.last_token`` attributes, and
the ``ASTTokens`` object offers helpful methods:

.. code-block:: python

    attr_node = next(n for n in ast.walk(atok.tree) if isinstance(n, ast.Attribute))
    print(atok.get_text(attr_node))
    start, end = attr_node.last_token.startpos, attr_node.last_token.endpos
    print(atok.text[:start] + 'RUN' + atok.text[end:])

Which produces this output:

.. code-block:: text

    Robot('blue').walk
    Robot('blue').RUN(steps=10*n)

The ``ASTTokens`` object also offers methods to walk and search the list of tokens that make up
the code (or a particular AST node), which is more useful and powerful than dealing with the text
directly.


Contribute
----------

To contribute:

1. Fork this repository, and clone your fork.
2. Install the package with test dependencies (ideally in a virtualenv) with::

    pip install -e '.[test]'

3. Run tests in your current interpreter with the command ``pytest`` or ``python -m pytest``.
4. Run tests across all supported interpreters with the ``tox`` command. You will need to have the interpreters installed separately. We recommend ``pyenv`` for that. Use ``tox -p auto`` to run the tests in parallel.
5. By default certain tests which take a very long time to run are skipped, but they are run on travis CI. To run them locally, set the environment variable ``ASTTOKENS_SLOW_TESTS``. For example run ``ASTTOKENS_SLOW_TESTS=1 tox`` to run the full suite of tests.
"""^^xsd:string ;
    schema1:name "asttokens"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.4.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/attrs> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/mypy>,
        <https://data.siemens-energy.com/cybersec/package/pytest>,
        <https://data.siemens-energy.com/cybersec/package/towncrier> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/22.2.0> ;
    schema1:description """<p align="center">
  <a href="https://www.attrs.org/">
    <img src="https://raw.githubusercontent.com/python-attrs/attrs/main/docs/_static/attrs_logo.svg" width="35%" alt="attrs" />
  </a>
</p>


*attrs* is the Python package that will bring back the **joy** of **writing classes** by relieving you from the drudgery of implementing object protocols (aka [dunder methods](https://www.attrs.org/en/latest/glossary.html#term-dunder-methods)).
[Trusted by NASA](https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-profile/customizing-your-profile/personalizing-your-profile#list-of-qualifying-repositories-for-mars-2020-helicopter-contributor-achievement) for Mars missions since 2020!

Its main goal is to help you to write **concise** and **correct** software without slowing down your code.


## Sponsors

*attrs* would not be possible without our [amazing sponsors](https://github.com/sponsors/hynek).
Especially those generously supporting us at the *The Organization* tier and higher:

<!-- sponsor-break-begin -->

<p align="center">

<!-- [[[cog
import pathlib, tomllib

for sponsor in tomllib.loads(pathlib.Path("pyproject.toml").read_text())["tool"]["sponcon"]["sponsors"]:
      print(f'<a href="{sponsor["url"]}"><img title="{sponsor["title"]}" src="https://www.attrs.org/en/24.2.0/_static/sponsors/{sponsor["img"]}" width="190" /></a>')
]]] -->
<a href="https://www.variomedia.de/"><img title="Variomedia AG" src="https://www.attrs.org/en/24.2.0/_static/sponsors/Variomedia.svg" width="190" /></a>
<a href="https://tidelift.com/?utm_source=lifter&utm_medium=referral&utm_campaign=hynek"><img title="Tidelift" src="https://www.attrs.org/en/24.2.0/_static/sponsors/Tidelift.svg" width="190" /></a>
<a href="https://klaviyo.com/"><img title="Klaviyo" src="https://www.attrs.org/en/24.2.0/_static/sponsors/Klaviyo.svg" width="190" /></a>
<a href="https://filepreviews.io/"><img title="FilePreviews" src="https://www.attrs.org/en/24.2.0/_static/sponsors/FilePreviews.svg" width="190" /></a>
<!-- [[[end]]] -->

</p>

<!-- sponsor-break-end -->

<p align="center">
   <strong>Please consider <a href="https://github.com/sponsors/hynek">joining them</a> to help make <em>attrs</em>â€™s maintenance more sustainable!</strong>
</p>

<!-- teaser-end -->

## Example

*attrs* gives you a class decorator and a way to declaratively define the attributes on that class:

<!-- code-begin -->

```pycon
>>> from attrs import asdict, define, make_class, Factory

>>> @define
... class SomeClass:
...     a_number: int = 42
...     list_of_numbers: list[int] = Factory(list)
...
...     def hard_math(self, another_number):
...         return self.a_number + sum(self.list_of_numbers) * another_number


>>> sc = SomeClass(1, [1, 2, 3])
>>> sc
SomeClass(a_number=1, list_of_numbers=[1, 2, 3])

>>> sc.hard_math(3)
19
>>> sc == SomeClass(1, [1, 2, 3])
True
>>> sc != SomeClass(2, [3, 2, 1])
True

>>> asdict(sc)
{'a_number': 1, 'list_of_numbers': [1, 2, 3]}

>>> SomeClass()
SomeClass(a_number=42, list_of_numbers=[])

>>> C = make_class("C", ["a", "b"])
>>> C("foo", "bar")
C(a='foo', b='bar')
```

After *declaring* your attributes, *attrs* gives you:

- a concise and explicit overview of the class's attributes,
- a nice human-readable `__repr__`,
- equality-checking methods,
- an initializer,
- and much more,

*without* writing dull boilerplate code again and again and *without* runtime performance penalties.

---

This example uses *attrs*'s modern APIs that have been introduced in version 20.1.0, and the *attrs* package import name that has been added in version 21.3.0.
The classic APIs (`@attr.s`, `attr.ib`, plus their serious-business aliases) and the `attr` package import name will remain **indefinitely**.

Check out [*On The Core API Names*](https://www.attrs.org/en/latest/names.html) for an in-depth explanation!


### Hate Type Annotations!?

No problem!
Types are entirely **optional** with *attrs*.
Simply assign `attrs.field()` to the attributes instead of annotating them with types:

```python
from attrs import define, field

@define
class SomeClass:
    a_number = field(default=42)
    list_of_numbers = field(factory=list)
```


## Data Classes

On the tin, *attrs* might remind you of `dataclasses` (and indeed, `dataclasses` [are a descendant](https://hynek.me/articles/import-attrs/) of *attrs*).
In practice it does a lot more and is more flexible.
For instance, it allows you to define [special handling of NumPy arrays for equality checks](https://www.attrs.org/en/stable/comparison.html#customization), allows more ways to [plug into the initialization process](https://www.attrs.org/en/stable/init.html#hooking-yourself-into-initialization), has a replacement for `__init_subclass__`, and allows for stepping through the generated methods using a debugger.

For more details, please refer to our [comparison page](https://www.attrs.org/en/stable/why.html#data-classes), but generally speaking, we are more likely to commit crimes against nature to make things work that one would expect to work, but that are quite complicated in practice.


## Project Information

- [**Changelog**](https://www.attrs.org/en/stable/changelog.html)
- [**Documentation**](https://www.attrs.org/)
- [**PyPI**](https://pypi.org/project/attrs/)
- [**Source Code**](https://github.com/python-attrs/attrs)
- [**Contributing**](https://github.com/python-attrs/attrs/blob/main/.github/CONTRIBUTING.md)
- [**Third-party Extensions**](https://github.com/python-attrs/attrs/wiki/Extensions-to-attrs)
- **Get Help**: use the `python-attrs` tag on [Stack Overflow](https://stackoverflow.com/questions/tagged/python-attrs)


### *attrs* for Enterprise

Available as part of the Tidelift Subscription.

The maintainers of *attrs* and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source packages you use to build your applications.
Save time, reduce risk, and improve code health, while paying the maintainers of the exact packages you use.
[Learn more](https://tidelift.com/?utm_source=lifter&utm_medium=referral&utm_campaign=hynek).

## Release Information

### Deprecations

- Given the amount of warnings raised in the broader ecosystem, we've decided to only soft-deprecate the *hash* argument to `@define` / `@attr.s`.
  Please don't use it in new code, but we don't intend to remove it anymore.
  [#1330](https://github.com/python-attrs/attrs/issues/1330)


### Changes

- `attrs.converters.pipe()` (and its syntactic sugar of passing a list for `attrs.field()`'s / `attr.ib()`'s *converter* argument) works again when passing `attrs.setters.convert` to *on_setattr* (which is default for `attrs.define`).
  [#1328](https://github.com/python-attrs/attrs/issues/1328)
- Restored support for PEP [649](https://peps.python.org/pep-0649/) / [749](https://peps.python.org/pep-0749/)-implementing Pythons -- currently 3.14-dev.
  [#1329](https://github.com/python-attrs/attrs/issues/1329)



---

[Full changelog â†’](https://www.attrs.org/en/stable/changelog.html)
"""^^xsd:string ;
    schema1:name "attrs"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "24.2.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/brotli> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.0.1>,
        <https://data.siemens-energy.com/cybersec/version/1.0.9> .

<https://data.siemens-energy.com/cybersec/package/brotlicffi> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.8.0> .

<https://data.siemens-energy.com/cybersec/package/click> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/8>,
        <https://data.siemens-energy.com/cybersec/version/8.1.3> ;
    schema1:description """\\$ click\\_
==========

Click is a Python package for creating beautiful command line interfaces
in a composable way with as little code as necessary. It's the "Command
Line Interface Creation Kit". It's highly configurable but comes with
sensible defaults out of the box.

It aims to make the process of writing command line tools quick and fun
while also preventing any frustration caused by the inability to
implement an intended CLI API.

Click in three points:

-   Arbitrary nesting of commands
-   Automatic help page generation
-   Supports lazy loading of subcommands at runtime


Installing
----------

Install and update using `pip`_:

.. code-block:: text

    $ pip install -U click

.. _pip: https://pip.pypa.io/en/stable/getting-started/


A Simple Example
----------------

.. code-block:: python

    import click

    @click.command()
    @click.option("--count", default=1, help="Number of greetings.")
    @click.option("--name", prompt="Your name", help="The person to greet.")
    def hello(count, name):
        \"\"\"Simple program that greets NAME for a total of COUNT times.\"\"\"
        for _ in range(count):
            click.echo(f"Hello, {name}!")

    if __name__ == '__main__':
        hello()

.. code-block:: text

    $ python hello.py --count=3
    Your name: Click
    Hello, Click!
    Hello, Click!
    Hello, Click!


Donate
------

The Pallets organization develops and supports Click and other popular
packages. In order to grow the community of contributors and users, and
allow the maintainers to devote more time to the projects, `please
donate today`_.

.. _please donate today: https://palletsprojects.com/donate


Links
-----

-   Documentation: https://click.palletsprojects.com/
-   Changes: https://click.palletsprojects.com/changes/
-   PyPI Releases: https://pypi.org/project/click/
-   Source Code: https://github.com/pallets/click
-   Issue Tracker: https://github.com/pallets/click/issues
-   Chat: https://discord.gg/pallets
"""^^xsd:string ;
    schema1:name "click"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "8.1.7"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/colorama> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.4.5>,
        <https://data.siemens-energy.com/cybersec/version/0.4.6> ;
    schema1:description """.. image:: https://img.shields.io/pypi/v/colorama.svg
    :target: https://pypi.org/project/colorama/
    :alt: Latest Version

.. image:: https://img.shields.io/pypi/pyversions/colorama.svg
    :target: https://pypi.org/project/colorama/
    :alt: Supported Python versions

.. image:: https://github.com/tartley/colorama/actions/workflows/test.yml/badge.svg
    :target: https://github.com/tartley/colorama/actions/workflows/test.yml
    :alt: Build Status

Colorama
========

Makes ANSI escape character sequences (for producing colored terminal text and
cursor positioning) work under MS Windows.

.. |donate| image:: https://www.paypalobjects.com/en_US/i/btn/btn_donate_SM.gif
  :target: https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=2MZ9D2GMLYCUJ&item_name=Colorama&currency_code=USD
  :alt: Donate with Paypal

`PyPI for releases <https://pypi.org/project/colorama/>`_ |
`Github for source <https://github.com/tartley/colorama>`_ |
`Colorama for enterprise on Tidelift <https://github.com/tartley/colorama/blob/master/ENTERPRISE.md>`_

If you find Colorama useful, please |donate| to the authors. Thank you!

Installation
------------

Tested on CPython 2.7, 3.7, 3.8, 3.9 and 3.10 and Pypy 2.7 and 3.8.

No requirements other than the standard library.

.. code-block:: bash

    pip install colorama
    # or
    conda install -c anaconda colorama

Description
-----------

ANSI escape character sequences have long been used to produce colored terminal
text and cursor positioning on Unix and Macs. Colorama makes this work on
Windows, too, by wrapping ``stdout``, stripping ANSI sequences it finds (which
would appear as gobbledygook in the output), and converting them into the
appropriate win32 calls to modify the state of the terminal. On other platforms,
Colorama does nothing.

This has the upshot of providing a simple cross-platform API for printing
colored terminal text from Python, and has the happy side-effect that existing
applications or libraries which use ANSI sequences to produce colored output on
Linux or Macs can now also work on Windows, simply by calling
``colorama.just_fix_windows_console()`` (since v0.4.6) or ``colorama.init()``
(all versions, but may have other side-effects â€“ see below).

An alternative approach is to install ``ansi.sys`` on Windows machines, which
provides the same behaviour for all applications running in terminals. Colorama
is intended for situations where that isn't easy (e.g., maybe your app doesn't
have an installer.)

Demo scripts in the source code repository print some colored text using
ANSI sequences. Compare their output under Gnome-terminal's built in ANSI
handling, versus on Windows Command-Prompt using Colorama:

.. image:: https://github.com/tartley/colorama/raw/master/screenshots/ubuntu-demo.png
    :width: 661
    :height: 357
    :alt: ANSI sequences on Ubuntu under gnome-terminal.

.. image:: https://github.com/tartley/colorama/raw/master/screenshots/windows-demo.png
    :width: 668
    :height: 325
    :alt: Same ANSI sequences on Windows, using Colorama.

These screenshots show that, on Windows, Colorama does not support ANSI 'dim
text'; it looks the same as 'normal text'.

Usage
-----

Initialisation
..............

If the only thing you want from Colorama is to get ANSI escapes to work on
Windows, then run:

.. code-block:: python

    from colorama import just_fix_windows_console
    just_fix_windows_console()

If you're on a recent version of Windows 10 or better, and your stdout/stderr
are pointing to a Windows console, then this will flip the magic configuration
switch to enable Windows' built-in ANSI support.

If you're on an older version of Windows, and your stdout/stderr are pointing to
a Windows console, then this will wrap ``sys.stdout`` and/or ``sys.stderr`` in a
magic file object that intercepts ANSI escape sequences and issues the
appropriate Win32 calls to emulate them.

In all other circumstances, it does nothing whatsoever. Basically the idea is
that this makes Windows act like Unix with respect to ANSI escape handling.

It's safe to call this function multiple times. It's safe to call this function
on non-Windows platforms, but it won't do anything. It's safe to call this
function when one or both of your stdout/stderr are redirected to a file â€“ it
won't do anything to those streams.

Alternatively, you can use the older interface with more features (but also more
potential footguns):

.. code-block:: python

    from colorama import init
    init()

This does the same thing as ``just_fix_windows_console``, except for the
following differences:

- It's not safe to call ``init`` multiple times; you can end up with multiple
  layers of wrapping and broken ANSI support.

- Colorama will apply a heuristic to guess whether stdout/stderr support ANSI,
  and if it thinks they don't, then it will wrap ``sys.stdout`` and
  ``sys.stderr`` in a magic file object that strips out ANSI escape sequences
  before printing them. This happens on all platforms, and can be convenient if
  you want to write your code to emit ANSI escape sequences unconditionally, and
  let Colorama decide whether they should actually be output. But note that
  Colorama's heuristic is not particularly clever.

- ``init`` also accepts explicit keyword args to enable/disable various
  functionality â€“ see below.

To stop using Colorama before your program exits, simply call ``deinit()``.
This will restore ``stdout`` and ``stderr`` to their original values, so that
Colorama is disabled. To resume using Colorama again, call ``reinit()``; it is
cheaper than calling ``init()`` again (but does the same thing).

Most users should depend on ``colorama >= 0.4.6``, and use
``just_fix_windows_console``. The old ``init`` interface will be supported
indefinitely for backwards compatibility, but we don't plan to fix any issues
with it, also for backwards compatibility.

Colored Output
..............

Cross-platform printing of colored text can then be done using Colorama's
constant shorthand for ANSI escape sequences. These are deliberately
rudimentary, see below.

.. code-block:: python

    from colorama import Fore, Back, Style
    print(Fore.RED + 'some red text')
    print(Back.GREEN + 'and with a green background')
    print(Style.DIM + 'and in dim text')
    print(Style.RESET_ALL)
    print('back to normal now')

...or simply by manually printing ANSI sequences from your own code:

.. code-block:: python

    print('\\033[31m' + 'some red text')
    print('\\033[39m') # and reset to default color

...or, Colorama can be used in conjunction with existing ANSI libraries
such as the venerable `Termcolor <https://pypi.org/project/termcolor/>`_
the fabulous `Blessings <https://pypi.org/project/blessings/>`_,
or the incredible `_Rich <https://pypi.org/project/rich/>`_.

If you wish Colorama's Fore, Back and Style constants were more capable,
then consider using one of the above highly capable libraries to generate
colors, etc, and use Colorama just for its primary purpose: to convert
those ANSI sequences to also work on Windows:

SIMILARLY, do not send PRs adding the generation of new ANSI types to Colorama.
We are only interested in converting ANSI codes to win32 API calls, not
shortcuts like the above to generate ANSI characters.

.. code-block:: python

    from colorama import just_fix_windows_console
    from termcolor import colored

    # use Colorama to make Termcolor work on Windows too
    just_fix_windows_console()

    # then use Termcolor for all colored text output
    print(colored('Hello, World!', 'green', 'on_red'))

Available formatting constants are::

    Fore: BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE, RESET.
    Back: BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE, RESET.
    Style: DIM, NORMAL, BRIGHT, RESET_ALL

``Style.RESET_ALL`` resets foreground, background, and brightness. Colorama will
perform this reset automatically on program exit.

These are fairly well supported, but not part of the standard::

    Fore: LIGHTBLACK_EX, LIGHTRED_EX, LIGHTGREEN_EX, LIGHTYELLOW_EX, LIGHTBLUE_EX, LIGHTMAGENTA_EX, LIGHTCYAN_EX, LIGHTWHITE_EX
    Back: LIGHTBLACK_EX, LIGHTRED_EX, LIGHTGREEN_EX, LIGHTYELLOW_EX, LIGHTBLUE_EX, LIGHTMAGENTA_EX, LIGHTCYAN_EX, LIGHTWHITE_EX

Cursor Positioning
..................

ANSI codes to reposition the cursor are supported. See ``demos/demo06.py`` for
an example of how to generate them.

Init Keyword Args
.................

``init()`` accepts some ``**kwargs`` to override default behaviour.

init(autoreset=False):
    If you find yourself repeatedly sending reset sequences to turn off color
    changes at the end of every print, then ``init(autoreset=True)`` will
    automate that:

    .. code-block:: python

        from colorama import init
        init(autoreset=True)
        print(Fore.RED + 'some red text')
        print('automatically back to default color again')

init(strip=None):
    Pass ``True`` or ``False`` to override whether ANSI codes should be
    stripped from the output. The default behaviour is to strip if on Windows
    or if output is redirected (not a tty).

init(convert=None):
    Pass ``True`` or ``False`` to override whether to convert ANSI codes in the
    output into win32 calls. The default behaviour is to convert if on Windows
    and output is to a tty (terminal).

init(wrap=True):
    On Windows, Colorama works by replacing ``sys.stdout`` and ``sys.stderr``
    with proxy objects, which override the ``.write()`` method to do their work.
    If this wrapping causes you problems, then this can be disabled by passing
    ``init(wrap=False)``. The default behaviour is to wrap if ``autoreset`` or
    ``strip`` or ``convert`` are True.

    When wrapping is disabled, colored printing on non-Windows platforms will
    continue to work as normal. To do cross-platform colored output, you can
    use Colorama's ``AnsiToWin32`` proxy directly:

    .. code-block:: python

        import sys
        from colorama import init, AnsiToWin32
        init(wrap=False)
        stream = AnsiToWin32(sys.stderr).stream

        # Python 2
        print >>stream, Fore.BLUE + 'blue text on stderr'

        # Python 3
        print(Fore.BLUE + 'blue text on stderr', file=stream)

Recognised ANSI Sequences
.........................

ANSI sequences generally take the form::

    ESC [ <param> ; <param> ... <command>

Where ``<param>`` is an integer, and ``<command>`` is a single letter. Zero or
more params are passed to a ``<command>``. If no params are passed, it is
generally synonymous with passing a single zero. No spaces exist in the
sequence; they have been inserted here simply to read more easily.

The only ANSI sequences that Colorama converts into win32 calls are::

    ESC [ 0 m       # reset all (colors and brightness)
    ESC [ 1 m       # bright
    ESC [ 2 m       # dim (looks same as normal brightness)
    ESC [ 22 m      # normal brightness

    # FOREGROUND:
    ESC [ 30 m      # black
    ESC [ 31 m      # red
    ESC [ 32 m      # green
    ESC [ 33 m      # yellow
    ESC [ 34 m      # blue
    ESC [ 35 m      # magenta
    ESC [ 36 m      # cyan
    ESC [ 37 m      # white
    ESC [ 39 m      # reset

    # BACKGROUND
    ESC [ 40 m      # black
    ESC [ 41 m      # red
    ESC [ 42 m      # green
    ESC [ 43 m      # yellow
    ESC [ 44 m      # blue
    ESC [ 45 m      # magenta
    ESC [ 46 m      # cyan
    ESC [ 47 m      # white
    ESC [ 49 m      # reset

    # cursor positioning
    ESC [ y;x H     # position cursor at x across, y down
    ESC [ y;x f     # position cursor at x across, y down
    ESC [ n A       # move cursor n lines up
    ESC [ n B       # move cursor n lines down
    ESC [ n C       # move cursor n characters forward
    ESC [ n D       # move cursor n characters backward

    # clear the screen
    ESC [ mode J    # clear the screen

    # clear the line
    ESC [ mode K    # clear the line

Multiple numeric params to the ``'m'`` command can be combined into a single
sequence::

    ESC [ 36 ; 45 ; 1 m     # bright cyan text on magenta background

All other ANSI sequences of the form ``ESC [ <param> ; <param> ... <command>``
are silently stripped from the output on Windows.

Any other form of ANSI sequence, such as single-character codes or alternative
initial characters, are not recognised or stripped. It would be cool to add
them though. Let me know if it would be useful for you, via the Issues on
GitHub.

Status & Known Problems
-----------------------

I've personally only tested it on Windows XP (CMD, Console2), Ubuntu
(gnome-terminal, xterm), and OS X.

Some valid ANSI sequences aren't recognised.

If you're hacking on the code, see `README-hacking.md`_. ESPECIALLY, see the
explanation there of why we do not want PRs that allow Colorama to generate new
types of ANSI codes.

See outstanding issues and wish-list:
https://github.com/tartley/colorama/issues

If anything doesn't work for you, or doesn't do what you expected or hoped for,
I'd love to hear about it on that issues list, would be delighted by patches,
and would be happy to grant commit access to anyone who submits a working patch
or two.

.. _README-hacking.md: README-hacking.md

License
-------

Copyright Jonathan Hartley & Arnon Yaari, 2013-2020. BSD 3-Clause license; see
LICENSE file.

Professional support
--------------------

.. |tideliftlogo| image:: https://cdn2.hubspot.net/hubfs/4008838/website/logos/logos_for_download/Tidelift_primary-shorthand-logo.png
   :alt: Tidelift
   :target: https://tidelift.com/subscription/pkg/pypi-colorama?utm_source=pypi-colorama&utm_medium=referral&utm_campaign=readme

.. list-table::
   :widths: 10 100

   * - |tideliftlogo|
     - Professional support for colorama is available as part of the
       `Tidelift Subscription`_.
       Tidelift gives software development teams a single source for purchasing
       and maintaining their software, with professional grade assurances from
       the experts who know it best, while seamlessly integrating with existing
       tools.

.. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-colorama?utm_source=pypi-colorama&utm_medium=referral&utm_campaign=readme

Thanks
------

See the CHANGELOG for more thanks!

* Marc Schlaich (schlamar) for a ``setup.py`` fix for Python2.5.
* Marc Abramowitz, reported & fixed a crash on exit with closed ``stdout``,
  providing a solution to issue #7's setuptools/distutils debate,
  and other fixes.
* User 'eryksun', for guidance on correctly instantiating ``ctypes.windll``.
* Matthew McCormick for politely pointing out a longstanding crash on non-Win.
* Ben Hoyt, for a magnificent fix under 64-bit Windows.
* Jesse at Empty Square for submitting a fix for examples in the README.
* User 'jamessp', an observant documentation fix for cursor positioning.
* User 'vaal1239', Dave Mckee & Lackner Kristof for a tiny but much-needed Win7
  fix.
* Julien Stuyck, for wisely suggesting Python3 compatible updates to README.
* Daniel Griffith for multiple fabulous patches.
* Oscar Lesta for a valuable fix to stop ANSI chars being sent to non-tty
  output.
* Roger Binns, for many suggestions, valuable feedback, & bug reports.
* Tim Golden for thought and much appreciated feedback on the initial idea.
* User 'Zearin' for updates to the README file.
* John Szakmeister for adding support for light colors
* Charles Merriam for adding documentation to demos
* Jurko for a fix on 64-bit Windows CPython2.5 w/o ctypes
* Florian Bruhin for a fix when stdout or stderr are None
* Thomas Weininger for fixing ValueError on Windows
* Remi Rampin for better Github integration and fixes to the README file
* Simeon Visser for closing a file handle using 'with' and updating classifiers
  to include Python 3.3 and 3.4
* Andy Neff for fixing RESET of LIGHT_EX colors.
* Jonathan Hartley for the initial idea and implementation.
"""^^xsd:string ;
    schema1:name "colorama"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.4.6"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/ipykernel> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/6.14>,
        <https://data.siemens-energy.com/cybersec/version/6.19.3> .

<https://data.siemens-energy.com/cybersec/package/jinja2> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.0>,
        <https://data.siemens-energy.com/cybersec/version/3.1.2> .

<https://data.siemens-energy.com/cybersec/package/lxml> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/Cython>,
        <https://data.siemens-energy.com/cybersec/package/cssselect> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/4.0>,
        <https://data.siemens-energy.com/cybersec/version/4.9.2> ;
    schema1:description """lxml is a Pythonic, mature binding for the libxml2 and libxslt libraries.  It
provides safe and convenient access to these libraries using the ElementTree
API.

It extends the ElementTree API significantly to offer support for XPath,
RelaxNG, XML Schema, XSLT, C14N and much more.

To contact the project, go to the `project home page
<https://lxml.de/>`_ or see our bug tracker at
https://launchpad.net/lxml

In case you want to use the current in-development version of lxml,
you can get it from the github repository at
https://github.com/lxml/lxml .  Note that this requires Cython to
build the sources, see the build instructions on the project home
page.  To the same end, running ``easy_install lxml==dev`` will
install lxml from
https://github.com/lxml/lxml/tarball/master#egg=lxml-dev if you have
an appropriate version of Cython installed.


After an official release of a new stable series, bug fixes may become
available at
https://github.com/lxml/lxml/tree/lxml-5.3 .
Running ``easy_install lxml==5.3bugfix`` will install
the unreleased branch state from
https://github.com/lxml/lxml/tarball/lxml-5.3#egg=lxml-5.3bugfix
as soon as a maintenance branch has been established.  Note that this
requires Cython to be installed at an appropriate version for the build.

5.3.0 (2024-08-10)
==================

Features added
--------------

* GH#421: Nested ``CDATA`` sections are no longer rejected but split on output
  to represent ``]]>`` correctly.
  Patch by Gertjan Klein.

Bugs fixed
----------

* LP#2060160: Attribute values serialised differently in ``xmlfile.element()`` and ``xmlfile.write()``.

* LP#2058177: The ISO-Schematron implementation could fail on unknown prefixes.
  Patch by David Lakin.

Other changes
-------------

* LP#2067707: The ``strip_cdata`` option in ``HTMLParser()`` turned out to be useless and is now deprecated.

* Binary wheels use the library versions libxml2 2.12.9 and libxslt 1.1.42.

* Windows binary wheels use the library versions libxml2 2.11.8 and libxslt 1.1.39.

* Built with Cython 3.0.11.


"""^^xsd:string ;
    schema1:name "lxml"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "5.3.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/matplotlib> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/contourpy>,
        <https://data.siemens-energy.com/cybersec/package/cycler>,
        <https://data.siemens-energy.com/cybersec/package/fonttools>,
        <https://data.siemens-energy.com/cybersec/package/kiwisolver>,
        <https://data.siemens-energy.com/cybersec/package/meson-python>,
        <https://data.siemens-energy.com/cybersec/package/numpy>,
        <https://data.siemens-energy.com/cybersec/package/packaging>,
        <https://data.siemens-energy.com/cybersec/package/pillow>,
        <https://data.siemens-energy.com/cybersec/package/pybind11>,
        <https://data.siemens-energy.com/cybersec/package/pyparsing>,
        <https://data.siemens-energy.com/cybersec/package/python-dateutil>,
        <https://data.siemens-energy.com/cybersec/package/setuptools>,
        <https://data.siemens-energy.com/cybersec/package/setuptools_scm> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.2.0>,
        <https://data.siemens-energy.com/cybersec/version/3.6.3> ;
    schema1:description """[![PyPi](https://img.shields.io/pypi/v/matplotlib)](https://pypi.org/project/matplotlib/)
[![Conda](https://img.shields.io/conda/vn/conda-forge/matplotlib)](https://anaconda.org/conda-forge/matplotlib)
[![Downloads](https://img.shields.io/pypi/dm/matplotlib)](https://pypi.org/project/matplotlib)
[![NUMFocus](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)

[![Discourse help forum](https://img.shields.io/badge/help_forum-discourse-blue.svg)](https://discourse.matplotlib.org)
[![Gitter](https://badges.gitter.im/matplotlib/matplotlib.svg)](https://gitter.im/matplotlib/matplotlib)
[![GitHub issues](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/matplotlib/matplotlib/issues)
[![Contributing](https://img.shields.io/badge/PR-Welcome-%23FF8300.svg?)](https://matplotlib.org/stable/devel/index.html)

[![GitHub actions status](https://github.com/matplotlib/matplotlib/workflows/Tests/badge.svg)](https://github.com/matplotlib/matplotlib/actions?query=workflow%3ATests)
[![Azure pipelines status](https://dev.azure.com/matplotlib/matplotlib/_apis/build/status/matplotlib.matplotlib?branchName=main)](https://dev.azure.com/matplotlib/matplotlib/_build/latest?definitionId=1&branchName=main)
[![AppVeyor status](https://ci.appveyor.com/api/projects/status/github/matplotlib/matplotlib?branch=main&svg=true)](https://ci.appveyor.com/project/matplotlib/matplotlib)
[![Codecov status](https://codecov.io/github/matplotlib/matplotlib/badge.svg?branch=main&service=github)](https://app.codecov.io/gh/matplotlib/matplotlib)
[![EffVer Versioning](https://img.shields.io/badge/version_scheme-EffVer-0097a7)](https://jacobtomlinson.dev/effver)

![Matplotlib logotype](https://matplotlib.org/_static/logo2.svg)

Matplotlib is a comprehensive library for creating static, animated, and
interactive visualizations in Python.

Check out our [home page](https://matplotlib.org/) for more information.

![image](https://matplotlib.org/_static/readme_preview.png)

Matplotlib produces publication-quality figures in a variety of hardcopy
formats and interactive environments across platforms. Matplotlib can be
used in Python scripts, Python/IPython shells, web application servers,
and various graphical user interface toolkits.

## Install

See the [install
documentation](https://matplotlib.org/stable/users/installing/index.html),
which is generated from `/doc/install/index.rst`

## Contribute

You've discovered a bug or something else you want to change â€” excellent!

You've worked out a way to fix it â€” even better!

You want to tell us about it â€” best of all!

Start at the [contributing
guide](https://matplotlib.org/devdocs/devel/contribute.html)!

## Contact

[Discourse](https://discourse.matplotlib.org/) is the discussion forum
for general questions and discussions and our recommended starting
point.

Our active mailing lists (which are mirrored on Discourse) are:

-   [Users](https://mail.python.org/mailman/listinfo/matplotlib-users)
    mailing list: <matplotlib-users@python.org>
-   [Announcement](https://mail.python.org/mailman/listinfo/matplotlib-announce)
    mailing list: <matplotlib-announce@python.org>
-   [Development](https://mail.python.org/mailman/listinfo/matplotlib-devel)
    mailing list: <matplotlib-devel@python.org>

[Gitter](https://gitter.im/matplotlib/matplotlib) is for coordinating
development and asking questions directly related to contributing to
matplotlib.

## Citing Matplotlib

If Matplotlib contributes to a project that leads to publication, please
acknowledge this by citing Matplotlib.

[A ready-made citation
entry](https://matplotlib.org/stable/users/project/citing.html) is
available.
"""^^xsd:string ;
    schema1:name "matplotlib"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.10.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/nbformat> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/fastjsonschema>,
        <https://data.siemens-energy.com/cybersec/package/jsonschema>,
        <https://data.siemens-energy.com/cybersec/package/jupyter-core>,
        <https://data.siemens-energy.com/cybersec/package/traitlets> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/5.1>,
        <https://data.siemens-energy.com/cybersec/version/5.7> ;
    schema1:description """This package contains the base implementation of the Jupyter Notebook format,
and Python APIs for working with notebooks."""^^xsd:string ;
    schema1:name "nbformat"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "5.10.4"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pip> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/19.1>,
        <https://data.siemens-energy.com/cybersec/version/22.2> ;
    schema1:description """pip - The Python Package Installer
==================================

.. |pypi-version| image:: https://img.shields.io/pypi/v/pip.svg
   :target: https://pypi.org/project/pip/
   :alt: PyPI

.. |python-versions| image:: https://img.shields.io/pypi/pyversions/pip
   :target: https://pypi.org/project/pip
   :alt: PyPI - Python Version

.. |docs-badge| image:: https://readthedocs.org/projects/pip/badge/?version=latest
   :target: https://pip.pypa.io/en/latest
   :alt: Documentation

|pypi-version| |python-versions| |docs-badge|

pip is the `package installer`_ for Python. You can use pip to install packages from the `Python Package Index`_ and other indexes.

Please take a look at our documentation for how to install and use pip:

* `Installation`_
* `Usage`_

We release updates regularly, with a new version every 3 months. Find more details in our documentation:

* `Release notes`_
* `Release process`_

If you find bugs, need help, or want to talk to the developers, please use our mailing lists or chat rooms:

* `Issue tracking`_
* `Discourse channel`_
* `User IRC`_

If you want to get involved head over to GitHub to get the source code, look at our development documentation and feel free to jump on the developer mailing lists and chat rooms:

* `GitHub page`_
* `Development documentation`_
* `Development IRC`_

Code of Conduct
---------------

Everyone interacting in the pip project's codebases, issue trackers, chat
rooms, and mailing lists is expected to follow the `PSF Code of Conduct`_.

.. _package installer: https://packaging.python.org/guides/tool-recommendations/
.. _Python Package Index: https://pypi.org
.. _Installation: https://pip.pypa.io/en/stable/installation/
.. _Usage: https://pip.pypa.io/en/stable/
.. _Release notes: https://pip.pypa.io/en/stable/news.html
.. _Release process: https://pip.pypa.io/en/latest/development/release-process/
.. _GitHub page: https://github.com/pypa/pip
.. _Development documentation: https://pip.pypa.io/en/latest/development
.. _Issue tracking: https://github.com/pypa/pip/issues
.. _Discourse channel: https://discuss.python.org/c/packaging
.. _User IRC: https://kiwiirc.com/nextclient/#ircs://irc.libera.chat:+6697/pypa
.. _Development IRC: https://kiwiirc.com/nextclient/#ircs://irc.libera.chat:+6697/pypa-dev
.. _PSF Code of Conduct: https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md
"""^^xsd:string ;
    schema1:name "pip"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "25.0.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/proselint> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.13> .

<https://data.siemens-energy.com/cybersec/package/pygments> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.4.0>,
        <https://data.siemens-energy.com/cybersec/version/2.4.1> .

<https://data.siemens-energy.com/cybersec/package/pytest-timeout> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.1>,
        <https://data.siemens-energy.com/cybersec/version/2.2> .

<https://data.siemens-energy.com/cybersec/package/pytz> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2020.1>,
        <https://data.siemens-energy.com/cybersec/version/2021.1> ;
    schema1:description """pytz - World Timezone Definitions for Python
============================================

:Author: Stuart Bishop <stuart@stuartbishop.net>

Introduction
~~~~~~~~~~~~

pytz brings the Olson tz database into Python. This library allows
accurate and cross platform timezone calculations using Python 2.4
or higher. It also solves the issue of ambiguous times at the end
of daylight saving time, which you can read more about in the Python
Library Reference (``datetime.tzinfo``).

Almost all of the Olson timezones are supported.

.. note::

    Projects using Python 3.9 or later should be using the support
    now included as part of the standard library, and third party
    packages work with it such as `tzdata <https://pypi.org/project/tzdata/>`_.
    pytz offers no advantages beyond backwards compatibility with
    code written for earlier versions of Python.

.. note::

    This library differs from the documented Python API for
    tzinfo implementations; if you want to create local wallclock
    times you need to use the ``localize()`` method documented in this
    document. In addition, if you perform date arithmetic on local
    times that cross DST boundaries, the result may be in an incorrect
    timezone (ie. subtract 1 minute from 2002-10-27 1:00 EST and you get
    2002-10-27 0:59 EST instead of the correct 2002-10-27 1:59 EDT). A
    ``normalize()`` method is provided to correct this. Unfortunately these
    issues cannot be resolved without modifying the Python datetime
    implementation (see PEP-431).


Installation
~~~~~~~~~~~~

This package can either be installed using ``pip`` or from a tarball using the
standard Python distutils.

If you are installing using ``pip``, you don't need to download anything as the
latest version will be downloaded for you from PyPI::

    pip install pytz

If you are installing from a tarball, run the following command as an
administrative user::

    python setup.py install


pytz for Enterprise
~~~~~~~~~~~~~~~~~~~

Available as part of the Tidelift Subscription.

The maintainers of pytz and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use. `Learn more. <https://tidelift.com/subscription/pkg/pypi-pytz?utm_source=pypi-pytz&utm_medium=referral&utm_campaign=enterprise&utm_term=repo>`_.


Example & Usage
~~~~~~~~~~~~~~~

Localized times and date arithmetic
-----------------------------------

>>> from datetime import datetime, timedelta
>>> from pytz import timezone
>>> import pytz
>>> utc = pytz.utc
>>> utc.zone
'UTC'
>>> eastern = timezone('US/Eastern')
>>> eastern.zone
'US/Eastern'
>>> amsterdam = timezone('Europe/Amsterdam')
>>> fmt = '%Y-%m-%d %H:%M:%S %Z%z'

This library only supports two ways of building a localized time. The
first is to use the ``localize()`` method provided by the pytz library.
This is used to localize a naive datetime (datetime with no timezone
information):

>>> loc_dt = eastern.localize(datetime(2002, 10, 27, 6, 0, 0))
>>> print(loc_dt.strftime(fmt))
2002-10-27 06:00:00 EST-0500

The second way of building a localized time is by converting an existing
localized time using the standard ``astimezone()`` method:

>>> ams_dt = loc_dt.astimezone(amsterdam)
>>> ams_dt.strftime(fmt)
'2002-10-27 12:00:00 CET+0100'

Unfortunately using the tzinfo argument of the standard datetime
constructors ''does not work'' with pytz for many timezones.

>>> datetime(2002, 10, 27, 12, 0, 0, tzinfo=amsterdam).strftime(fmt)  # /!\\ Does not work this way!
'2002-10-27 12:00:00 LMT+0018'

It is safe for timezones without daylight saving transitions though, such
as UTC:

>>> datetime(2002, 10, 27, 12, 0, 0, tzinfo=pytz.utc).strftime(fmt)  # /!\\ Not recommended except for UTC
'2002-10-27 12:00:00 UTC+0000'

The preferred way of dealing with times is to always work in UTC,
converting to localtime only when generating output to be read
by humans.

>>> utc_dt = datetime(2002, 10, 27, 6, 0, 0, tzinfo=utc)
>>> loc_dt = utc_dt.astimezone(eastern)
>>> loc_dt.strftime(fmt)
'2002-10-27 01:00:00 EST-0500'

This library also allows you to do date arithmetic using local
times, although it is more complicated than working in UTC as you
need to use the ``normalize()`` method to handle daylight saving time
and other timezone transitions. In this example, ``loc_dt`` is set
to the instant when daylight saving time ends in the US/Eastern
timezone.

>>> before = loc_dt - timedelta(minutes=10)
>>> before.strftime(fmt)
'2002-10-27 00:50:00 EST-0500'
>>> eastern.normalize(before).strftime(fmt)
'2002-10-27 01:50:00 EDT-0400'
>>> after = eastern.normalize(before + timedelta(minutes=20))
>>> after.strftime(fmt)
'2002-10-27 01:10:00 EST-0500'

Creating local times is also tricky, and the reason why working with
local times is not recommended. Unfortunately, you cannot just pass
a ``tzinfo`` argument when constructing a datetime (see the next
section for more details)

>>> dt = datetime(2002, 10, 27, 1, 30, 0)
>>> dt1 = eastern.localize(dt, is_dst=True)
>>> dt1.strftime(fmt)
'2002-10-27 01:30:00 EDT-0400'
>>> dt2 = eastern.localize(dt, is_dst=False)
>>> dt2.strftime(fmt)
'2002-10-27 01:30:00 EST-0500'

Converting between timezones is more easily done, using the
standard astimezone method.

>>> utc_dt = datetime.fromtimestamp(1143408899, tz=utc)
>>> utc_dt.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'
>>> au_tz = timezone('Australia/Sydney')
>>> au_dt = utc_dt.astimezone(au_tz)
>>> au_dt.strftime(fmt)
'2006-03-27 08:34:59 AEDT+1100'
>>> utc_dt2 = au_dt.astimezone(utc)
>>> utc_dt2.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'
>>> utc_dt == utc_dt2
True

You can take shortcuts when dealing with the UTC side of timezone
conversions. ``normalize()`` and ``localize()`` are not really
necessary when there are no daylight saving time transitions to
deal with.

>>> utc_dt = datetime.fromtimestamp(1143408899, tz=utc)
>>> utc_dt.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'
>>> au_tz = timezone('Australia/Sydney')
>>> au_dt = au_tz.normalize(utc_dt.astimezone(au_tz))
>>> au_dt.strftime(fmt)
'2006-03-27 08:34:59 AEDT+1100'
>>> utc_dt2 = au_dt.astimezone(utc)
>>> utc_dt2.strftime(fmt)
'2006-03-26 21:34:59 UTC+0000'


``tzinfo`` API
--------------

The ``tzinfo`` instances returned by the ``timezone()`` function have
been extended to cope with ambiguous times by adding an ``is_dst``
parameter to the ``utcoffset()``, ``dst()`` && ``tzname()`` methods.

>>> tz = timezone('America/St_Johns')

>>> normal = datetime(2009, 9, 1)
>>> ambiguous = datetime(2009, 10, 31, 23, 30)

The ``is_dst`` parameter is ignored for most timestamps. It is only used
during DST transition ambiguous periods to resolve that ambiguity.

>>> print(tz.utcoffset(normal, is_dst=True))
-1 day, 21:30:00
>>> print(tz.dst(normal, is_dst=True))
1:00:00
>>> tz.tzname(normal, is_dst=True)
'NDT'

>>> print(tz.utcoffset(ambiguous, is_dst=True))
-1 day, 21:30:00
>>> print(tz.dst(ambiguous, is_dst=True))
1:00:00
>>> tz.tzname(ambiguous, is_dst=True)
'NDT'

>>> print(tz.utcoffset(normal, is_dst=False))
-1 day, 21:30:00
>>> tz.dst(normal, is_dst=False).seconds
3600
>>> tz.tzname(normal, is_dst=False)
'NDT'

>>> print(tz.utcoffset(ambiguous, is_dst=False))
-1 day, 20:30:00
>>> tz.dst(ambiguous, is_dst=False)
datetime.timedelta(0)
>>> tz.tzname(ambiguous, is_dst=False)
'NST'

If ``is_dst`` is not specified, ambiguous timestamps will raise
an ``pytz.exceptions.AmbiguousTimeError`` exception.

>>> print(tz.utcoffset(normal))
-1 day, 21:30:00
>>> print(tz.dst(normal))
1:00:00
>>> tz.tzname(normal)
'NDT'

>>> import pytz.exceptions
>>> try:
...     tz.utcoffset(ambiguous)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % ambiguous)
pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00
>>> try:
...     tz.dst(ambiguous)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % ambiguous)
pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00
>>> try:
...     tz.tzname(ambiguous)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % ambiguous)
pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00


Problems with Localtime
~~~~~~~~~~~~~~~~~~~~~~~

The major problem we have to deal with is that certain datetimes
may occur twice in a year. For example, in the US/Eastern timezone
on the last Sunday morning in October, the following sequence
happens:

    - 01:00 EDT occurs
    - 1 hour later, instead of 2:00am the clock is turned back 1 hour
      and 01:00 happens again (this time 01:00 EST)

In fact, every instant between 01:00 and 02:00 occurs twice. This means
that if you try and create a time in the 'US/Eastern' timezone
the standard datetime syntax, there is no way to specify if you meant
before of after the end-of-daylight-saving-time transition. Using the
pytz custom syntax, the best you can do is make an educated guess:

>>> loc_dt = eastern.localize(datetime(2002, 10, 27, 1, 30, 00))
>>> loc_dt.strftime(fmt)
'2002-10-27 01:30:00 EST-0500'

As you can see, the system has chosen one for you and there is a 50%
chance of it being out by one hour. For some applications, this does
not matter. However, if you are trying to schedule meetings with people
in different timezones or analyze log files it is not acceptable.

The best and simplest solution is to stick with using UTC.  The pytz
package encourages using UTC for internal timezone representation by
including a special UTC implementation based on the standard Python
reference implementation in the Python documentation.

The UTC timezone unpickles to be the same instance, and pickles to a
smaller size than other pytz tzinfo instances.  The UTC implementation
can be obtained as pytz.utc, pytz.UTC, or pytz.timezone('UTC').

>>> import pickle, pytz
>>> dt = datetime(2005, 3, 1, 14, 13, 21, tzinfo=utc)
>>> naive = dt.replace(tzinfo=None)
>>> p = pickle.dumps(dt, 1)
>>> naive_p = pickle.dumps(naive, 1)
>>> len(p) - len(naive_p)
17
>>> new = pickle.loads(p)
>>> new == dt
True
>>> new is dt
False
>>> new.tzinfo is dt.tzinfo
True
>>> pytz.utc is pytz.UTC is pytz.timezone('UTC')
True

Note that some other timezones are commonly thought of as the same (GMT,
Greenwich, Universal, etc.). The definition of UTC is distinct from these
other timezones, and they are not equivalent. For this reason, they will
not compare the same in Python.

>>> utc == pytz.timezone('GMT')
False

See the section `What is UTC`_, below.

If you insist on working with local times, this library provides a
facility for constructing them unambiguously:

>>> loc_dt = datetime(2002, 10, 27, 1, 30, 00)
>>> est_dt = eastern.localize(loc_dt, is_dst=True)
>>> edt_dt = eastern.localize(loc_dt, is_dst=False)
>>> print(est_dt.strftime(fmt) + ' / ' + edt_dt.strftime(fmt))
2002-10-27 01:30:00 EDT-0400 / 2002-10-27 01:30:00 EST-0500

If you pass None as the is_dst flag to localize(), pytz will refuse to
guess and raise exceptions if you try to build ambiguous or non-existent
times.

For example, 1:30am on 27th Oct 2002 happened twice in the US/Eastern
timezone when the clocks where put back at the end of Daylight Saving
Time:

>>> dt = datetime(2002, 10, 27, 1, 30, 00)
>>> try:
...     eastern.localize(dt, is_dst=None)
... except pytz.exceptions.AmbiguousTimeError:
...     print('pytz.exceptions.AmbiguousTimeError: %s' % dt)
pytz.exceptions.AmbiguousTimeError: 2002-10-27 01:30:00

Similarly, 2:30am on 7th April 2002 never happened at all in the
US/Eastern timezone, as the clocks where put forward at 2:00am skipping
the entire hour:

>>> dt = datetime(2002, 4, 7, 2, 30, 00)
>>> try:
...     eastern.localize(dt, is_dst=None)
... except pytz.exceptions.NonExistentTimeError:
...     print('pytz.exceptions.NonExistentTimeError: %s' % dt)
pytz.exceptions.NonExistentTimeError: 2002-04-07 02:30:00

Both of these exceptions share a common base class to make error handling
easier:

>>> isinstance(pytz.AmbiguousTimeError(), pytz.InvalidTimeError)
True
>>> isinstance(pytz.NonExistentTimeError(), pytz.InvalidTimeError)
True


A special case is where countries change their timezone definitions
with no daylight savings time switch. For example, in 1915 Warsaw
switched from Warsaw time to Central European time with no daylight savings
transition. So at the stroke of midnight on August 5th 1915 the clocks
were wound back 24 minutes creating an ambiguous time period that cannot
be specified without referring to the timezone abbreviation or the
actual UTC offset. In this case midnight happened twice, neither time
during a daylight saving time period. pytz handles this transition by
treating the ambiguous period before the switch as daylight savings
time, and the ambiguous period after as standard time.


>>> warsaw = pytz.timezone('Europe/Warsaw')
>>> amb_dt1 = warsaw.localize(datetime(1915, 8, 4, 23, 59, 59), is_dst=True)
>>> amb_dt1.strftime(fmt)
'1915-08-04 23:59:59 WMT+0124'
>>> amb_dt2 = warsaw.localize(datetime(1915, 8, 4, 23, 59, 59), is_dst=False)
>>> amb_dt2.strftime(fmt)
'1915-08-04 23:59:59 CET+0100'
>>> switch_dt = warsaw.localize(datetime(1915, 8, 5, 00, 00, 00), is_dst=False)
>>> switch_dt.strftime(fmt)
'1915-08-05 00:00:00 CET+0100'
>>> str(switch_dt - amb_dt1)
'0:24:01'
>>> str(switch_dt - amb_dt2)
'0:00:01'

The best way of creating a time during an ambiguous time period is
by converting from another timezone such as UTC:

>>> utc_dt = datetime(1915, 8, 4, 22, 36, tzinfo=pytz.utc)
>>> utc_dt.astimezone(warsaw).strftime(fmt)
'1915-08-04 23:36:00 CET+0100'

The standard Python way of handling all these ambiguities is not to
handle them, such as demonstrated in this example using the US/Eastern
timezone definition from the Python documentation (Note that this
implementation only works for dates between 1987 and 2006 - it is
included for tests only!):

>>> from pytz.reference import Eastern # pytz.reference only for tests
>>> dt = datetime(2002, 10, 27, 0, 30, tzinfo=Eastern)
>>> str(dt)
'2002-10-27 00:30:00-04:00'
>>> str(dt + timedelta(hours=1))
'2002-10-27 01:30:00-05:00'
>>> str(dt + timedelta(hours=2))
'2002-10-27 02:30:00-05:00'
>>> str(dt + timedelta(hours=3))
'2002-10-27 03:30:00-05:00'

Notice the first two results? At first glance you might think they are
correct, but taking the UTC offset into account you find that they are
actually two hours appart instead of the 1 hour we asked for.

>>> from pytz.reference import UTC # pytz.reference only for tests
>>> str(dt.astimezone(UTC))
'2002-10-27 04:30:00+00:00'
>>> str((dt + timedelta(hours=1)).astimezone(UTC))
'2002-10-27 06:30:00+00:00'


Country Information
~~~~~~~~~~~~~~~~~~~

A mechanism is provided to access the timezones commonly in use
for a particular country, looked up using the ISO 3166 country code.
It returns a list of strings that can be used to retrieve the relevant
tzinfo instance using ``pytz.timezone()``:

>>> print(' '.join(pytz.country_timezones['nz']))
Pacific/Auckland Pacific/Chatham

The Olson database comes with a ISO 3166 country code to English country
name mapping that pytz exposes as a dictionary:

>>> print(pytz.country_names['nz'])
New Zealand


What is UTC
~~~~~~~~~~~

'UTC' is `Coordinated Universal Time`_. It is a successor to, but distinct
from, Greenwich Mean Time (GMT) and the various definitions of Universal
Time. UTC is now the worldwide standard for regulating clocks and time
measurement.

All other timezones are defined relative to UTC, and include offsets like
UTC+0800 - hours to add or subtract from UTC to derive the local time. No
daylight saving time occurs in UTC, making it a useful timezone to perform
date arithmetic without worrying about the confusion and ambiguities caused
by daylight saving time transitions, your country changing its timezone, or
mobile computers that roam through multiple timezones.

..  _Coordinated Universal Time: https://en.wikipedia.org/wiki/Coordinated_Universal_Time


Helpers
~~~~~~~

There are two lists of timezones provided.

``all_timezones`` is the exhaustive list of the timezone names that can
be used.

>>> from pytz import all_timezones
>>> len(all_timezones) >= 500
True
>>> 'Etc/Greenwich' in all_timezones
True

``common_timezones`` is a list of useful, current timezones. It doesn't
contain deprecated zones or historical zones, except for a few I've
deemed in common usage, such as US/Eastern (open a bug report if you
think other timezones are deserving of being included here). It is also
a sequence of strings.

>>> from pytz import common_timezones
>>> len(common_timezones) < len(all_timezones)
True
>>> 'Etc/Greenwich' in common_timezones
False
>>> 'Australia/Melbourne' in common_timezones
True
>>> 'US/Eastern' in common_timezones
True
>>> 'Canada/Eastern' in common_timezones
True
>>> 'Australia/Yancowinna' in all_timezones
True
>>> 'Australia/Yancowinna' in common_timezones
False

Both ``common_timezones`` and ``all_timezones`` are alphabetically
sorted:

>>> common_timezones_dupe = common_timezones[:]
>>> common_timezones_dupe.sort()
>>> common_timezones == common_timezones_dupe
True
>>> all_timezones_dupe = all_timezones[:]
>>> all_timezones_dupe.sort()
>>> all_timezones == all_timezones_dupe
True

``all_timezones`` and ``common_timezones`` are also available as sets.

>>> from pytz import all_timezones_set, common_timezones_set
>>> 'US/Eastern' in all_timezones_set
True
>>> 'US/Eastern' in common_timezones_set
True
>>> 'Australia/Victoria' in common_timezones_set
False

You can also retrieve lists of timezones used by particular countries
using the ``country_timezones()`` function. It requires an ISO-3166
two letter country code.

>>> from pytz import country_timezones
>>> print(' '.join(country_timezones('ch')))
Europe/Zurich
>>> print(' '.join(country_timezones('CH')))
Europe/Zurich


Internationalization - i18n/l10n
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pytz is an interface to the IANA database, which uses ASCII names. The `Unicode  Consortium's Unicode Locales (CLDR) <http://cldr.unicode.org>`_
project provides translations. Python packages such as
`Babel <https://babel.pocoo.org/en/latest/api/dates.html#timezone-functionality>`_
and Thomas Khyn's `l18n <https://pypi.org/project/l18n/>`_ package can be used
to access these translations from Python.


License
~~~~~~~

MIT license.

This code is also available as part of Zope 3 under the Zope Public
License,  Version 2.1 (ZPL).

I'm happy to relicense this code if necessary for inclusion in other
open source projects.


Latest Versions
~~~~~~~~~~~~~~~

This package will be updated after releases of the Olson timezone
database.  The latest version can be downloaded from the `Python Package
Index <https://pypi.org/project/pytz/>`_.  The code that is used
to generate this distribution is hosted on Github and available
using git::

    git clone https://github.com/stub42/pytz.git

Announcements of new releases are made on
`Launchpad <https://launchpad.net/pytz>`_, and the
`Atom feed <http://feeds.launchpad.net/pytz/announcements.atom>`_
hosted there.


Bugs, Feature Requests & Patches
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Bugs should be reported on `Github <https://github.com/stub42/pytz/issues>`_.
Feature requests are unlikely to be considered, and efforts instead directed
to timezone support now built into Python or packages that work with it.


Security Issues
~~~~~~~~~~~~~~~

Reports about security issues can be made via `Tidelift <https://tidelift.com/security>`_.


Issues & Limitations
~~~~~~~~~~~~~~~~~~~~

- This project is in maintenance mode. Projects using Python 3.9 or later
  are best served by using the timezone functionaly now included in core
  Python and packages that work with it such as `tzdata <https://pypi.org/project/tzdata/>`_.

- Offsets from UTC are rounded to the nearest whole minute, so timezones
  such as Europe/Amsterdam pre 1937 will be up to 30 seconds out. This
  was a limitation of the Python datetime library.

- If you think a timezone definition is incorrect, I probably can't fix
  it. pytz is a direct translation of the Olson timezone database, and
  changes to the timezone definitions need to be made to this source.
  If you find errors they should be reported to the time zone mailing
  list, linked from http://www.iana.org/time-zones.


Further Reading
~~~~~~~~~~~~~~~

More info than you want to know about timezones:
https://data.iana.org/time-zones/tz-link.html


Contact
~~~~~~~

Stuart Bishop <stuart@stuartbishop.net>


"""^^xsd:string ;
    schema1:name "pytz"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2024.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/referencing> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/attrs>,
        <https://data.siemens-energy.com/cybersec/package/rpds-py> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.28.4>,
        <https://data.siemens-energy.com/cybersec/version/0.31.0> ;
    schema1:description """===============
``referencing``
===============

|PyPI| |Pythons| |CI| |ReadTheDocs| |pre-commit|

.. |PyPI| image:: https://img.shields.io/pypi/v/referencing.svg
  :alt: PyPI version
  :target: https://pypi.org/project/referencing/

.. |Pythons| image:: https://img.shields.io/pypi/pyversions/referencing.svg
  :alt: Supported Python versions
  :target: https://pypi.org/project/referencing/

.. |CI| image:: https://github.com/python-jsonschema/referencing/workflows/CI/badge.svg
  :alt: Build status
  :target: https://github.com/python-jsonschema/referencing/actions?query=workflow%3ACI

.. |ReadTheDocs| image:: https://readthedocs.org/projects/referencing/badge/?version=stable&style=flat
   :alt: ReadTheDocs status
   :target: https://referencing.readthedocs.io/en/stable/

.. |pre-commit| image:: https://results.pre-commit.ci/badge/github/python-jsonschema/referencing/main.svg
  :alt: pre-commit.ci status
  :target: https://results.pre-commit.ci/latest/github/python-jsonschema/referencing/main


An implementation-agnostic implementation of JSON reference resolution.

See `the documentation <https://referencing.readthedocs.io/>`_ for more details.
"""^^xsd:string ;
    schema1:name "referencing"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.35.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/rpds-py> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.7.0>,
        <https://data.siemens-energy.com/cybersec/version/0.7.1> ;
    schema1:description """===========
``rpds.py``
===========

|PyPI| |Pythons| |CI|

.. |PyPI| image:: https://img.shields.io/pypi/v/rpds-py.svg
  :alt: PyPI version
  :target: https://pypi.org/project/rpds-py/

.. |Pythons| image:: https://img.shields.io/pypi/pyversions/rpds-py.svg
  :alt: Supported Python versions
  :target: https://pypi.org/project/rpds-py/

.. |CI| image:: https://github.com/crate-py/rpds/workflows/CI/badge.svg
  :alt: Build status
  :target: https://github.com/crate-py/rpds/actions?query=workflow%3ACI

.. |ReadTheDocs| image:: https://readthedocs.org/projects/referencing/badge/?version=stable&style=flat
   :alt: ReadTheDocs status
   :target: https://referencing.readthedocs.io/en/stable/


Python bindings to the `Rust rpds crate <https://docs.rs/rpds/>`_ for persistent data structures.

What's here is quite minimal (in transparency, it was written initially to support replacing ``pyrsistent`` in the `referencing library <https://github.com/python-jsonschema/referencing>`_).
If you see something missing (which is very likely), a PR is definitely welcome to add it.

Installation
------------

The distribution on PyPI is named ``rpds.py`` (equivalently ``rpds-py``), and thus can be installed via e.g.:

.. code:: sh

    $ pip install rpds-py

Note that if you install ``rpds-py`` from source, you will need a Rust toolchain installed, as it is a build-time dependency.
An example of how to do so in a ``Dockerfile`` can be found `here <https://github.com/bowtie-json-schema/bowtie/blob/e77fd93598cb6e7dc1b8b1f53c00e5aa410c201a/implementations/python-jsonschema/Dockerfile#L1-L8>`_.

If you believe you are on a common platform which should have wheels built (i.e. and not need to compile from source), feel free to file an issue or pull request modifying the GitHub action used here to build wheels via ``maturin``.

Usage
-----

Methods in general are named similarly to their ``rpds`` counterparts (rather than ``pyrsistent``\\ 's conventions, though probably a full drop-in ``pyrsistent``\\ -compatible wrapper module is a good addition at some point).

.. code:: python

    >>> from rpds import HashTrieMap, HashTrieSet, List

    >>> m = HashTrieMap({"foo": "bar", "baz": "quux"})
    >>> m.insert("spam", 37) == HashTrieMap({"foo": "bar", "baz": "quux", "spam": 37})
    True
    >>> m.remove("foo") == HashTrieMap({"baz": "quux"})
    True

    >>> s = HashTrieSet({"foo", "bar", "baz", "quux"})
    >>> s.insert("spam") == HashTrieSet({"foo", "bar", "baz", "quux", "spam"})
    True
    >>> s.remove("foo") == HashTrieSet({"bar", "baz", "quux"})
    True

    >>> L = List([1, 3, 5])
    >>> L.push_front(-1) == List([-1, 1, 3, 5])
    True
    >>> L.rest == List([3, 5])
    True

"""^^xsd:string ;
    schema1:name "rpds-py"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.20.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/tornado> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/6.1>,
        <https://data.siemens-energy.com/cybersec/version/6.2> ;
    schema1:description """Tornado Web Server
==================

.. image:: https://badges.gitter.im/Join%20Chat.svg
   :alt: Join the chat at https://gitter.im/tornadoweb/tornado
   :target: https://gitter.im/tornadoweb/tornado?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

`Tornado <http://www.tornadoweb.org>`_ is a Python web framework and
asynchronous networking library, originally developed at `FriendFeed
<http://friendfeed.com>`_.  By using non-blocking network I/O, Tornado
can scale to tens of thousands of open connections, making it ideal for
`long polling <http://en.wikipedia.org/wiki/Push_technology#Long_Polling>`_,
`WebSockets <http://en.wikipedia.org/wiki/WebSocket>`_, and other
applications that require a long-lived connection to each user.

Hello, world
------------

Here is a simple "Hello, world" example web app for Tornado:

.. code-block:: python

    import asyncio
    import tornado

    class MainHandler(tornado.web.RequestHandler):
        def get(self):
            self.write("Hello, world")

    def make_app():
        return tornado.web.Application([
            (r"/", MainHandler),
        ])

    async def main():
        app = make_app()
        app.listen(8888)
        await asyncio.Event().wait()

    if __name__ == "__main__":
        asyncio.run(main())

This example does not use any of Tornado's asynchronous features; for
that see this `simple chat room
<https://github.com/tornadoweb/tornado/tree/stable/demos/chat>`_.

Documentation
-------------

Documentation and links to additional resources are available at
https://www.tornadoweb.org
"""^^xsd:string ;
    schema1:name "tornado"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "6.4.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/wheel> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.36.0>,
        <https://data.siemens-energy.com/cybersec/version/0.43.0>,
        <https://data.siemens-energy.com/cybersec/version/0.44.0> ;
    schema1:description """wheel
=====

This library is the reference implementation of the Python wheel packaging
standard, as defined in `PEP 427`_.

It has two different roles:

#. A setuptools_ extension for building wheels that provides the
   ``bdist_wheel`` setuptools command
#. A command line tool for working with wheel files

It should be noted that wheel is **not** intended to be used as a library, and
as such there is no stable, public API.

.. _PEP 427: https://www.python.org/dev/peps/pep-0427/
.. _setuptools: https://pypi.org/project/setuptools/

Documentation
-------------

The documentation_ can be found on Read The Docs.

.. _documentation: https://wheel.readthedocs.io/

Code of Conduct
---------------

Everyone interacting in the wheel project's codebases, issue trackers, chat
rooms, and mailing lists is expected to follow the `PSF Code of Conduct`_.

.. _PSF Code of Conduct: https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md
"""^^xsd:string ;
    schema1:name "wheel"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "0.38.4"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/zstandard> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/0.18.0>,
        <https://data.siemens-energy.com/cybersec/version/0.19.0> .

<https://data.siemens-energy.com/cybersec/version/0.1.7> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "7"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.19.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "19"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.2.1> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.4> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/0.5.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "5"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.0.1> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.1> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.2.0> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.3> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.3.0> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.4.1> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.5> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "5"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/1.5.7> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "7"^^xsd:int ;
    software_vocab:version_minor "5"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/19.1> software_vocab:version_major "19"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.0> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.0.1> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "1"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.2.0> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.3> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.7> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "7"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2022.12.0> software_vocab:version_major "2022"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "12"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.1.2> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.12> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "12"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/3.4.0> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "4"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/4.0> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/5.0.2> software_vocab:version_major "5"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/6> software_vocab:version_major "6"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7.3.2> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "2"^^xsd:int ;
    software_vocab:version_minor "3"^^xsd:int .

<https://data.siemens-energy.com/cybersec/package/covdefaults> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.3> .

<https://data.siemens-energy.com/cybersec/package/coverage> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/7.2.7>,
        <https://data.siemens-energy.com/cybersec/version/7.3.2>,
        <https://data.siemens-energy.com/cybersec/version/7.4.2> .

<https://data.siemens-energy.com/cybersec/package/filelock> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/covdefaults>,
        <https://data.siemens-energy.com/cybersec/package/coverage>,
        <https://data.siemens-energy.com/cybersec/package/diff-cover>,
        <https://data.siemens-energy.com/cybersec/package/furo>,
        <https://data.siemens-energy.com/cybersec/package/pytest>,
        <https://data.siemens-energy.com/cybersec/package/pytest-asyncio>,
        <https://data.siemens-energy.com/cybersec/package/pytest-cov>,
        <https://data.siemens-energy.com/cybersec/package/pytest-mock>,
        <https://data.siemens-energy.com/cybersec/package/pytest-timeout>,
        <https://data.siemens-energy.com/cybersec/package/sphinx>,
        <https://data.siemens-energy.com/cybersec/package/sphinx-autodoc-typehints>,
        <https://data.siemens-energy.com/cybersec/package/typing-extensions>,
        <https://data.siemens-energy.com/cybersec/package/virtualenv> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3>,
        <https://data.siemens-energy.com/cybersec/version/3.4.0>,
        <https://data.siemens-energy.com/cybersec/version/4> ;
    schema1:description """# filelock

[![PyPI](https://img.shields.io/pypi/v/filelock)](https://pypi.org/project/filelock/)
[![Supported Python
versions](https://img.shields.io/pypi/pyversions/filelock.svg)](https://pypi.org/project/filelock/)
[![Documentation
status](https://readthedocs.org/projects/py-filelock/badge/?version=latest)](https://py-filelock.readthedocs.io/en/latest/?badge=latest)
[![Code style:
black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Downloads](https://static.pepy.tech/badge/filelock/month)](https://pepy.tech/project/filelock)
[![check](https://github.com/tox-dev/py-filelock/actions/workflows/check.yml/badge.svg)](https://github.com/tox-dev/py-filelock/actions/workflows/check.yml)

For more information checkout the [official documentation](https://py-filelock.readthedocs.io/en/latest/index.html).
"""^^xsd:string ;
    schema1:name "filelock"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "3.15.4"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/importlib-resources> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.4.0>,
        <https://data.siemens-energy.com/cybersec/version/5.10.2> ;
    schema1:description """.. image:: https://img.shields.io/pypi/v/importlib_resources.svg
   :target: https://pypi.org/project/importlib_resources

.. image:: https://img.shields.io/pypi/pyversions/importlib_resources.svg

.. image:: https://github.com/python/importlib_resources/workflows/tests/badge.svg
   :target: https://github.com/python/importlib_resources/actions?query=workflow%3A%22tests%22
   :alt: tests

.. image:: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json
    :target: https://github.com/astral-sh/ruff
    :alt: Ruff

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :target: https://github.com/psf/black
   :alt: Code style: Black

.. image:: https://readthedocs.org/projects/importlib-resources/badge/?version=latest
   :target: https://importlib-resources.readthedocs.io/en/latest/?badge=latest

.. image:: https://img.shields.io/badge/skeleton-2023-informational
   :target: https://blog.jaraco.com/skeleton

.. image:: https://tidelift.com/badges/package/pypi/importlib-resources
   :target: https://tidelift.com/subscription/pkg/pypi-importlib-resources?utm_source=pypi-importlib-resources&utm_medium=readme

``importlib_resources`` is a backport of Python standard library
`importlib.resources
<https://docs.python.org/3/library/importlib.html#module-importlib.resources>`_
module for older Pythons.

The key goal of this module is to replace parts of `pkg_resources
<https://setuptools.readthedocs.io/en/latest/pkg_resources.html>`_ with a
solution in Python's stdlib that relies on well-defined APIs.  This makes
reading resources included in packages easier, with more stable and consistent
semantics.

Compatibility
=============

New features are introduced in this third-party library and later merged
into CPython. The following table indicates which versions of this library
were contributed to different versions in the standard library:

.. list-table::
   :header-rows: 1

   * - importlib_resources
     - stdlib
   * - 5.9
     - 3.12
   * - 5.7
     - 3.11
   * - 5.0
     - 3.10
   * - 1.3
     - 3.9
   * - 0.5 (?)
     - 3.7

For Enterprise
==============

Available as part of the Tidelift Subscription.

This project and the maintainers of thousands of other packages are working with Tidelift to deliver one enterprise subscription that covers all of the open source you use.

`Learn more <https://tidelift.com/subscription/pkg/pypi-importlib-resources?utm_source=pypi-importlib-resources&utm_medium=referral&utm_campaign=github>`_.

Security Contact
================

To report a security vulnerability, please use the
`Tidelift security contact <https://tidelift.com/security>`_.
Tidelift will coordinate the fix and disclosure.
"""^^xsd:string ;
    schema1:name "importlib-resources"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "5.13.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/six> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.12.0>,
        <https://data.siemens-energy.com/cybersec/version/1.5>,
        <https://data.siemens-energy.com/cybersec/version/1.9.0> ;
    schema1:description """.. image:: https://img.shields.io/pypi/v/six.svg
   :target: https://pypi.org/project/six/
   :alt: six on PyPI

.. image:: https://travis-ci.org/benjaminp/six.svg?branch=master
   :target: https://travis-ci.org/benjaminp/six
   :alt: six on TravisCI

.. image:: https://readthedocs.org/projects/six/badge/?version=latest
   :target: https://six.readthedocs.io/
   :alt: six's documentation on Read the Docs

.. image:: https://img.shields.io/badge/license-MIT-green.svg
   :target: https://github.com/benjaminp/six/blob/master/LICENSE
   :alt: MIT License badge

Six is a Python 2 and 3 compatibility library.  It provides utility functions
for smoothing over the differences between the Python versions with the goal of
writing Python code that is compatible on both Python versions.  See the
documentation for more information on what is provided.

Six supports Python 2.7 and 3.3+.  It is contained in only one Python
file, so it can be easily copied into your project. (The copyright and license
notice must be retained.)

Online documentation is at https://six.readthedocs.io/.

Bugs can be reported to https://github.com/benjaminp/six.  The code can also
be found there.


"""^^xsd:string ;
    schema1:name "six"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "1.16.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/sphinx-autodoc-typehints> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.10>,
        <https://data.siemens-energy.com/cybersec/version/1.23.4>,
        <https://data.siemens-energy.com/cybersec/version/1.25.2> .

<https://data.siemens-energy.com/cybersec/package/tomli> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.1.0>,
        <https://data.siemens-energy.com/cybersec/version/2.0.1> ;
    schema1:description """[![Build Status](https://github.com/hukkin/tomli/workflows/Tests/badge.svg?branch=master)](https://github.com/hukkin/tomli/actions?query=workflow%3ATests+branch%3Amaster+event%3Apush)
[![codecov.io](https://codecov.io/gh/hukkin/tomli/branch/master/graph/badge.svg)](https://codecov.io/gh/hukkin/tomli)
[![PyPI version](https://img.shields.io/pypi/v/tomli)](https://pypi.org/project/tomli)

# Tomli

> A lil' TOML parser

**Table of Contents**  *generated with [mdformat-toc](https://github.com/hukkin/mdformat-toc)*

<!-- mdformat-toc start --slug=github --maxlevel=6 --minlevel=2 -->

- [Intro](#intro)
- [Installation](#installation)
- [Usage](#usage)
  - [Parse a TOML string](#parse-a-toml-string)
  - [Parse a TOML file](#parse-a-toml-file)
  - [Handle invalid TOML](#handle-invalid-toml)
  - [Construct `decimal.Decimal`s from TOML floats](#construct-decimaldecimals-from-toml-floats)
- [FAQ](#faq)
  - [Why this parser?](#why-this-parser)
  - [Is comment preserving round-trip parsing supported?](#is-comment-preserving-round-trip-parsing-supported)
  - [Is there a `dumps`, `write` or `encode` function?](#is-there-a-dumps-write-or-encode-function)
  - [How do TOML types map into Python types?](#how-do-toml-types-map-into-python-types)
- [Performance](#performance)

<!-- mdformat-toc end -->

## Intro<a name="intro"></a>

Tomli is a Python library for parsing [TOML](https://toml.io).
Tomli is fully compatible with [TOML v1.0.0](https://toml.io/en/v1.0.0).

## Installation<a name="installation"></a>

```bash
pip install tomli
```

## Usage<a name="usage"></a>

### Parse a TOML string<a name="parse-a-toml-string"></a>

```python
import tomli

toml_str = \"\"\"
           gretzky = 99

           [kurri]
           jari = 17
           \"\"\"

toml_dict = tomli.loads(toml_str)
assert toml_dict == {"gretzky": 99, "kurri": {"jari": 17}}
```

### Parse a TOML file<a name="parse-a-toml-file"></a>

```python
import tomli

with open("path_to_file/conf.toml", "rb") as f:
    toml_dict = tomli.load(f)
```

The file must be opened in binary mode (with the `"rb"` flag).
Binary mode will enforce decoding the file as UTF-8 with universal newlines disabled,
both of which are required to correctly parse TOML.

### Handle invalid TOML<a name="handle-invalid-toml"></a>

```python
import tomli

try:
    toml_dict = tomli.loads("]] this is invalid TOML [[")
except tomli.TOMLDecodeError:
    print("Yep, definitely not valid.")
```

Note that error messages are considered informational only.
They should not be assumed to stay constant across Tomli versions.

### Construct `decimal.Decimal`s from TOML floats<a name="construct-decimaldecimals-from-toml-floats"></a>

```python
from decimal import Decimal
import tomli

toml_dict = tomli.loads("precision-matters = 0.982492", parse_float=Decimal)
assert toml_dict["precision-matters"] == Decimal("0.982492")
```

Note that `decimal.Decimal` can be replaced with another callable that converts a TOML float from string to a Python type.
The `decimal.Decimal` is, however, a practical choice for use cases where float inaccuracies can not be tolerated.

Illegal types are `dict` and `list`, and their subtypes.
A `ValueError` will be raised if `parse_float` produces illegal types.

## FAQ<a name="faq"></a>

### Why this parser?<a name="why-this-parser"></a>

- it's lil'
- pure Python with zero dependencies
- the fastest pure Python parser [\\*](#performance):
  15x as fast as [tomlkit](https://pypi.org/project/tomlkit/),
  2.4x as fast as [toml](https://pypi.org/project/toml/)
- outputs [basic data types](#how-do-toml-types-map-into-python-types) only
- 100% spec compliant: passes all tests in
  [a test set](https://github.com/toml-lang/compliance/pull/8)
  soon to be merged to the official
  [compliance tests for TOML](https://github.com/toml-lang/compliance)
  repository
- thoroughly tested: 100% branch coverage

### Is comment preserving round-trip parsing supported?<a name="is-comment-preserving-round-trip-parsing-supported"></a>

No.

The `tomli.loads` function returns a plain `dict` that is populated with builtin types and types from the standard library only.
Preserving comments requires a custom type to be returned so will not be supported,
at least not by the `tomli.loads` and `tomli.load` functions.

Look into [TOML Kit](https://github.com/sdispater/tomlkit) if preservation of style is what you need.

### Is there a `dumps`, `write` or `encode` function?<a name="is-there-a-dumps-write-or-encode-function"></a>

[Tomli-W](https://github.com/hukkin/tomli-w) is the write-only counterpart of Tomli, providing `dump` and `dumps` functions.

The core library does not include write capability, as most TOML use cases are read-only, and Tomli intends to be minimal.

### How do TOML types map into Python types?<a name="how-do-toml-types-map-into-python-types"></a>

| TOML type        | Python type         | Details                                                      |
| ---------------- | ------------------- | ------------------------------------------------------------ |
| Document Root    | `dict`              |                                                              |
| Key              | `str`               |                                                              |
| String           | `str`               |                                                              |
| Integer          | `int`               |                                                              |
| Float            | `float`             |                                                              |
| Boolean          | `bool`              |                                                              |
| Offset Date-Time | `datetime.datetime` | `tzinfo` attribute set to an instance of `datetime.timezone` |
| Local Date-Time  | `datetime.datetime` | `tzinfo` attribute set to `None`                             |
| Local Date       | `datetime.date`     |                                                              |
| Local Time       | `datetime.time`     |                                                              |
| Array            | `list`              |                                                              |
| Table            | `dict`              |                                                              |
| Inline Table     | `dict`              |                                                              |

## Performance<a name="performance"></a>

The `benchmark/` folder in this repository contains a performance benchmark for comparing the various Python TOML parsers.
The benchmark can be run with `tox -e benchmark-pypi`.
Running the benchmark on my personal computer output the following:

```console
foo@bar:~/dev/tomli$ tox -e benchmark-pypi
benchmark-pypi installed: attrs==19.3.0,click==7.1.2,pytomlpp==1.0.2,qtoml==0.3.0,rtoml==0.7.0,toml==0.10.2,tomli==1.1.0,tomlkit==0.7.2
benchmark-pypi run-test-pre: PYTHONHASHSEED='2658546909'
benchmark-pypi run-test: commands[0] | python -c 'import datetime; print(datetime.date.today())'
2021-07-23
benchmark-pypi run-test: commands[1] | python --version
Python 3.8.10
benchmark-pypi run-test: commands[2] | python benchmark/run.py
Parsing data.toml 5000 times:
------------------------------------------------------
    parser |  exec time | performance (more is better)
-----------+------------+-----------------------------
     rtoml |    0.901 s | baseline (100%)
  pytomlpp |     1.08 s | 83.15%
     tomli |     3.89 s | 23.15%
      toml |     9.36 s | 9.63%
     qtoml |     11.5 s | 7.82%
   tomlkit |     56.8 s | 1.59%
```

The parsers are ordered from fastest to slowest, using the fastest parser as baseline.
Tomli performed the best out of all pure Python TOML parsers,
losing only to pytomlpp (wraps C++) and rtoml (wraps Rust).

"""^^xsd:string ;
    schema1:name "tomli"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.0.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/towncrier> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/23.6>,
        <https://data.siemens-energy.com/cybersec/version/24.7> .

<https://data.siemens-energy.com/cybersec/package/virtualenv> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/covdefaults>,
        <https://data.siemens-energy.com/cybersec/package/coverage>,
        <https://data.siemens-energy.com/cybersec/package/coverage-enable-subprocess>,
        <https://data.siemens-energy.com/cybersec/package/distlib>,
        <https://data.siemens-energy.com/cybersec/package/filelock>,
        <https://data.siemens-energy.com/cybersec/package/flaky>,
        <https://data.siemens-energy.com/cybersec/package/furo>,
        <https://data.siemens-energy.com/cybersec/package/importlib-metadata>,
        <https://data.siemens-energy.com/cybersec/package/packaging>,
        <https://data.siemens-energy.com/cybersec/package/platformdirs>,
        <https://data.siemens-energy.com/cybersec/package/proselint>,
        <https://data.siemens-energy.com/cybersec/package/pytest>,
        <https://data.siemens-energy.com/cybersec/package/pytest-env>,
        <https://data.siemens-energy.com/cybersec/package/pytest-freezer>,
        <https://data.siemens-energy.com/cybersec/package/pytest-mock>,
        <https://data.siemens-energy.com/cybersec/package/pytest-randomly>,
        <https://data.siemens-energy.com/cybersec/package/pytest-timeout>,
        <https://data.siemens-energy.com/cybersec/package/setuptools>,
        <https://data.siemens-energy.com/cybersec/package/sphinx>,
        <https://data.siemens-energy.com/cybersec/package/sphinx-argparse>,
        <https://data.siemens-energy.com/cybersec/package/sphinxcontrib-towncrier>,
        <https://data.siemens-energy.com/cybersec/package/time-machine>,
        <https://data.siemens-energy.com/cybersec/package/towncrier> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/13.0.0>,
        <https://data.siemens-energy.com/cybersec/version/20.0.35>,
        <https://data.siemens-energy.com/cybersec/version/20.26.2> ;
    schema1:description """# virtualenv

[![PyPI](https://img.shields.io/pypi/v/virtualenv?style=flat-square)](https://pypi.org/project/virtualenv)
[![PyPI - Implementation](https://img.shields.io/pypi/implementation/virtualenv?style=flat-square)](https://pypi.org/project/virtualenv)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/virtualenv?style=flat-square)](https://pypi.org/project/virtualenv)
[![Documentation](https://readthedocs.org/projects/virtualenv/badge/?version=latest&style=flat-square)](http://virtualenv.pypa.io)
[![Discord](https://img.shields.io/discord/803025117553754132)](https://discord.gg/pypa)
[![Downloads](https://static.pepy.tech/badge/virtualenv/month)](https://pepy.tech/project/virtualenv)
[![PyPI - License](https://img.shields.io/pypi/l/virtualenv?style=flat-square)](https://opensource.org/licenses/MIT)
[![Build Status](https://github.com/pypa/virtualenv/workflows/check/badge.svg?branch=main&event=push)](https://github.com/pypa/virtualenv/actions?query=workflow%3Acheck)

A tool for creating isolated `virtual` python environments.

- [Installation](https://virtualenv.pypa.io/en/latest/installation.html)
- [Documentation](https://virtualenv.pypa.io)
- [Changelog](https://virtualenv.pypa.io/en/latest/changelog.html)
- [Issues](https://github.com/pypa/virtualenv/issues)
- [PyPI](https://pypi.org/project/virtualenv)
- [Github](https://github.com/pypa/virtualenv)

## Code of Conduct

Everyone interacting in the virtualenv project's codebases, issue trackers, chat rooms, and mailing lists is expected to
follow the [PSF Code of Conduct](https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md).
"""^^xsd:string ;
    schema1:name "virtualenv"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "20.26.3"^^xsd:string .

<https://data.siemens-energy.com/cybersec/version/0.8.0> software_vocab:version_major "0"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "8"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.0.0> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2.2> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "2"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2022.11.0> software_vocab:version_major "2022"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "11"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/5> software_vocab:version_major "5"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/5.1> software_vocab:version_major "5"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "1"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/7.0.0> software_vocab:version_major "7"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/8> software_vocab:version_major "8"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/package/furo> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2023.08.17>,
        <https://data.siemens-energy.com/cybersec/version/2023.7.26>,
        <https://data.siemens-energy.com/cybersec/version/2023.9.10> .

<https://data.siemens-energy.com/cybersec/package/jupyter-core> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/4.7> .

<https://data.siemens-energy.com/cybersec/package/numpy> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.21>,
        <https://data.siemens-energy.com/cybersec/version/1.22.4>,
        <https://data.siemens-energy.com/cybersec/version/1.23>,
        <https://data.siemens-energy.com/cybersec/version/1.23.2>,
        <https://data.siemens-energy.com/cybersec/version/1.26.0> ;
    schema1:description """<h1 align="center">
<img src="https://raw.githubusercontent.com/numpy/numpy/main/branding/logo/primary/numpylogo.svg" width="300">
</h1><br>


[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](
https://numfocus.org)
[![PyPI Downloads](https://img.shields.io/pypi/dm/numpy.svg?label=PyPI%20downloads)](
https://pypi.org/project/numpy/)
[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/numpy.svg?label=Conda%20downloads)](
https://anaconda.org/conda-forge/numpy)
[![Stack Overflow](https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg)](
https://stackoverflow.com/questions/tagged/numpy)
[![Nature Paper](https://img.shields.io/badge/DOI-10.1038%2Fs41586--020--2649--2-blue)](
https://doi.org/10.1038/s41586-020-2649-2)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/numpy/numpy/badge)](https://securityscorecards.dev/viewer/?uri=github.com/numpy/numpy)


NumPy is the fundamental package for scientific computing with Python.

- **Website:** https://www.numpy.org
- **Documentation:** https://numpy.org/doc
- **Mailing list:** https://mail.python.org/mailman/listinfo/numpy-discussion
- **Source code:** https://github.com/numpy/numpy
- **Contributing:** https://www.numpy.org/devdocs/dev/index.html
- **Bug reports:** https://github.com/numpy/numpy/issues
- **Report a security vulnerability:** https://tidelift.com/docs/security

It provides:

- a powerful N-dimensional array object
- sophisticated (broadcasting) functions
- tools for integrating C/C++ and Fortran code
- useful linear algebra, Fourier transform, and random number capabilities

Testing:

NumPy requires `pytest` and `hypothesis`.  Tests can then be run after installation with:

    python -c "import numpy, sys; sys.exit(numpy.test() is False)"

Code of Conduct
----------------------

NumPy is a community-driven open source project developed by a diverse group of
[contributors](https://numpy.org/teams/). The NumPy leadership has made a strong
commitment to creating an open, inclusive, and positive community. Please read the
[NumPy Code of Conduct](https://numpy.org/code-of-conduct/) for guidance on how to interact
with others in a way that makes our community thrive.

Call for Contributions
----------------------

The NumPy project welcomes your expertise and enthusiasm!

Small improvements or fixes are always appreciated. If you are considering larger contributions
to the source code, please contact us through the [mailing
list](https://mail.python.org/mailman/listinfo/numpy-discussion) first.

Writing code isnâ€™t the only way to contribute to NumPy. You can also:
- review pull requests
- help us stay on top of new and old issues
- develop tutorials, presentations, and other educational materials
- maintain and improve [our website](https://github.com/numpy/numpy.org)
- develop graphic design for our brand assets and promotional materials
- translate website content
- help with outreach and onboard new contributors
- write grant proposals and help with other fundraising efforts

For more information about the ways you can contribute to NumPy, visit [our website](https://numpy.org/contribute/). 
If youâ€™re unsure where to start or how your skills fit in, reach out! You can
ask on the mailing list or here, on GitHub, by opening a new issue or leaving a
comment on a relevant issue that is already open.

Our preferred channels of communication are all public, but if youâ€™d like to
speak to us in private first, contact our community coordinators at
numpy-team@googlegroups.com or on Slack (write numpy-team@googlegroups.com for
an invitation).

We also have a biweekly community call, details of which are announced on the
mailing list. You are very welcome to join.

If you are new to contributing to open source, [this
guide](https://opensource.guide/how-to-contribute/) helps explain why, what,
and how to successfully get involved.
"""^^xsd:string ;
    schema1:name "numpy"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.1.3"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/packaging> a software_vocab:Software_package ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/19.1>,
        <https://data.siemens-energy.com/cybersec/version/20.0>,
        <https://data.siemens-energy.com/cybersec/version/23.1>,
        <https://data.siemens-energy.com/cybersec/version/23.2>,
        <https://data.siemens-energy.com/cybersec/version/24> ;
    schema1:description """packaging
=========

.. start-intro

Reusable core utilities for various Python Packaging
`interoperability specifications <https://packaging.python.org/specifications/>`_.

This library provides utilities that implement the interoperability
specifications which have clearly one correct behaviour (eg: :pep:`440`)
or benefit greatly from having a single shared implementation (eg: :pep:`425`).

.. end-intro

The ``packaging`` project includes the following: version handling, specifiers,
markers, requirements, tags, utilities.

Documentation
-------------

The `documentation`_ provides information and the API for the following:

- Version Handling
- Specifiers
- Markers
- Requirements
- Tags
- Utilities

Installation
------------

Use ``pip`` to install these utilities::

    pip install packaging

The ``packaging`` library uses calendar-based versioning (``YY.N``).

Discussion
----------

If you run into bugs, you can file them in our `issue tracker`_.

You can also join ``#pypa`` on Freenode to ask questions or get involved.


.. _`documentation`: https://packaging.pypa.io/
.. _`issue tracker`: https://github.com/pypa/packaging/issues


Code of Conduct
---------------

Everyone interacting in the packaging project's codebases, issue trackers, chat
rooms, and mailing lists is expected to follow the `PSF Code of Conduct`_.

.. _PSF Code of Conduct: https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md

Contributing
------------

The ``CONTRIBUTING.rst`` file outlines how to contribute to this project as
well as how to report a potential security issue. The documentation for this
project also covers information about `project development`_ and `security`_.

.. _`project development`: https://packaging.pypa.io/en/latest/development/
.. _`security`: https://packaging.pypa.io/en/latest/security/

Project History
---------------

Please review the ``CHANGELOG.rst`` file or the `Changelog documentation`_ for
recent changes and project history.

.. _`Changelog documentation`: https://packaging.pypa.io/en/latest/changelog/

"""^^xsd:string ;
    schema1:name "packaging"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "24.1"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/platformdirs> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/appdirs>,
        <https://data.siemens-energy.com/cybersec/package/covdefaults>,
        <https://data.siemens-energy.com/cybersec/package/furo>,
        <https://data.siemens-energy.com/cybersec/package/mypy>,
        <https://data.siemens-energy.com/cybersec/package/proselint>,
        <https://data.siemens-energy.com/cybersec/package/pytest>,
        <https://data.siemens-energy.com/cybersec/package/pytest-cov>,
        <https://data.siemens-energy.com/cybersec/package/pytest-mock>,
        <https://data.siemens-energy.com/cybersec/package/sphinx>,
        <https://data.siemens-energy.com/cybersec/package/sphinx-autodoc-typehints> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.2.0>,
        <https://data.siemens-energy.com/cybersec/version/2.5>,
        <https://data.siemens-energy.com/cybersec/version/2.6.2>,
        <https://data.siemens-energy.com/cybersec/version/5> ;
    schema1:description """The problem
===========

.. image:: https://github.com/platformdirs/platformdirs/actions/workflows/check.yml/badge.svg
   :target: https://github.com/platformdirs/platformdirs/actions

When writing desktop application, finding the right location to store user data
and configuration varies per platform. Even for single-platform apps, there
may by plenty of nuances in figuring out the right location.

For example, if running on macOS, you should use::

    ~/Library/Application Support/<AppName>

If on Windows (at least English Win) that should be::

    C:\\Documents and Settings\\<User>\\Application Data\\Local Settings\\<AppAuthor>\\<AppName>

or possibly::

    C:\\Documents and Settings\\<User>\\Application Data\\<AppAuthor>\\<AppName>

for `roaming profiles <https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-vista/cc766489(v=ws.10)>`_ but that is another story.

On Linux (and other Unices), according to the `XDG Basedir Spec`_, it should be::

    ~/.local/share/<AppName>

.. _XDG Basedir Spec: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html

``platformdirs`` to the rescue
==============================

This kind of thing is what the ``platformdirs`` package is for.
``platformdirs`` will help you choose an appropriate:

- user data dir (``user_data_dir``)
- user config dir (``user_config_dir``)
- user cache dir (``user_cache_dir``)
- site data dir (``site_data_dir``)
- site config dir (``site_config_dir``)
- user log dir (``user_log_dir``)
- user documents dir (``user_documents_dir``)
- user downloads dir (``user_downloads_dir``)
- user pictures dir (``user_pictures_dir``)
- user videos dir (``user_videos_dir``)
- user music dir (``user_music_dir``)
- user desktop dir (``user_desktop_dir``)
- user runtime dir (``user_runtime_dir``)

And also:

- Is slightly opinionated on the directory names used. Look for "OPINION" in
  documentation and code for when an opinion is being applied.

Example output
==============

On macOS:

.. code-block:: pycon

    >>> from platformdirs import *
    >>> appname = "SuperApp"
    >>> appauthor = "Acme"
    >>> user_data_dir(appname, appauthor)
    '/Users/trentm/Library/Application Support/SuperApp'
    >>> site_data_dir(appname, appauthor)
    '/Library/Application Support/SuperApp'
    >>> user_cache_dir(appname, appauthor)
    '/Users/trentm/Library/Caches/SuperApp'
    >>> user_log_dir(appname, appauthor)
    '/Users/trentm/Library/Logs/SuperApp'
    >>> user_documents_dir()
    '/Users/trentm/Documents'
    >>> user_downloads_dir()
    '/Users/trentm/Downloads'
    >>> user_pictures_dir()
    '/Users/trentm/Pictures'
    >>> user_videos_dir()
    '/Users/trentm/Movies'
    >>> user_music_dir()
    '/Users/trentm/Music'
    >>> user_desktop_dir()
    '/Users/trentm/Desktop'
    >>> user_runtime_dir(appname, appauthor)
    '/Users/trentm/Library/Caches/TemporaryItems/SuperApp'

On Windows:

.. code-block:: pycon

    >>> from platformdirs import *
    >>> appname = "SuperApp"
    >>> appauthor = "Acme"
    >>> user_data_dir(appname, appauthor)
    'C:\\\\Users\\\\trentm\\\\AppData\\\\Local\\\\Acme\\\\SuperApp'
    >>> user_data_dir(appname, appauthor, roaming=True)
    'C:\\\\Users\\\\trentm\\\\AppData\\\\Roaming\\\\Acme\\\\SuperApp'
    >>> user_cache_dir(appname, appauthor)
    'C:\\\\Users\\\\trentm\\\\AppData\\\\Local\\\\Acme\\\\SuperApp\\\\Cache'
    >>> user_log_dir(appname, appauthor)
    'C:\\\\Users\\\\trentm\\\\AppData\\\\Local\\\\Acme\\\\SuperApp\\\\Logs'
    >>> user_documents_dir()
    'C:\\\\Users\\\\trentm\\\\Documents'
    >>> user_downloads_dir()
    'C:\\\\Users\\\\trentm\\\\Downloads'
    >>> user_pictures_dir()
    'C:\\\\Users\\\\trentm\\\\Pictures'
    >>> user_videos_dir()
    'C:\\\\Users\\\\trentm\\\\Videos'
    >>> user_music_dir()
    'C:\\\\Users\\\\trentm\\\\Music'
    >>> user_desktop_dir()
    'C:\\\\Users\\\\trentm\\\\Desktop'
    >>> user_runtime_dir(appname, appauthor)
    'C:\\\\Users\\\\trentm\\\\AppData\\\\Local\\\\Temp\\\\Acme\\\\SuperApp'

On Linux:

.. code-block:: pycon

    >>> from platformdirs import *
    >>> appname = "SuperApp"
    >>> appauthor = "Acme"
    >>> user_data_dir(appname, appauthor)
    '/home/trentm/.local/share/SuperApp'
    >>> site_data_dir(appname, appauthor)
    '/usr/local/share/SuperApp'
    >>> site_data_dir(appname, appauthor, multipath=True)
    '/usr/local/share/SuperApp:/usr/share/SuperApp'
    >>> user_cache_dir(appname, appauthor)
    '/home/trentm/.cache/SuperApp'
    >>> user_log_dir(appname, appauthor)
    '/home/trentm/.local/state/SuperApp/log'
    >>> user_config_dir(appname)
    '/home/trentm/.config/SuperApp'
    >>> user_documents_dir()
    '/home/trentm/Documents'
    >>> user_downloads_dir()
    '/home/trentm/Downloads'
    >>> user_pictures_dir()
    '/home/trentm/Pictures'
    >>> user_videos_dir()
    '/home/trentm/Videos'
    >>> user_music_dir()
    '/home/trentm/Music'
    >>> user_desktop_dir()
    '/home/trentm/Desktop'
    >>> user_runtime_dir(appname, appauthor)
    '/run/user/{os.getuid()}/SuperApp'
    >>> site_config_dir(appname)
    '/etc/xdg/SuperApp'
    >>> os.environ["XDG_CONFIG_DIRS"] = "/etc:/usr/local/etc"
    >>> site_config_dir(appname, multipath=True)
    '/etc/SuperApp:/usr/local/etc/SuperApp'

On Android::

    >>> from platformdirs import *
    >>> appname = "SuperApp"
    >>> appauthor = "Acme"
    >>> user_data_dir(appname, appauthor)
    '/data/data/com.myApp/files/SuperApp'
    >>> user_cache_dir(appname, appauthor)
    '/data/data/com.myApp/cache/SuperApp'
    >>> user_log_dir(appname, appauthor)
    '/data/data/com.myApp/cache/SuperApp/log'
    >>> user_config_dir(appname)
    '/data/data/com.myApp/shared_prefs/SuperApp'
    >>> user_documents_dir()
    '/storage/emulated/0/Documents'
    >>> user_downloads_dir()
    '/storage/emulated/0/Downloads'
    >>> user_pictures_dir()
    '/storage/emulated/0/Pictures'
    >>> user_videos_dir()
    '/storage/emulated/0/DCIM/Camera'
    >>> user_music_dir()
    '/storage/emulated/0/Music'
    >>> user_desktop_dir()
    '/storage/emulated/0/Desktop'
    >>> user_runtime_dir(appname, appauthor)
    '/data/data/com.myApp/cache/SuperApp/tmp'

Note: Some android apps like Termux and Pydroid are used as shells. These
apps are used by the end user to emulate Linux environment. Presence of
``SHELL`` environment variable is used by Platformdirs to differentiate
between general android apps and android apps used as shells. Shell android
apps also support ``XDG_*`` environment variables.


``PlatformDirs`` for convenience
================================

.. code-block:: pycon

    >>> from platformdirs import PlatformDirs
    >>> dirs = PlatformDirs("SuperApp", "Acme")
    >>> dirs.user_data_dir
    '/Users/trentm/Library/Application Support/SuperApp'
    >>> dirs.site_data_dir
    '/Library/Application Support/SuperApp'
    >>> dirs.user_cache_dir
    '/Users/trentm/Library/Caches/SuperApp'
    >>> dirs.user_log_dir
    '/Users/trentm/Library/Logs/SuperApp'
    >>> dirs.user_documents_dir
    '/Users/trentm/Documents'
    >>> dirs.user_downloads_dir
    '/Users/trentm/Downloads'
    >>> dirs.user_pictures_dir
    '/Users/trentm/Pictures'
    >>> dirs.user_videos_dir
    '/Users/trentm/Movies'
    >>> dirs.user_music_dir
    '/Users/trentm/Music'
    >>> dirs.user_desktop_dir
    '/Users/trentm/Desktop'
    >>> dirs.user_runtime_dir
    '/Users/trentm/Library/Caches/TemporaryItems/SuperApp'

Per-version isolation
=====================

If you have multiple versions of your app in use that you want to be
able to run side-by-side, then you may want version-isolation for these
dirs::

    >>> from platformdirs import PlatformDirs
    >>> dirs = PlatformDirs("SuperApp", "Acme", version="1.0")
    >>> dirs.user_data_dir
    '/Users/trentm/Library/Application Support/SuperApp/1.0'
    >>> dirs.site_data_dir
    '/Library/Application Support/SuperApp/1.0'
    >>> dirs.user_cache_dir
    '/Users/trentm/Library/Caches/SuperApp/1.0'
    >>> dirs.user_log_dir
    '/Users/trentm/Library/Logs/SuperApp/1.0'
    >>> dirs.user_documents_dir
    '/Users/trentm/Documents'
    >>> dirs.user_downloads_dir
    '/Users/trentm/Downloads'
    >>> dirs.user_pictures_dir
    '/Users/trentm/Pictures'
    >>> dirs.user_videos_dir
    '/Users/trentm/Movies'
    >>> dirs.user_music_dir
    '/Users/trentm/Music'
    >>> dirs.user_desktop_dir
    '/Users/trentm/Desktop'
    >>> dirs.user_runtime_dir
    '/Users/trentm/Library/Caches/TemporaryItems/SuperApp/1.0'

Be wary of using this for configuration files though; you'll need to handle
migrating configuration files manually.

Why this Fork?
==============

This repository is a friendly fork of the wonderful work started by
`ActiveState <https://github.com/ActiveState/appdirs>`_ who created
``appdirs``, this package's ancestor.

Maintaining an open source project is no easy task, particularly
from within an organization, and the Python community is indebted
to ``appdirs`` (and to Trent Mick and Jeff Rouse in particular) for
creating an incredibly useful simple module, as evidenced by the wide
number of users it has attracted over the years.

Nonetheless, given the number of long-standing open issues
and pull requests, and no clear path towards `ensuring
that maintenance of the package would continue or grow
<https://github.com/ActiveState/appdirs/issues/79>`_, this fork was
created.

Contributions are most welcome.
"""^^xsd:string ;
    schema1:name "platformdirs"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "4.2.2"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/pytest-cov> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.12>,
        <https://data.siemens-energy.com/cybersec/version/4.0>,
        <https://data.siemens-energy.com/cybersec/version/4.1> .

<https://data.siemens-energy.com/cybersec/package/pytest-mock> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2>,
        <https://data.siemens-energy.com/cybersec/version/3.11.1>,
        <https://data.siemens-energy.com/cybersec/version/3.12> .

<https://data.siemens-energy.com/cybersec/package/pytest-xdist> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.34>,
        <https://data.siemens-energy.com/cybersec/version/2>,
        <https://data.siemens-energy.com/cybersec/version/2.2.0>,
        <https://data.siemens-energy.com/cybersec/version/3> .

<https://data.siemens-energy.com/cybersec/package/python-dateutil> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/six> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/2.7>,
        <https://data.siemens-energy.com/cybersec/version/2.7.0>,
        <https://data.siemens-energy.com/cybersec/version/2.8.2> ;
    schema1:description """dateutil - powerful extensions to datetime
==========================================

|pypi| |support| |licence|

|gitter| |readthedocs|

|travis| |appveyor| |pipelines| |coverage|

.. |pypi| image:: https://img.shields.io/pypi/v/python-dateutil.svg?style=flat-square
    :target: https://pypi.org/project/python-dateutil/
    :alt: pypi version

.. |support| image:: https://img.shields.io/pypi/pyversions/python-dateutil.svg?style=flat-square
    :target: https://pypi.org/project/python-dateutil/
    :alt: supported Python version

.. |travis| image:: https://img.shields.io/travis/dateutil/dateutil/master.svg?style=flat-square&label=Travis%20Build
    :target: https://travis-ci.org/dateutil/dateutil
    :alt: travis build status

.. |appveyor| image:: https://img.shields.io/appveyor/ci/dateutil/dateutil/master.svg?style=flat-square&logo=appveyor
    :target: https://ci.appveyor.com/project/dateutil/dateutil
    :alt: appveyor build status

.. |pipelines| image:: https://dev.azure.com/pythondateutilazure/dateutil/_apis/build/status/dateutil.dateutil?branchName=master
    :target: https://dev.azure.com/pythondateutilazure/dateutil/_build/latest?definitionId=1&branchName=master
    :alt: azure pipelines build status

.. |coverage| image:: https://codecov.io/gh/dateutil/dateutil/branch/master/graphs/badge.svg?branch=master
    :target: https://codecov.io/gh/dateutil/dateutil?branch=master
    :alt: Code coverage

.. |gitter| image:: https://badges.gitter.im/dateutil/dateutil.svg
   :alt: Join the chat at https://gitter.im/dateutil/dateutil
   :target: https://gitter.im/dateutil/dateutil

.. |licence| image:: https://img.shields.io/pypi/l/python-dateutil.svg?style=flat-square
    :target: https://pypi.org/project/python-dateutil/
    :alt: licence

.. |readthedocs| image:: https://img.shields.io/readthedocs/dateutil/latest.svg?style=flat-square&label=Read%20the%20Docs
   :alt: Read the documentation at https://dateutil.readthedocs.io/en/latest/
   :target: https://dateutil.readthedocs.io/en/latest/

The `dateutil` module provides powerful extensions to
the standard `datetime` module, available in Python.

Installation
============
`dateutil` can be installed from PyPI using `pip` (note that the package name is
different from the importable name)::

    pip install python-dateutil

Download
========
dateutil is available on PyPI
https://pypi.org/project/python-dateutil/

The documentation is hosted at:
https://dateutil.readthedocs.io/en/stable/

Code
====
The code and issue tracker are hosted on GitHub:
https://github.com/dateutil/dateutil/

Features
========

* Computing of relative deltas (next month, next year,
  next Monday, last week of month, etc);
* Computing of relative deltas between two given
  date and/or datetime objects;
* Computing of dates based on very flexible recurrence rules,
  using a superset of the `iCalendar <https://www.ietf.org/rfc/rfc2445.txt>`_
  specification. Parsing of RFC strings is supported as well.
* Generic parsing of dates in almost any string format;
* Timezone (tzinfo) implementations for tzfile(5) format
  files (/etc/localtime, /usr/share/zoneinfo, etc), TZ
  environment string (in all known formats), iCalendar
  format files, given ranges (with help from relative deltas),
  local machine timezone, fixed offset timezone, UTC timezone,
  and Windows registry-based time zones.
* Internal up-to-date world timezone information based on
  Olson's database.
* Computing of Easter Sunday dates for any given year,
  using Western, Orthodox or Julian algorithms;
* A comprehensive test suite.

Quick example
=============
Here's a snapshot, just to give an idea about the power of the
package. For more examples, look at the documentation.

Suppose you want to know how much time is left, in
years/months/days/etc, before the next easter happening on a
year with a Friday 13th in August, and you want to get today's
date out of the "date" unix system command. Here is the code:

.. code-block:: python3

    >>> from dateutil.relativedelta import *
    >>> from dateutil.easter import *
    >>> from dateutil.rrule import *
    >>> from dateutil.parser import *
    >>> from datetime import *
    >>> now = parse("Sat Oct 11 17:13:46 UTC 2003")
    >>> today = now.date()
    >>> year = rrule(YEARLY,dtstart=now,bymonth=8,bymonthday=13,byweekday=FR)[0].year
    >>> rdelta = relativedelta(easter(year), today)
    >>> print("Today is: %s" % today)
    Today is: 2003-10-11
    >>> print("Year with next Aug 13th on a Friday is: %s" % year)
    Year with next Aug 13th on a Friday is: 2004
    >>> print("How far is the Easter of that year: %s" % rdelta)
    How far is the Easter of that year: relativedelta(months=+6)
    >>> print("And the Easter of that year is: %s" % (today+rdelta))
    And the Easter of that year is: 2004-04-11

Being exactly 6 months ahead was **really** a coincidence :)

Contributing
============

We welcome many types of contributions - bug reports, pull requests (code, infrastructure or documentation fixes). For more information about how to contribute to the project, see the ``CONTRIBUTING.md`` file in the repository.


Author
======
The dateutil module was written by Gustavo Niemeyer <gustavo@niemeyer.net>
in 2003.

It is maintained by:

* Gustavo Niemeyer <gustavo@niemeyer.net> 2003-2011
* Tomi PievilÃ¤inen <tomi.pievilainen@iki.fi> 2012-2014
* Yaron de Leeuw <me@jarondl.net> 2014-2016
* Paul Ganssle <paul@ganssle.io> 2015-

Starting with version 2.4.1 and running until 2.8.2, all source and binary
distributions will be signed by a PGP key that has, at the very least, been
signed by the key which made the previous release. A table of release signing
keys can be found below:

===========  ============================
Releases     Signing key fingerprint
===========  ============================
2.4.1-2.8.2  `6B49 ACBA DCF6 BD1C A206 67AB CD54 FCE3 D964 BEFB`_
===========  ============================

New releases *may* have signed tags, but binary and source distributions
uploaded to PyPI will no longer have GPG signatures attached.

Contact
=======
Our mailing list is available at `dateutil@python.org <https://mail.python.org/mailman/listinfo/dateutil>`_. As it is hosted by the PSF, it is subject to the `PSF code of
conduct <https://www.python.org/psf/conduct/>`_.

License
=======

All contributions after December 1, 2017 released under dual license - either `Apache 2.0 License <https://www.apache.org/licenses/LICENSE-2.0>`_ or the `BSD 3-Clause License <https://opensource.org/licenses/BSD-3-Clause>`_. Contributions before December 1, 2017 - except those those explicitly relicensed - are released only under the BSD 3-Clause License.


.. _6B49 ACBA DCF6 BD1C A206 67AB CD54 FCE3 D964 BEFB:
   https://pgp.mit.edu/pks/lookup?op=vindex&search=0xCD54FCE3D964BEFB
"""^^xsd:string ;
    schema1:name "python-dateutil"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "2.9.0.post0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/setuptools> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/filelock>,
        <https://data.siemens-energy.com/cybersec/package/importlib-metadata>,
        <https://data.siemens-energy.com/cybersec/package/importlib-resources>,
        <https://data.siemens-energy.com/cybersec/package/more-itertools>,
        <https://data.siemens-energy.com/cybersec/package/mypy>,
        <https://data.siemens-energy.com/cybersec/package/packaging>,
        <https://data.siemens-energy.com/cybersec/package/pip>,
        <https://data.siemens-energy.com/cybersec/package/platformdirs>,
        <https://data.siemens-energy.com/cybersec/package/pygments-github-lexers>,
        <https://data.siemens-energy.com/cybersec/package/pyproject-hooks>,
        <https://data.siemens-energy.com/cybersec/package/pytest>,
        <https://data.siemens-energy.com/cybersec/package/pytest-checkdocs>,
        <https://data.siemens-energy.com/cybersec/package/pytest-enabler>,
        <https://data.siemens-energy.com/cybersec/package/pytest-home>,
        <https://data.siemens-energy.com/cybersec/package/pytest-ruff>,
        <https://data.siemens-energy.com/cybersec/package/pytest-xdist>,
        <https://data.siemens-energy.com/cybersec/package/ruff>,
        <https://data.siemens-energy.com/cybersec/package/sphinx>,
        <https://data.siemens-energy.com/cybersec/package/sphinx-notfound-page>,
        <https://data.siemens-energy.com/cybersec/package/tomli>,
        <https://data.siemens-energy.com/cybersec/package/tomli-w>,
        <https://data.siemens-energy.com/cybersec/package/towncrier>,
        <https://data.siemens-energy.com/cybersec/package/virtualenv>,
        <https://data.siemens-energy.com/cybersec/package/wheel> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/18.5>,
        <https://data.siemens-energy.com/cybersec/version/42.0.0>,
        <https://data.siemens-energy.com/cybersec/version/56.0.0>,
        <https://data.siemens-energy.com/cybersec/version/64>,
        <https://data.siemens-energy.com/cybersec/version/67.8.0>,
        <https://data.siemens-energy.com/cybersec/version/68> ;
    schema1:description """.. |pypi-version| image:: https://img.shields.io/pypi/v/setuptools.svg
   :target: https://pypi.org/project/setuptools

.. |py-version| image:: https://img.shields.io/pypi/pyversions/setuptools.svg

.. |test-badge| image:: https://github.com/pypa/setuptools/actions/workflows/main.yml/badge.svg
   :target: https://github.com/pypa/setuptools/actions?query=workflow%3A%22tests%22
   :alt: tests

.. |ruff-badge| image:: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json
   :target: https://github.com/astral-sh/ruff
   :alt: Ruff

.. |docs-badge| image:: https://img.shields.io/readthedocs/setuptools/latest.svg
   :target: https://setuptools.pypa.io

.. |skeleton-badge| image:: https://img.shields.io/badge/skeleton-2024-informational
   :target: https://blog.jaraco.com/skeleton

.. |codecov-badge| image:: https://img.shields.io/codecov/c/github/pypa/setuptools/master.svg?logo=codecov&logoColor=white
   :target: https://codecov.io/gh/pypa/setuptools

.. |tidelift-badge| image:: https://tidelift.com/badges/github/pypa/setuptools?style=flat
   :target: https://tidelift.com/subscription/pkg/pypi-setuptools?utm_source=pypi-setuptools&utm_medium=readme

.. |discord-badge| image:: https://img.shields.io/discord/803025117553754132
   :target: https://discord.com/channels/803025117553754132/815945031150993468
   :alt: Discord

|pypi-version| |py-version| |test-badge| |ruff-badge| |docs-badge| |skeleton-badge| |codecov-badge| |discord-badge|

See the `Quickstart <https://setuptools.pypa.io/en/latest/userguide/quickstart.html>`_
and the `User's Guide <https://setuptools.pypa.io/en/latest/userguide/>`_ for
instructions on how to use Setuptools.

Questions and comments should be directed to `GitHub Discussions
<https://github.com/pypa/setuptools/discussions>`_.
Bug reports and especially tested patches may be
submitted directly to the `bug tracker
<https://github.com/pypa/setuptools/issues>`_.


Code of Conduct
===============

Everyone interacting in the setuptools project's codebases, issue trackers,
chat rooms, and fora is expected to follow the
`PSF Code of Conduct <https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md>`_.


For Enterprise
==============

Available as part of the Tidelift Subscription.

Setuptools and the maintainers of thousands of other packages are working with Tidelift to deliver one enterprise subscription that covers all of the open source you use.

`Learn more <https://tidelift.com/subscription/pkg/pypi-setuptools?utm_source=pypi-setuptools&utm_medium=referral&utm_campaign=github>`_.
"""^^xsd:string ;
    schema1:name "setuptools"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "74.0.0"^^xsd:string .

<https://data.siemens-energy.com/cybersec/version/1.0.0> software_vocab:version_major "1"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/version/2> software_vocab:version_major "2"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/package/mypy> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.11.1>,
        <https://data.siemens-energy.com/cybersec/version/1.7.0>,
        <https://data.siemens-energy.com/cybersec/version/1.8> .

<https://data.siemens-energy.com/cybersec/version/3> software_vocab:version_major "3"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/package/importlib-metadata> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.6>,
        <https://data.siemens-energy.com/cybersec/version/4.6>,
        <https://data.siemens-energy.com/cybersec/version/4.8.3>,
        <https://data.siemens-energy.com/cybersec/version/5.1>,
        <https://data.siemens-energy.com/cybersec/version/6>,
        <https://data.siemens-energy.com/cybersec/version/6.6>,
        <https://data.siemens-energy.com/cybersec/version/7.0.2> .

<https://data.siemens-energy.com/cybersec/package/traitlets> a software_vocab:Software_package ;
    software_vocab:hasDependency <https://data.siemens-energy.com/cybersec/package/argcomplete>,
        <https://data.siemens-energy.com/cybersec/package/mypy>,
        <https://data.siemens-energy.com/cybersec/package/pytest> ;
    software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/5>,
        <https://data.siemens-energy.com/cybersec/version/5.1>,
        <https://data.siemens-energy.com/cybersec/version/5.3>,
        <https://data.siemens-energy.com/cybersec/version/5.4> ;
    schema1:description """# Traitlets

[![Tests](https://github.com/ipython/traitlets/actions/workflows/tests.yml/badge.svg)](https://github.com/ipython/traitlets/actions/workflows/tests.yml)
[![Documentation Status](https://readthedocs.org/projects/traitlets/badge/?version=latest)](https://traitlets.readthedocs.io/en/latest/?badge=latest)
[![Tidelift](https://tidelift.com/subscription/pkg/pypi-traitlets)](https://tidelift.com/badges/package/pypi/traitlets)

|               |                                      |
| ------------- | ------------------------------------ |
| **home**      | https://github.com/ipython/traitlets |
| **pypi-repo** | https://pypi.org/project/traitlets/  |
| **docs**      | https://traitlets.readthedocs.io/    |
| **license**   | Modified BSD License                 |

Traitlets is a pure Python library enabling:

- the enforcement of strong typing for attributes of Python objects
  (typed attributes are called _"traits"_);
- dynamically calculated default values;
- automatic validation and coercion of trait attributes when attempting a
  change;
- registering for receiving notifications when trait values change;
- reading configuring values from files or from command line
  arguments - a distinct layer on top of traitlets, so you may use
  traitlets without the configuration machinery.

Its implementation relies on the [descriptor](https://docs.python.org/howto/descriptor.html)
pattern, and it is a lightweight pure-python alternative of the
[_traits_ library](https://docs.enthought.com/traits/).

Traitlets powers the configuration system of IPython and Jupyter
and the declarative API of IPython interactive widgets.

## Installation

For a local installation, make sure you have
[pip installed](https://pip.pypa.io/en/stable/installing/) and run:

```bash
pip install traitlets
```

For a **development installation**, clone this repository, change into the
`traitlets` root directory, and run pip:

```bash
git clone https://github.com/ipython/traitlets.git
cd traitlets
pip install -e .
```

## Running the tests

```bash
pip install "traitlets[test]"
py.test traitlets
```

## Code Styling

`traitlets` has adopted automatic code formatting so you shouldn't
need to worry too much about your code style.
As long as your code is valid,
the pre-commit hook should take care of how it should look.

To install `pre-commit` locally, run the following::

```
pip install pre-commit
pre-commit install
```

You can invoke the pre-commit hook by hand at any time with::

```
pre-commit run
```

which should run any autoformatting on your code
and tell you about any errors it couldn't fix automatically.
You may also install [black integration](https://github.com/psf/black#editor-integration)
into your text editor to format code automatically.

If you have already committed files before setting up the pre-commit
hook with `pre-commit install`, you can fix everything up using
`pre-commit run --all-files`. You need to make the fixing commit
yourself after that.

Some of the hooks only run on CI by default, but you can invoke them by
running with the `--hook-stage manual` argument.

## Usage

Any class with trait attributes must inherit from `HasTraits`.
For the list of available trait types and their properties, see the
[Trait Types](https://traitlets.readthedocs.io/en/latest/trait_types.html)
section of the documentation.

### Dynamic default values

To calculate a default value dynamically, decorate a method of your class with
`@default({traitname})`. This method will be called on the instance, and
should return the default value. In this example, the `_username_default`
method is decorated with `@default('username')`:

```Python
import getpass
from traitlets import HasTraits, Unicode, default

class Identity(HasTraits):
    username = Unicode()

    @default('username')
    def _username_default(self):
        return getpass.getuser()
```

### Callbacks when a trait attribute changes

When a trait changes, an application can follow this trait change with
additional actions.

To do something when a trait attribute is changed, decorate a method with
[`traitlets.observe()`](https://traitlets.readthedocs.io/en/latest/api.html?highlight=observe#traitlets.observe).
The method will be called with a single argument, a dictionary which contains
an owner, new value, old value, name of the changed trait, and the event type.

In this example, the `_num_changed` method is decorated with `` @observe(`num`) ``:

```Python
from traitlets import HasTraits, Integer, observe

class TraitletsExample(HasTraits):
    num = Integer(5, help="a number").tag(config=True)

    @observe('num')
    def _num_changed(self, change):
        print("{name} changed from {old} to {new}".format(**change))
```

and is passed the following dictionary when called:

```Python
{
  'owner': object,  # The HasTraits instance
  'new': 6,         # The new value
  'old': 5,         # The old value
  'name': "foo",    # The name of the changed trait
  'type': 'change', # The event type of the notification, usually 'change'
}
```

### Validation and coercion

Each trait type (`Int`, `Unicode`, `Dict` etc.) may have its own validation or
coercion logic. In addition, we can register custom cross-validators
that may depend on the state of other attributes. For example:

```Python
from traitlets import HasTraits, TraitError, Int, Bool, validate

class Parity(HasTraits):
    value = Int()
    parity = Int()

    @validate('value')
    def _valid_value(self, proposal):
        if proposal['value'] % 2 != self.parity:
            raise TraitError('value and parity should be consistent')
        return proposal['value']

    @validate('parity')
    def _valid_parity(self, proposal):
        parity = proposal['value']
        if parity not in [0, 1]:
            raise TraitError('parity should be 0 or 1')
        if self.value % 2 != parity:
            raise TraitError('value and parity should be consistent')
        return proposal['value']

parity_check = Parity(value=2)

# Changing required parity and value together while holding cross validation
with parity_check.hold_trait_notifications():
    parity_check.value = 1
    parity_check.parity = 1
```

However, we **recommend** that custom cross-validators don't modify the state
of the HasTraits instance.

## About the IPython Development Team

The IPython Development Team is the set of all contributors to the IPython project.
This includes all of the IPython subprojects.

The core team that coordinates development on GitHub can be found here:
https://github.com/jupyter/.

## Our Copyright Policy

IPython uses a shared copyright model. Each contributor maintains copyright
over their contributions to IPython. But, it is important to note that these
contributions are typically only changes to the repositories. Thus, the IPython
source code, in its entirety is not the copyright of any single person or
institution. Instead, it is the collective copyright of the entire IPython
Development Team. If individual contributors want to maintain a record of what
changes/contributions they have specific copyright on, they should indicate
their copyright in the commit message of the change, when they commit the
change to one of the IPython repositories.

With this in mind, the following banner should be used in any source code file
to indicate the copyright and license terms:

```
# Copyright (c) IPython Development Team.
# Distributed under the terms of the Modified BSD License.
```
"""^^xsd:string ;
    schema1:name "traitlets"^^xsd:string ;
    schema1:programmingLanguage "Python"^^xsd:string ;
    schema1:version "5.14.3"^^xsd:string .

<https://data.siemens-energy.com/cybersec/package/typing-extensions> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/3.10.0>,
        <https://data.siemens-energy.com/cybersec/version/3.7.4.3>,
        <https://data.siemens-energy.com/cybersec/version/4.0.0>,
        <https://data.siemens-energy.com/cybersec/version/4.12.2>,
        <https://data.siemens-energy.com/cybersec/version/4.6.0>,
        <https://data.siemens-energy.com/cybersec/version/4.6.1>,
        <https://data.siemens-energy.com/cybersec/version/4.8> .

<https://data.siemens-energy.com/cybersec/version/4> software_vocab:version_major "4"^^xsd:int ;
    software_vocab:version_micro "0"^^xsd:int ;
    software_vocab:version_minor "0"^^xsd:int .

<https://data.siemens-energy.com/cybersec/package/sphinx> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/1.3>,
        <https://data.siemens-energy.com/cybersec/version/1.7>,
        <https://data.siemens-energy.com/cybersec/version/3.5>,
        <https://data.siemens-energy.com/cybersec/version/4>,
        <https://data.siemens-energy.com/cybersec/version/5.0.2>,
        <https://data.siemens-energy.com/cybersec/version/7.0.0>,
        <https://data.siemens-energy.com/cybersec/version/7.2>,
        <https://data.siemens-energy.com/cybersec/version/7.2.6>,
        <https://data.siemens-energy.com/cybersec/version/7.3>,
        <https://data.siemens-energy.com/cybersec/version/8.1> .

<https://data.siemens-energy.com/cybersec/package/pytest> software_vocab:hasVersion <https://data.siemens-energy.com/cybersec/version/4.3.0>,
        <https://data.siemens-energy.com/cybersec/version/6.2.4>,
        <https://data.siemens-energy.com/cybersec/version/7>,
        <https://data.siemens-energy.com/cybersec/version/7.0.0>,
        <https://data.siemens-energy.com/cybersec/version/7.1>,
        <https://data.siemens-energy.com/cybersec/version/7.2.0>,
        <https://data.siemens-energy.com/cybersec/version/7.3.2>,
        <https://data.siemens-energy.com/cybersec/version/7.4>,
        <https://data.siemens-energy.com/cybersec/version/7.4.3>,
        <https://data.siemens-energy.com/cybersec/version/8>,
        <https://data.siemens-energy.com/cybersec/version/8.2>,
        <https://data.siemens-energy.com/cybersec/version/8.2.0> .

